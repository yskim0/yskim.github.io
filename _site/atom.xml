<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-05-26T20:42:28+09:00</updated><id>http://localhost:4000/atom.xml</id><title type="html">Yeonsoo Kimâ€™s blog</title><author><name>Yeonsoo Kim</name></author><entry><title type="html">Data2Visë¥¼ ì´ìš©í•œ ìë™í™”ëœ ì‹œê°í™” ì¶”ì²œ ëª¨ë¸ êµ¬ì¶•í•˜ê¸°(Visualization Recommendation)</title><link href="http://localhost:4000/project/2021/05/23/Capstone_Start7_review/" rel="alternate" type="text/html" title="Data2Visë¥¼ ì´ìš©í•œ ìë™í™”ëœ ì‹œê°í™” ì¶”ì²œ ëª¨ë¸ êµ¬ì¶•í•˜ê¸°(Visualization Recommendation)" /><published>2021-05-23T00:00:00+09:00</published><updated>2021-05-23T00:00:00+09:00</updated><id>http://localhost:4000/project/2021/05/23/Capstone_Start7_review</id><content type="html" xml:base="http://localhost:4000/project/2021/05/23/Capstone_Start7_review/">&lt;p&gt;ì´í™”ì—¬ëŒ€ 2021-1í•™ê¸° ìº¡ìŠ¤í†¤ë””ìì¸í”„ë¡œì íŠ¸B ìŠ¤íƒ€íŠ¸7íŒ€ Ewha visualization Recommendation Program(ERP) ê¸°ìˆ  íŠœí† ë¦¬ì–¼ì— ê´€í•œ ê¸€ì…ë‹ˆë‹¤.
ë³¸ í¬ìŠ¤íŒ…ì€ Data2Vis ë…¼ë¬¸ì— ì œì‹œëœ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì— ì•Œë§ì€ Plotì„ ìë™ìœ¼ë¡œ ì¶”ì²œí•´ì£¼ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;
&lt;blockquote&gt;
  &lt;p&gt;Contents&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
  &lt;li&gt;About Our Project&lt;/li&gt;
  &lt;li&gt;Data2Vis
    &lt;ul&gt;
      &lt;li&gt;Model Archi.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Our Model
    &lt;ul&gt;
      &lt;li&gt;Dataset&lt;/li&gt;
      &lt;li&gt;Model config &amp;amp; Training&lt;/li&gt;
      &lt;li&gt;Tensorboard - Loss&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Web Demo&lt;/li&gt;
  &lt;li&gt;Conclusion&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;about-our-projectand-goal&quot;&gt;About Our Projectâ€¦(and goal)&lt;/h1&gt;

&lt;p&gt;ì•ˆë…•í•˜ì„¸ìš”. ë…¼ë¬¸ ë¦¬ë·° ê¸€ì´ ì•„ë‹Œ í”„ë¡œì íŠ¸ ê´€ë ¨ ê¸€ì€ ì˜¤ëœë§Œì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ìš”ì¦˜ ìºê¸€, ë°ì´ì½˜ ë“± ë°ì´í„° ë¶„ì„ ê´€ë ¨ Competitionë“¤ì´ êµ‰ì¥íˆ ë§ë‹¤ëŠ” ê±¸ ê´€ì‹¬ìˆìœ¼ì‹  ë¶„ë“¤ì€ ì•„ì‹¤ ê²ë‹ˆë‹¤.
ì €ë„ ë°ì´ì½˜ ëŒ€íšŒì— ìˆ˜ìƒí•œ ê²½í—˜ì´ ìˆê³ , ì§€ê¸ˆì€ ì˜ˆì „ë§Œí¼ì€ ëª»í•˜ì§€ë§Œ í•œì°½ Data Science ê³µë¶€ë¥¼ ë§ì´ í•  ë•Œê°€ ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì²˜ìŒ ë°ì´í„° ë¶„ì„ì„ ê³µë¶€í•˜ì‹œëŠ” ë¶„ë“¤ë¶€í„° ì „ë¬¸ì ìœ¼ë¡œ ë‹¤ë£¨ì‹œëŠ” ë¶„ë“¤ê¹Œì§€, ë°ì´í„° ë¶„ì„ ë¶„ì•¼ì—ì„œ ì§€ë‚˜ì¹  ìˆ˜ ì—†ëŠ” ë¶€ë¶„ì€ ë‹¨ì—°ì½” &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Visualization about DATA&lt;/code&gt; ì¼ ê²ƒì…ë‹ˆë‹¤. ìºê¸€ì—ì„œ ëŒ€íšŒ í•˜ë‚˜ê°€ ì—´ë¦¬ìë§ˆì í•œì‹œê°„ ë‚´ë¡œ ì•„ë¦„ë‹¤ìš´ EDA plotë“¤ì´ ê³ ìˆ˜ë‹˜ë“¤ì˜ ì†ì„ ê±°ì³ ê·¸ë ¤ì§€ëŠ” ê²ƒì„ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (Kaggleì—ì„œëŠ” Notebook ë¶„ì•¼ì˜ ê·¸ëœë“œ ë§ˆìŠ¤í„°ë¶„ë“¤ì´ ì£¼ë¡œ ì´ ì—­í• ì„ ë§¡ì£ .)&lt;/p&gt;

&lt;p&gt;ë˜í•œ ì´ëŸ¬í•œ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ëŒ€íšŒê°€ ì•„ë‹ˆë”ë¼ë„,  ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ì— ìˆì–´ì„œ í˜„ì¬ ìì‚¬/ê°œì¸ì´ ê°–ê³  ìˆëŠ” ë°ì´í„°ê°€ ì–´ë–¤ í˜•íƒœì´ê³  ì–´ë–¤ ì˜ë¯¸ì¸ì§€ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•œ ì¼ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ëŸ¬í•œ ìƒí™©(=ë°ì´í„°ì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•œ ìƒí™©)ì— ë°œë§ì¶”ì–´ Data Visualizationì— ëŒ€í•œ ì—°êµ¬ ë˜í•œ ë°œì „ë˜ê³  ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì €í¬ íŒ€ì€ ì´ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Data Visualization&lt;/code&gt; ì—°êµ¬ì—ì„œë„ cold-start ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” &lt;strong&gt;(ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ) Data Visualization Recommendation&lt;/strong&gt;ì„ í”„ë¡œì íŠ¸ ì£¼ì œë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤. ìì„¸íˆëŠ” ì €í¬ê°€ ì œê³µí•˜ëŠ” ìƒ˜í”Œ ë°ì´í„° ë˜ëŠ” ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë°ì´í„°ì…‹ì„ upload í–ˆì„ ë•Œ, ì €í¬ì˜ Visualization Recommendation ëª¨ë¸ì´ ë°ì´í„°ì…‹ì„ &lt;strong&gt;í•´ì„&lt;/strong&gt;í•˜ì—¬ ì—¬ëŸ¬ ê°œì˜ plotì„ ê·¸ë ¤ì£¼ì–´ ì‚¬ìš©ìì—ê²Œ ì¶”ì²œí•´ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ì¦‰ ìš”ì•½í•˜ìë©´,&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ë°ì´í„°ì…‹(e.g., csv, tsv, json)ì„ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;input&lt;/code&gt;ìœ¼ë¡œ í•˜ì—¬&lt;/li&gt;
  &lt;li&gt;ì €í¬ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ í•´ë‹¹ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì ì ˆí•œ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;chart(visualization) recommendation&lt;/code&gt;ì„ kê°œ ë¦¬ìŠ¤íŠ¸ì—…í•˜ì—¬ ë³´ì—¬ì£¼ê³ &lt;/li&gt;
  &lt;li&gt;ì‚¬ìš©ìê°€ chartë¥¼ ì„ íƒ
    &lt;ul&gt;
      &lt;li&gt;ì¶”ê°€ì ìœ¼ë¡œëŠ” ì°¨íŠ¸ë¥¼ í¸ì§‘í•˜ê³  ì €ì¥í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ì˜ ê¸°ëŠ¥ì„ êµ¬í˜„í•œ ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬í˜„í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë³¸ í¬ìŠ¤íŒ…ì—ì„œëŠ” â€˜ì‚¬ìš©ìê°€ chartë¥¼ ì„ íƒí•˜ê¸° ì „â€™ê¹Œì§€ì˜ ê³¼ì •, ì¦‰ ë°ì´í„°ì…‹ì„ ë°›ì•„ ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ chart recommendationì„ ëª‡ ê°€ì§€ ë³´ì—¬ì£¼ëŠ” ë‹¨ê³„ê¹Œì§€ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;data2vis&quot;&gt;Data2Vis&lt;/h1&gt;

&lt;p&gt;ì•ì„œ Data Visualizationì— ëŒ€í•œ ì—°êµ¬ê°€ í™œì„±í™”ë˜ê³  ìˆë‹¤ í•˜ì˜€ëŠ”ë°, ì´ì— ë”°ë¼ ë‹¹ì—°íˆ Visualizaiton Recommendation ì—°êµ¬ë„ ìƒë‹¹íˆ ë°œì „í•˜ì˜€ìŠµë‹ˆë‹¤. ê´€ë ¨í•˜ì—¬ ì—¬ëŸ¬ ê°œì˜ ë…¼ë¬¸ì„ ì½ì–´ë³´ì•˜ê³  github ì½”ë“œ ë“±ì„ ë‹¤ ì‚´í´ë³¸ í›„ ë‚´ë¦° ê²°ê³¼, ì €í¬ëŠ” &lt;a href=&quot;https://arxiv.org/abs/1804.03126&quot;&gt;Data2Vis : Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural Networks&lt;/a&gt; ë…¼ë¬¸ì— ì œì‹œëœ ëª¨ë¸ì„ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤. &lt;strong&gt;(ë¬¼ë¡  ì €í¬ê°€ ì‚¬ìš©í•  ë°ì´í„°ì…‹ì— ë” optimizeí•  ì˜ˆì •ì´ë¯€ë¡œ ëª¨ë¸ì— ëŒ€í•œ ì—¬ëŸ¬ ê°€ì§€ ì‹¤í—˜ë“¤ì„ ë°©í•™ë™ì•ˆ ê±°ì¹  ì˜ˆì •ì…ë‹ˆë‹¤.)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Data2Vis&lt;/code&gt;ëŠ” visualization generation ë¬¸ì œë¥¼ í•˜ë‚˜ì˜ language translation problemìœ¼ë¡œ ë³´ì•˜ê³ , ì´ ë¬¸ì œë¥¼ &lt;strong&gt;attention ë² ì´ìŠ¤ì˜ LSTM encoder-decoder ëª¨ë¸&lt;/strong&gt;ì„ ì‚¬ìš©í•˜ì—¬ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë˜í•œ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë‚´ëŠ” grammarë¡œ JSON í¬ë§·ì˜ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Vega-Lite&lt;/code&gt;ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ë”°ë¼ì„œ, í•™ìŠµìš© ë°ì´í„°ì…‹ë„ Vega-Lite ì½”ë“œ!&lt;/li&gt;
  &lt;li&gt;ë°ì´í„°ì…‹ì— ëŒ€í•œ ì´ì•¼ê¸°ëŠ” ì•„ë˜ ì„¹ì…˜ì—ì„œ ë” ìì„¸í•˜ê²Œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Data2Vis ëª¨ë¸ì— ëŒ€í•´ ê°„ëµí•˜ê²Œ ì •ë¦¬í•´ë³´ê³  í•´ë‹¹ ì„¹ì…˜ì€ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤. (&lt;a href=&quot;https://yskim0.github.io/paper%20review/2021/05/06/Data2Vis/&quot;&gt;ì´ì „ ê²Œì‹œê¸€&lt;/a&gt;ì— ë” ìì„¸íˆ ë¦¬ë·°ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ë¶€ë¶„ì€ í•´ë‹¹ ê¸€ì„ ë°œì·Œí•˜ì˜€ìŠµë‹ˆë‹¤.)&lt;/p&gt;

&lt;h2 id=&quot;model-architecture&quot;&gt;Model Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/117335052-adad1280-aed5-11eb-9188-0b30c1cb9533.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the data visualization problem as a &lt;strong&gt;Seq2Seq translation problem&lt;/strong&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input : dataset (fields, values in JSON format)
output : valid Vega-Lite visualization specification
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;encoder-decoder archi.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;where the encoder reads and encodes a source sequence into a fixed length vector, and a decoder outputs a translation based on this vector.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Attention&lt;/strong&gt; Mechanism
    &lt;blockquote&gt;
      &lt;p&gt;Attention mechanisms allow a model to focus on aspects of an input sequence while generating output tokens.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Beam Search algorithm&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The beam search algorithm used in sequence-to-sequence neural translation models keeps track of k most probable output tokens at each step of decoding, where k is known as the beamwidth. This enables the generation of k most likely output sequences for a given input sequence.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;THREE techniques : &lt;strong&gt;bidirectional encoding, differential weighing of context via an attention mechanism, and beam search&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;character&lt;/strong&gt;-based sequence model&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;our-model-dataset-training-settings&quot;&gt;Our Model (Dataset, Training-Settingsâ€¦)&lt;/h1&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ğŸ“© input : 1ê°œì˜ dataset
ğŸ“¬ output : visualization recommendation plot
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ë³¸ê²©ì ìœ¼ë¡œ ì €í¬ í”„ë¡œì íŠ¸ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ë§Œë“¤ê² ìŠµë‹ˆë‹¤. ì‚¬ì‹¤ data2vis ê¹ƒí—ˆë¸Œì—ì„œ ì œê³µí•˜ëŠ” pretrained modelì´ ìˆì§€ë§Œ, ì–´ì°¨í”¼ ë°©í•™ì— ì—¬ëŸ¬ ì‹¤í—˜ì„ í• ê±°ë‹ˆê¹Œ ë¯¸ë¦¬ ì—°ìŠµí•´ë³¸ë‹¤ëŠ” ìƒê°ìœ¼ë¡œ &lt;strong&gt;ì²˜ìŒë¶€í„° ìƒˆë¡œ í•™ìŠµì‹œì¼°ìŠµë‹ˆë‹¤.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;

&lt;p&gt;Vega-Lite ê·¸ë˜ë¨¸ ê¸°ë°˜ì˜ ì–‘ì§ˆì˜ plot ë°ì´í„°ì…‹ì„ í¬ë¡¤ë§í•˜ëŠ” ë“± ë”°ë¡œ ìˆ˜ì§‘í•´ì˜¤ê¸° ì–´ë µë‹¤ê³  íŒë‹¨í•œ ê²°ê³¼, &lt;strong&gt;data2visê°€ í•™ìŠµí•  ë•Œ ì‚¬ìš©í–ˆë˜ ë°ì´í„°ì…‹ì„ ê·¸ëŒ€ë¡œ&lt;/strong&gt; ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. (ë³¸ í¬ìŠ¤íŒ…ì—ì„œ ì‚¬ìš©ëœ ë°ì´í„°ì…‹ì€ data2vis githubì— ìˆìŠµë‹ˆë‹¤.)&lt;/p&gt;

&lt;h3 id=&quot;ì›ë³¸-ë°ì´í„°-ì˜ˆì‹œ&quot;&gt;ì›ë³¸ ë°ì´í„° ì˜ˆì‹œ&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;barley.json&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/118830315-cff15800-b8f9-11eb-8b11-c386ae61f3d1.png&quot; alt=&quot;data&quot; /&gt;&lt;/p&gt;

&lt;p&gt;plotì„ ê·¸ë¦¬ê³ ì í•˜ëŠ” ì›ë³¸ ë°ì´í„°ê°€ ìœ„ì™€ ê°™ì€ í˜•íƒœë¥¼ ê°€ì§„ json íƒ€ì…ì˜ ë°ì´í„°ë¼ ê°€ì •í•˜ê² ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/118835697-32e4ee00-b8fe-11eb-86db-be0fc6ff7fb3.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì´ëŠ” ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë•Œ ì‚¬ìš©ë˜ëŠ” &lt;strong&gt;training data&lt;/strong&gt; ì¤‘ í•˜ë‚˜ë¡œ, ìœ„ì˜ barely.json ë°ì´í„°ì— ëŒ€í•œ plotì„ ê·¸ë¦¬ëŠ” Vega-lite ê·¸ë˜ë¨¸ ê¸°ë°˜ì˜ ë°ì´í„°ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì¦‰, Vega-lite ê·¸ë˜ë¨¸ ê¸°ë°˜ì˜ plotì„ ìƒì„±í•˜ê¸° ìœ„í•˜ì—¬ ì´ëŸ¬í•œ í˜•ì‹ì˜ ë°ì´í„°ì…‹ì´ í•™ìŠµìš©ìœ¼ë¡œ í•„ìš”í•œ ê²ƒì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/118832412-928dca00-b8fb-11eb-9832-73bbedccd0e9.png&quot; alt=&quot;plot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;barley.json ë°ì´í„°ì˜ ì¼ë¶€ì™€ ìœ„ì˜ Vega-lite ê·¸ë˜ë¨¸ ê¸°ë°˜ ì½”ë“œë¥¼ &lt;a href=&quot;http://vega.github.io/editor/&quot;&gt;Vega Editor&lt;/a&gt;ë¥¼ í†µí•´ ê·¸ë¦° plotì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;mark&quot; : &quot;bar&quot;&lt;/code&gt;ë¡œ ì½”ë”©í–ˆë“¯ì´ bar ì°¨íŠ¸ì´ê³ , &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;encoding&lt;/code&gt; íŒŒíŠ¸ë¥¼ ë´¤ì„ ë•Œ xì¶•ì€ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;yield&lt;/code&gt;, yì¶•ì€ yield ë²”ìœ„ì— ë”°ë¥¸ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;count&lt;/code&gt; aggregation functionì´ ì ìš©ë˜ì–´ ê·¸ë¦¬ê³ ì í•œ ì°¨íŠ¸ê°€ ì˜ ê·¸ë ¤ì¡ŒìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ë”°ë¼ì„œ,&lt;/strong&gt; ì €í¬ê°€ í•™ìŠµì‹œí‚¤ê³ ì í•˜ëŠ” ëª¨ë¸ì€&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;barley.json&lt;/code&gt;ê³¼ ê°™ì€ ë°ì´í„°ê°€ &lt;strong&gt;input&lt;/strong&gt;ìœ¼ë¡œ ë“¤ì–´ì™”ì„ ë•Œ&lt;/li&gt;
  &lt;li&gt;ëª¨ë¸ ì•ˆì—ì„œ Vega-lite ê·¸ë˜ë¨¸ ê¸°ë°˜ì˜ &lt;strong&gt;ì ì ˆí•œ&lt;/strong&gt; plotì„ ê·¸ë¦´ ìˆ˜ ìˆëŠ” ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ëŠ” language translation ë¬¸ì œë¥¼ í•´ê²°
í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    &lt;ul&gt;
      &lt;li&gt;ì´ë¥¼ ìœ„í•´ì„œ ì‹¤ì œ ë°ì´í„°ì˜ í•„ë“œë¥¼ str, numìœ¼ë¡œ ë°”ê¾¸ì–´ ëª¨ë¸ì— ë„£ì€ í›„ Vega-lite Specë¥¼ ì•„ì›ƒí’‹ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜¨ Vega-lite specì— ì›ë³¸ ë°ì´í„° í•„ë“œë¥¼ ë‹¤ì‹œ mapping ì‹œì¼œ ìµœì¢… Vega-lite specì„ ë§Œë“  í›„ ê·¸ëŒ€ë¡œ ì›¹ì— ì˜¬ë ¤ì£¼ë©´ ë©ë‹ˆë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;training-data&quot;&gt;Training data&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/118835445-f87b5100-b8fd-11eb-8b53-61dc15ec90b2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;11ê°œì˜ ë°ì´í„°ì…‹ì— ëŒ€í•œ &lt;strong&gt;4300ì—¬ê°œ&lt;/strong&gt;ì˜ Vega-Lite codeë¥¼ í•™ìŠµìš© ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
ì°¸ê³ ë¡œ ë°ì´í„°ì…‹ ë¶„í¬ëŠ” &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Training : Eval : Test = 0.8 : 0.1 : 0.1&lt;/code&gt; ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;model-config--training&quot;&gt;Model config &amp;amp; Training&lt;/h2&gt;

&lt;h3 id=&quot;config&quot;&gt;Config&lt;/h3&gt;

&lt;p&gt;&lt;img height=&quot;500&quot; src=&quot;https://user-images.githubusercontent.com/48315997/119250775-6022ec00-bbdd-11eb-9c0a-d066f2c0d55d.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;í•™ìŠµì‹œí‚¤ëŠ” ëª¨ë¸ì˜ configì…ë‹ˆë‹¤. 
ê¸°ì¡´ëŒ€ë¡œ ëª¨ë¸ì€ &lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AttentionSeq2Seq&lt;/code&gt;&lt;/strong&gt;ì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë‚˜ ë‹¤ë¥¸ ë¶€ë¶„ë“¤ì„ ëª‡ ê°€ì§€ ìˆ˜ì •í•´ë³´ì•˜ìŠµë‹ˆë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;LSTM cell -&amp;gt; &lt;strong&gt;GRU cell&lt;/strong&gt; ë¡œ ë³€ê²½&lt;/li&gt;
  &lt;li&gt;Dropout ìˆ˜ì¹˜ë¥¼ 0.5 -&amp;gt; &lt;strong&gt;0.8ë¡œ ë³€ê²½&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;epoch ìˆ˜ë¥¼ 20000 -&amp;gt; &lt;strong&gt;15000ìœ¼ë¡œ ë³€ê²½&lt;/strong&gt; (3000ë§ˆë‹¤ save)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;ì´ì œ í„°ë¯¸ë„ì— ê°€ì„œ ê°€ìƒí™˜ê²½ ì˜ ì„¤ì •í•´ë‘ê³  &lt;strong&gt;ì•„ë˜ì™€ ê°™ì€ ëª…ë ¹ì–´&lt;/strong&gt;ë¥¼ ì¹œ í›„ í•™ìŠµì´ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ë©´ ë©ë‹ˆë‹¤.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python3 &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; bin.train &lt;span class=&quot;nt&quot;&gt;--config_paths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;s2&quot;&gt;&quot;example_configs/nmt_medium-Copy1.yml,example_configs/train_seq2seq.yml,example_configs/text_metrics_bpe.yml&quot;&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;[ê²°ê³¼]&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/119250967-b3496e80-bbde-11eb-9f64-44e6c33de0a6.png&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-05-23 á„‹á…©á„’á…® 3 51 10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ëŒ€ëµ &lt;strong&gt;2ì¼ì •ë„&lt;/strong&gt; ì†Œìš”ë˜ì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;tensorboard---loss&quot;&gt;Tensorboard - Loss&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;tensorboard&lt;/strong&gt;ë¥¼ í™œìš©í•´ train ê³¼ì •ì´ ì–´ë• ëŠ”ì§€ ëª¨ë‹ˆí„°ë§ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë‹¤ë“¤ ì˜ ì•„ì‹¤ ê²ƒ ê°™ìŠµë‹ˆë‹¤ë§Œ, summary ë°ì´í„° íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•˜ì—¬ ì•„ë˜ì™€ ê°™ì€ ëª…ë ¹ì–´ë¥¼ ì¹©ë‹ˆë‹¤.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensorboard --logdir .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/119251097-9d887900-bbdf-11eb-9952-f2fc88d27ccc.png&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-05-23 á„‹á…©á„’á…® 3 57 44&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;í•™ìŠµì´ ë§ˆë¬´ë¦¬ë  ì¦ˆìŒì—ëŠ” lossê°€ 0.03 ì •ë„ì˜ ê°’ì„ ê°€ì§€ê³  ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ì•„ì‰½ê²Œë„ accuracyê°™ì€ ì •ëŸ‰ì ì¸ metricì„ ì‚¬ìš©í•˜ê¸°ëŠ” ì–´ë µê¸° ë•Œë¬¸ì— ì§ì ‘ ê²°ê³¼ë¥¼ ë³´ê³  í‰ê°€í•´ë³´ê² ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;web-demo&quot;&gt;Web Demo&lt;/h1&gt;

&lt;p&gt;&lt;img width=&quot;800&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-05-23 á„‹á…©á„’á…® 4 04 28&quot; src=&quot;https://user-images.githubusercontent.com/48315997/119251228-8f872800-bbe0-11eb-8211-e7cc84d25eb0.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì§ì ‘ í•™ìŠµì‹œí‚¨ ëª¨ë¸(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model.ckpt-15000&lt;/code&gt;)ì„ web demoì™€ ì—°ë™ì‹œì¼°ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ëª¨ë¸ì— randomí•œ test dataë¥¼ ë¶ˆëŸ¬ì™€ì„œ Inferenceí•œ í›„ Vega-lite specì„ ì›¹ì— ê·¸ë¦° ê²°ê³¼ì…ë‹ˆë‹¤.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;ê·¸ëŸ´ë“¯í•œ plot&lt;/em&gt;&lt;/strong&gt;ì´ ê½¤ ë‚˜ì˜¤ëŠ” ê²ƒì„ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤!&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;í˜„ì¬ ì €í¬ëŠ” í”„ë¡œì íŠ¸ì— ë§ëŠ” ì›¹ í˜ì´ì§€ë¥¼ êµ¬í˜„í•˜ëŠ” ì¤‘ì´ê³ , ëª¨ë¸ ê°œì„ ê³¼ ê´€ë ¨í•˜ì—¬ íŒ€ ë‚´ì—ì„œ ë…¼ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ëª¨ë¸ ìª½ì€ ì €ì˜ ì¼ì´ê¸° ë•Œë¬¸ì—, ì œê°€ ìƒê°í•´ë‘” ë°”ë¡œëŠ”&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;plotì˜ &lt;strong&gt;ì¢…ë¥˜ë¥¼ ë‹¤ì–‘í™”&lt;/strong&gt;í•œë‹¤.&lt;/li&gt;
  &lt;li&gt;plotì˜ &lt;strong&gt;í…Œë§ˆ&lt;/strong&gt;ë¥¼ ë‹¤ë¥´ê²Œ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ í•œë‹¤.&lt;/li&gt;
  &lt;li&gt;ë” ë§ì€ &lt;strong&gt;aggregation&lt;/strong&gt; ê¸°ëŠ¥ì„ ì¶”ê°€í•œë‹¤.&lt;/li&gt;
  &lt;li&gt;(ë³¸ì§ˆì ìœ¼ë¡œëŠ” Vega-lite ë§ê³  &lt;del&gt;&lt;strong&gt;plotly&lt;/strong&gt;&lt;/del&gt;ë¡œ ê°€ê³  ì‹¶ë‹¤ëŠ” ìƒê°ì´ ìˆìŒâ€¦) : ë‹¤ì–‘í™”ì‹œí‚¤ê¸° ìˆ˜ì›”í•´ë³´ì—¬ì„œ
ë“±ì´ ìˆìŠµë‹ˆë‹¤.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Capstone" /><category term="Visualization" /><category term="Recommendation" /><summary type="html">ì´í™”ì—¬ëŒ€ 2021-1í•™ê¸° ìº¡ìŠ¤í†¤ë””ìì¸í”„ë¡œì íŠ¸B ìŠ¤íƒ€íŠ¸7íŒ€ Ewha visualization Recommendation Program(ERP) ê¸°ìˆ  íŠœí† ë¦¬ì–¼ì— ê´€í•œ ê¸€ì…ë‹ˆë‹¤. ë³¸ í¬ìŠ¤íŒ…ì€ Data2Vis ë…¼ë¬¸ì— ì œì‹œëœ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì— ì•Œë§ì€ Plotì„ ìë™ìœ¼ë¡œ ì¶”ì²œí•´ì£¼ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.</summary></entry><entry><title type="html">sudo ê¶Œí•œì´ ì—†ëŠ” ì„œë²„ì—ì„œ git-lfs ì‚¬ìš©í•˜ê¸°</title><link href="http://localhost:4000/troubleshooting/2021/05/19/git_lfs/" rel="alternate" type="text/html" title="sudo ê¶Œí•œì´ ì—†ëŠ” ì„œë²„ì—ì„œ git-lfs ì‚¬ìš©í•˜ê¸°" /><published>2021-05-19T00:00:00+09:00</published><updated>2021-05-19T00:00:00+09:00</updated><id>http://localhost:4000/troubleshooting/2021/05/19/git_lfs</id><content type="html" xml:base="http://localhost:4000/troubleshooting/2021/05/19/git_lfs/">&lt;p&gt;How to Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git-lfs&lt;/code&gt; without sudo (sudo ê¶Œí•œì´ ì—†ëŠ” ì„œë²„ì—ì„œ git-lfs ì‚¬ìš©í•˜ê¸°)&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(base) ~$ wget https://github.com/git-lfs/git-lfs/releases/download/v2.10.0/git-lfs-linux-386-v2.10.0.tar.gz
(base) ~$ tar -xvf git-lfs-linux-386-v2.10.0.tar.gz
README.md
CHANGELOG.md
git-lfs
install.sh


(base) ~$ chmod 755 install.sh
(base) ~$ vi ./install.sh # --&amp;gt; prefix ë³€ìˆ˜ë¥¼ í˜„ì¬ ê¶Œí•œë°›ì€ ê²½ë¡œë¡œ ë³€ê²½
(base) ~$ bash ./install.sh
Git LFS initialized.

(base) ~$ git lfs install
(base) ~$ git lfs pull
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finish&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="git" /><summary type="html">How to Use git-lfs without sudo (sudo ê¶Œí•œì´ ì—†ëŠ” ì„œë²„ì—ì„œ git-lfs ì‚¬ìš©í•˜ê¸°)</summary></entry><entry><title type="html">Mac OS BigSur 11.2.3 â€œbundle exec jekyll serveâ€ Error</title><link href="http://localhost:4000/troubleshooting/2021/05/12/BigSur_Jekyll_TroubleShooting/" rel="alternate" type="text/html" title="Mac OS BigSur 11.2.3 &quot;bundle exec jekyll serve&quot; Error" /><published>2021-05-12T00:00:00+09:00</published><updated>2021-05-12T00:00:00+09:00</updated><id>http://localhost:4000/troubleshooting/2021/05/12/BigSur_Jekyll_TroubleShooting</id><content type="html" xml:base="http://localhost:4000/troubleshooting/2021/05/12/BigSur_Jekyll_TroubleShooting/">&lt;p&gt;How to Solve â€œMac OS BigSur 11.2.3 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle exec jekyll serve&lt;/code&gt; Errorâ€&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;problem&quot;&gt;Problem&lt;/h1&gt;

&lt;h2 id=&quot;1-problem-recognition&quot;&gt;1. Problem Recognition&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/117841944-6dc0a380-b2b8-11eb-8ff4-43268452b53e.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;í‰ì†Œì™€ ê°™ì´ github ë¸”ë¡œê·¸ ê¸€ì„ ì“°ê³  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle exec jekyll serve&lt;/code&gt; ë¥¼ í„°ë¯¸ë„ì— ì…ë ¥í–ˆëŠ”ë° ì•„ë˜ì™€ ê°™ì€ ì—ëŸ¬ê°€ ëœ¸&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Could not find commonmarker-0.17.13 in any of the sources.&lt;br /&gt;
Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle install&lt;/code&gt; to install missing gems.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;2-to-solve-1-bundle-install-or-install-commonmarker-directly-but-&quot;&gt;2. To solve #1., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle install&lt;/code&gt; or install commonmarker directlyâ€¦ But :(&lt;/h2&gt;

&lt;p&gt;I didnâ€™t screenshot these commandsâ€™ error messageâ€¦ but if you get error message like&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/117842511-f8090780-b2b8-11eb-9237-f48769da9548.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(Gem::FilePermissionError)&lt;br /&gt;
    You donâ€™t have write permissions for the /Library/Ruby/Gems/2.X.0 directory.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Then, it must be installed &lt;strong&gt;rbenv ruby-build&lt;/strong&gt;.&lt;/p&gt;

&lt;h1 id=&quot;solution&quot;&gt;Solution&lt;/h1&gt;
&lt;h2 id=&quot;1-brew-install-rbenv-ruby-build&quot;&gt;1. brew install rbenv ruby-build&lt;/h2&gt;
&lt;p&gt;If you installed sucessfully, then go to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;number 3&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;But if you got another problem like meâ€¦&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/117843531-dbb99a80-b2b9-11eb-831b-12e75ffd770b.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Error: Your CLT does not support macOS 11. &lt;br /&gt;
It is either outdated or was modified. &lt;br /&gt;
Please update your CLT or delete it if no updates are available. &lt;br /&gt;
Update them from Software Update in System Preferences or run:&lt;br /&gt;
â€¦&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It is probably the effect of the &lt;strong&gt;Big Sur update&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;So I solved this problem by â€¦&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/117843903-34893300-b2ba-11eb-95f5-7ab80b0c42f2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;sudo rm -rf /Library/Developer/CommandLineTools&lt;br /&gt;
sudo xcode-select â€“install&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ref)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://apple.stackexchange.com/questions/401899/-homebrew-your-clt-does-not-support-macos-11-0&quot;&gt;https://apple.stackexchange.com/questions/401899/-homebrew-your-clt-does-not-support-macos-11-0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://flaviocopes.com/how-to-fix-clt-support-macos-11/&quot;&gt;https://flaviocopes.com/how-to-fix-clt-support-macos-11/&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;ì—¬ê¸°ì—ìˆëŠ” ì‚¬ì§„ë“¤ì´ ì •ìƒì ìœ¼ë¡œ ë‚˜ì˜¤ë©´ OKì¸ë“¯.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-follow-this-blogs-stepssource-zshrc&quot;&gt;2. Follow &lt;a href=&quot;https://popcorn16.tistory.com/56&quot;&gt;this&lt;/a&gt; blogâ€™s steps(~source ~/.zshrc)&lt;/h2&gt;

&lt;h2 id=&quot;3-bundle-install&quot;&gt;3. bundle install&lt;/h2&gt;

&lt;p&gt;Finally, I installed commonmarker successfully.&lt;/p&gt;

&lt;h2 id=&quot;4-bundle-exec-jekyll-serve&quot;&gt;4. bundle exec jekyll serve&lt;/h2&gt;

&lt;p&gt;Finish!&lt;/p&gt;

&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;ì–´ëŠ ë‹¨ê³„ì¸ì§€ ê¸°ì–µì€ ì•ˆë‚˜ëŠ”ë° Gemfileì— 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gem 'commonmarker'&lt;/code&gt;
ë¥¼ ì¶”ê°€í•˜ê¸°ëŠ” í–ˆìŒ. (ì´ê²Œ í•„ìˆ˜ì¸ì§€ëŠ”â€¦ ìˆœì„œê°€ ë’¤ì£½ë°•ì£½ì´ë¼ ëª¨ë¥´ê² ìŒ, ê·¼ë° bundle instasllë¡œ ë‹¤ ë˜ì§€ ì•Šì•˜ì„ê¹Œ ì‹¶ìŒ. Bundle exec Jekyll serve ë¥¼ í–ˆì„ ë•Œ 1ë²ˆê³¼ ê°™ì€ ì—ëŸ¬ê°€ ëœ¬ë‹¤ë©´ ê·¸ ë•Œ Gemfileì— ì¨ë„ ëŠ¦ì§€ ì•Šì„ ë“¯.)&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yeonsoo Kim</name></author><category term="jekyll" /><summary type="html">How to Solve â€œMac OS BigSur 11.2.3 bundle exec jekyll serve Errorâ€</summary></entry><entry><title type="html">Data2Vis - Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural Networks</title><link href="http://localhost:4000/paper%20review/2021/05/06/Data2Vis/" rel="alternate" type="text/html" title="Data2Vis - Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural Networks" /><published>2021-05-06T00:00:00+09:00</published><updated>2021-05-06T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2021/05/06/Data2Vis</id><content type="html" xml:base="http://localhost:4000/paper%20review/2021/05/06/Data2Vis/">&lt;p&gt;Data2Vis: Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural Networks ì„ ì½ê³  ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;data2vis-automatic-generation-of-data-visualizations-using-sequence-to-sequence-recurrent-neural-networks&quot;&gt;Data2Vis: Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural Networks&lt;/h1&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;end-to-end trainable neural translation model&lt;/li&gt;
  &lt;li&gt;formulate visualization generation as &lt;strong&gt;a language translation problem&lt;/strong&gt;, where data specifications are mapped to visualization specifications in a declarative language &lt;strong&gt;(Vega-Lite)&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;Vege-Lite -&amp;gt; JSON format&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;multilayered ateention-based encoder-decoder network with LSTM&lt;/li&gt;
  &lt;li&gt;introduce 2 metrics - language syntax validity, visualization grammar syntax validity&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&quot;declarative-visualization-specification&quot;&gt;Declarative Visualization Specification&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;One of our aims with Data2Vis is to bridge this gap between the speed and expressivity in specifying visualizations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;automated-visulaization&quot;&gt;Automated Visulaization&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;We pose visualization specifica- tion as a machine translation problem and intro- duce Data2Vis, a deep neural translation model trained to automatically translate data specifica- tions to visualization specifications. Data2Vis emphasizes the creation of visualizations using rules learned from examples, without resorting to a predefined enumeration or extraction of con- straints, rules, heuristics, and features.&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Machine Translation Problem&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;dnns-for-machine-translation&quot;&gt;DNNs for Machine Translation&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;Data2Vis is also a sequence- to-sequence model using the textual source and target specifications directly for translation, with- out relying on explicit syntax representations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;p&gt;&lt;img width=&quot;754&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-05-07 á„‹á…©á„Œá…¥á†« 1 43 49&quot; src=&quot;https://user-images.githubusercontent.com/48315997/117335052-adad1280-aed5-11eb-9188-0b30c1cb9533.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the data visualization problem as a &lt;strong&gt;Seq2Seq translation problem&lt;/strong&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input : dataset (fields, values in JSON format)
output : valid Vega-Lite visualization specification
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;encoder-decoder archi.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;where the encoder reads and encodes a source sequence into a fixed length vector, and a decoder outputs a translation based on this vec- tor.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Attention&lt;/strong&gt; Mechanism
    &lt;blockquote&gt;
      &lt;p&gt;Atten- tion mechanisms allow a model to focus on aspects of an input sequence while generating out- put tokens.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Beam Search algorithm&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The beam search algorithm used in sequence-to-sequence neural translation models keeps track of k most probable output tokens at each step of decoding, where k is known as the beamwidth. This enables the generation of k most likely output sequences for a given input sequence.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;THREE techniques : &lt;strong&gt;bidirectional encoding, differential weighing of context via an attention mechanism, and beam search&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;character&lt;/strong&gt;-based sequence model&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data-and-preprocessing&quot;&gt;Data and Preprocessing&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;the model must select a subset of fields to focus on when creating visual- izations (most datasets have multiple fields that cannot all be simultaneously visualized)&lt;/li&gt;
  &lt;li&gt;the model must learn differences in data types across the data fields (numeric, string, temporal, ordinal, categorical, etc.), which in turn guides how each field is specified in the generation of a visualiza- tion specification.&lt;/li&gt;
  &lt;li&gt;the model must learn the appropriate transformations to apply to a field given its data type (e.g., aggregate transform does not apply to string fields).&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;view-level transforms : aggregate, bin, calculate, filter, timeUnit&lt;/li&gt;
  &lt;li&gt;field-level transforms : aggregate, bin, sort, timeUnit&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluation-metrics&quot;&gt;Evaluation Metrics&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;language syntax validity(lsv)
    &lt;ul&gt;
      &lt;li&gt;measure of how well a model learns the syntax of the underlying language used to specify the visualization.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;grammar syntax validity(gsv)
    &lt;ul&gt;
      &lt;li&gt;a measure of how well a model learns the syntax of the grammar for visualization specification.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;
&lt;p&gt;&lt;img width=&quot;887&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-05-07 á„‹á…©á„Œá…¥á†« 1 49 00&quot; src=&quot;https://user-images.githubusercontent.com/48315997/117335706-66735180-aed6-11eb-90cc-95b38ea1224a.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;765&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-05-07 á„‹á…©á„Œá…¥á†« 1 49 10&quot; src=&quot;https://user-images.githubusercontent.com/48315997/117335723-6bd09c00-aed6-11eb-8c0e-9e9d0e1348a2.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Field Selection and Transformation&lt;/li&gt;
  &lt;li&gt;Training Data&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Training Data and Training Strategy&lt;/li&gt;
  &lt;li&gt;Extending Data2Vis to Generate Multiple Plausible Visualizations&lt;/li&gt;
  &lt;li&gt;Targeting Additional Grammars&lt;/li&gt;
  &lt;li&gt;Natural Language and Visualization Specification&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Visualization" /><summary type="html">Data2Vis: Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural Networks ì„ ì½ê³  ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.</summary></entry><entry><title type="html">VizML - A Machine Learning Approach to Visualization Recommendation</title><link href="http://localhost:4000/paper%20review/2021/05/06/VizML/" rel="alternate" type="text/html" title="VizML - A Machine Learning Approach to Visualization Recommendation" /><published>2021-05-06T00:00:00+09:00</published><updated>2021-05-06T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2021/05/06/VizML</id><content type="html" xml:base="http://localhost:4000/paper%20review/2021/05/06/VizML/">&lt;p&gt;VizML : A Machine Learning Approach to Visualization Recommendation ì„ ì½ê³  ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;vizml--a-machine-learning-approach-to-visualization-recommendation&quot;&gt;VizML : A Machine Learning Approach to Visualization Recommendation&lt;/h1&gt;

&lt;p&gt;&lt;img width=&quot;600&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-05-06 á„‹á…©á„’á…® 11 37 45&quot; src=&quot;https://user-images.githubusercontent.com/48315997/117316985-1095ae00-aec4-11eb-9934-cc9110acc841.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;ML approach to visualization recommendation&lt;/li&gt;
  &lt;li&gt;learns visualization design choices from a large corpus of datasets and associated visualization
    &lt;ul&gt;
      &lt;li&gt;identify five key design choices (viz. type, encoding type, â€¦)&lt;/li&gt;
      &lt;li&gt;train models to predict these design choices using 1M dataset-viz. pairs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;NN predicts well&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problem-formulation&quot;&gt;Problem Formulation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;representation&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;are specified using &lt;strong&gt;encodings&lt;/strong&gt; that map from data to the retinal properties(position, length, color) of graphical marks(points, lines, rectangles)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;433&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-05-06 á„‹á…©á„’á…® 11 41 19&quot; src=&quot;https://user-images.githubusercontent.com/48315997/117317548-90237d00-aec4-11eb-8a08-fac0d1997ced.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;That is, to create basic visualizations in many grammars or tools, &lt;strong&gt;an analyst specifes higher-level design choices&lt;/strong&gt;, which we defne as statements that compactly and uniquely specify a bundle of lower-level encodings. Equivalently, each gram- mar or tool affords a design space of visualizations, which a user constrains by making choices.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Trained with a corpus of datasets ${d}$ and corresponding design choices ${C}$, ML-based recommender systems treat recommendation as an optimization problem,such that predicted $ C_{rec} âˆ¼ C_{max}. $&lt;/p&gt;

&lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&quot;rule-based&quot;&gt;Rule-based&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;encode visualization guidelines as collection of â€œif-thenâ€ statements or &lt;strong&gt;rules.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;sometimes effective, but high cost&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ml-based&quot;&gt;ML-based&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;learn the relationship between data and visualizations by training models on analyst interaction&lt;/li&gt;
  &lt;li&gt;DeepEye, Data2Vis, Draco-Learn
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;do not learn to make visualization design choices&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;trained with annotations on rule-generated visualizations in controlled settings -&amp;gt; limit&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DeepEye
    &lt;ul&gt;
      &lt;li&gt;combines rule-based visualization generation with models trained to 1) classify Good/Bad 2) rank lists of viz.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;learning to rank&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Data2Vis
    &lt;ul&gt;
      &lt;li&gt;Seq2Seq Model that maps JSON-encoded datasets to Vega-lite visualization specifications
        &lt;blockquote&gt;
          &lt;p&gt;Vega and Vega-Lite are visualization tools implementing a grammar of graphics, similar to ggplot2.&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
      &lt;li&gt;4300 automatically generated Vega-Lite ex.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Draco-Learn
    &lt;ul&gt;
      &lt;li&gt;represents 1) visualizations as logical facts 2)design guidelines as hard and soft constraints, SVM&lt;/li&gt;
      &lt;li&gt;recommends visualizations that satisfy these constraints&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VizML&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;In terms of &lt;strong&gt;LEARNING TASK&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;DeepEye learns to classify and rank visualizations&lt;/li&gt;
          &lt;li&gt;Data2Vis learns an &lt;strong&gt;end-to-end&lt;/strong&gt; generation model&lt;/li&gt;
          &lt;li&gt;Draco-Learn learns soft constraints weights&lt;/li&gt;
          &lt;li&gt;By learning to &lt;strong&gt;predict design choices&lt;/strong&gt;, &lt;strong&gt;VizML models are easier to quantitatively validate&lt;/strong&gt;, provide interpretable measures of feature importance, and can be more easily integrated into visualization systems.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;In terms of &lt;strong&gt;DATA QUANTITY&lt;/strong&gt; â€¦&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;BUT 3 ML-Based systems recommend &lt;strong&gt;both data queries and visual encodings&lt;/strong&gt;, while VizML only recommends &lt;strong&gt;the latter.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt;
&lt;h3 id=&quot;feature-extracting&quot;&gt;Feature Extracting&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/mitmedialab/vizml/blob/b36310106791927eaef3831a0cda7abcec598999/feature_extraction/extract.py&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;453&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-05-06 á„‹á…©á„’á…® 11 57 01&quot; src=&quot;https://user-images.githubusercontent.com/48315997/117319963-c235de80-aec6-11eb-9a3f-f3e8bfff428f.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We map each dataset to 841 features, mapped from 81 single- column features and 30 pairwise-column features using 16 aggregation functions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Each Col. -&amp;gt; 81 single-column features across four categories&lt;/li&gt;
  &lt;li&gt;Dimension(D) feature = # of rows in col.&lt;/li&gt;
  &lt;li&gt;Types(T) feature = categorical/temporal/quantitative&lt;/li&gt;
  &lt;li&gt;Values(V) feature = the statistical &amp;amp; structural properties of the values within a col.&lt;/li&gt;
  &lt;li&gt;Names(N) feature = column name&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;We distinguish between these feature categories for three reasons.&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;First, these categories let us &lt;strong&gt;organize how we create and interpret features.&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;Second, we can observe the contribution of diferent types of features.&lt;/li&gt;
    &lt;li&gt;Third, some categories of features may be less generalizable than others.&lt;/li&gt;
    &lt;li&gt;We order these categories &lt;strong&gt;(D â†’ T â†’ V â†’ N)&lt;/strong&gt; by how biased we expect those features to be towards the Plotly corpus.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;We create 841 dataset-level features by aggregating these single- and pairwise-column features using the 16 ag- gregation functions&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;design-choice-extraction&quot;&gt;Design Choice Extraction&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;Examples of encoding-level design choices include mark type, such as scatter, line, bar; and X or Y column encoding, which specifes which column is represented on which axis; and whether or not an X or Y column is the single column represented along that axis.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;By aggregating these &lt;strong&gt;encoding-level design choices&lt;/strong&gt;, we can characterize &lt;strong&gt;visualization-level design choices&lt;/strong&gt; of a chart&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;
&lt;h3 id=&quot;feature-preprocessing&quot;&gt;Feature Preprocessing&lt;/h3&gt;
&lt;h3 id=&quot;prediction-tasks&quot;&gt;Prediction Tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Two visualization-level prediction tasks&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Dataset-level features to predict visualization-level design&lt;/li&gt;
      &lt;li&gt;1) Visualization Type[VT]&lt;/li&gt;
      &lt;li&gt;2) Has Shared Axis [HSA]&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Three encoding-level prediction tasks&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;use features about individual columns to predict how theay are visually encoded&lt;/li&gt;
      &lt;li&gt;consider col. indep.&lt;/li&gt;
      &lt;li&gt;1) Mark Type[MT]&lt;/li&gt;
      &lt;li&gt;2) Is Shared X-axis or Y-axis [ISA]&lt;/li&gt;
      &lt;li&gt;3) Is on X-axis or Y-axis [XY]&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;For the &lt;strong&gt;VT, MT&lt;/strong&gt; tasks, the 2- class task predicts line vs. bar, and the 3-class predicts scatter vs. line vs. bar.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;neural-network-and-baseline-models&quot;&gt;Neural Network and Baseline Models&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;In terms of features, we constructed four diferent feature sets by incrementally adding the Dimensions (D), Types (T), Values (V), and Names (N) categories of features, in that order. We refer to these feature sets as &lt;strong&gt;D, D+T, D+T+V, and D+T+V+N=All&lt;/strong&gt;. The neural network was trained and tested using all four feature sets independently. The four base- line models only used the full feature set (D+T+V+N=All).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;additional-studies&quot;&gt;Additional Studies&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;DeepEye: Towards Automatic Data Visualization.&lt;/li&gt;
  &lt;li&gt;Data2Vis: Automatic Generation of Data Vi-
sualizations Using Sequence to Sequence Recurrent Neural Networks.&lt;/li&gt;
  &lt;li&gt;Draco-Learn : Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco.&lt;/li&gt;
  &lt;li&gt;Vega- Lite: A Grammar of Interactive Graphics.&lt;/li&gt;
  &lt;li&gt;Polaris: a system for query, analysis, and visualization of multidimensional databases&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Machine Learning" /><category term="Visualization" /><summary type="html">VizML : A Machine Learning Approach to Visualization Recommendation ì„ ì½ê³  ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.</summary></entry><entry><title type="html">Stanford CS231n Lec 02. Image Classification</title><link href="http://localhost:4000/cs231n/2021/03/21/cs231n_lec02/" rel="alternate" type="text/html" title="Stanford CS231n Lec 02. Image Classification" /><published>2021-03-21T00:00:00+09:00</published><updated>2021-03-21T00:00:00+09:00</updated><id>http://localhost:4000/cs231n/2021/03/21/cs231n_lec02</id><content type="html" xml:base="http://localhost:4000/cs231n/2021/03/21/cs231n_lec02/">&lt;p&gt;Stanford CS231n 2017 ê°•ì˜ë¥¼ ë“£ê³  ê°œì¸ì ìœ¼ë¡œ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;lecture-2--image-classification-pipeline&quot;&gt;Lecture 2 : Image Classification pipeline&lt;/h1&gt;
&lt;h2 id=&quot;image-classification&quot;&gt;Image Classification&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Computer Vision Task&lt;/li&gt;
  &lt;li&gt;Problem is â€¦
    &lt;ul&gt;
      &lt;li&gt;Semantic Gap : between image and pixels (what the computer sees)
        &lt;ul&gt;
          &lt;li&gt;Computer understands the image as a big grid of numbers (800,600,3)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Challenges (algorithm should be robust to these challenges)
    &lt;ul&gt;
      &lt;li&gt;Viewpoint variation
        &lt;ul&gt;
          &lt;li&gt;all pixels change when the viewpoint is changed&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Illumination
        &lt;ul&gt;
          &lt;li&gt;different light condition&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Deformation
        &lt;ul&gt;
          &lt;li&gt;Example : catâ€¦&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Occlusion
        &lt;ul&gt;
          &lt;li&gt;The image shows just â€œpartâ€ of a cat&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Background Cluttuer&lt;/li&gt;
      &lt;li&gt;I track as Variation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;image-classifier&quot;&gt;Image Classifier&lt;/h2&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;classify_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;# some magic!
&lt;/span&gt;	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_label&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;Attempts have been made
    &lt;ul&gt;
      &lt;li&gt;find edges and corners&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data-Driven Approach&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Collect a dataset of images and labels&lt;/li&gt;
      &lt;li&gt;Use ML to train a classifier&lt;/li&gt;
      &lt;li&gt;Evaluate the classifier on new images
        &lt;h2 id=&quot;first-classifier--nearest-neighbor&quot;&gt;First Classifier : &lt;strong&gt;Nearest Neighbor&lt;/strong&gt;&lt;/h2&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;train : &lt;strong&gt;memorize all training data&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;O(1)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Predict : predict &lt;strong&gt;the label of most similar training image&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;O(N)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;But we want classifier that are fast at prediction; slow for training is OK.&lt;/strong&gt;**&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;K-Nearest Nighbors&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Take &lt;strong&gt;majority vote&lt;/strong&gt; from K closest points
  &lt;img src=&quot;https://user-images.githubusercontent.com/48315997/112023051-36fec480-8b76-11eb-819a-447974d99b0c.png&quot; alt=&quot;image&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Distance Metric (to compare images)
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/112022917-12a2e800-8b76-11eb-93f5-c1a1dd4cc39d.png&quot; alt=&quot;image&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;L1 distance(Manhattan distance)
        &lt;ul&gt;
          &lt;li&gt;Calculate the difference of image&lt;/li&gt;
          &lt;li&gt;Depends on &lt;strong&gt;choice of coordinate system&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;Use when individual vector is meaningful&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;L2 distance(Euclidean distance)
        &lt;ul&gt;
          &lt;li&gt;Use when &lt;strong&gt;generic vector&lt;/strong&gt; in some space&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;kNN on images never used&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Very slow at test time&lt;/li&gt;
      &lt;li&gt;Distance metrics on pixels are not informative
        &lt;ul&gt;
          &lt;li&gt;couldnâ€™t reflected â€œperceptional distanceâ€&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Curse of dimensionality
  &lt;img src=&quot;https://user-images.githubusercontent.com/48315997/112023009-2f3f2000-8b76-11eb-85b2-5ad0eba72979.png&quot; alt=&quot;image&quot; /&gt;
        &lt;ul&gt;
          &lt;li&gt;If dimensions are increased in image, data points must densely cover to these dimensions&lt;/li&gt;
          &lt;li&gt;Training examples are exponentially needed.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hyperparameters--pipeline&quot;&gt;Hyperparameters &amp;amp; Pipeline&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Problem-dependent
    &lt;ul&gt;
      &lt;li&gt;try them all out and see what works best â€¦&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Split data into &lt;strong&gt;train, val, and test&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;underlying : the same probability distribution&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cross-Validation
    &lt;ul&gt;
      &lt;li&gt;Split data into &lt;strong&gt;folds&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;We can know which hyperparameters are going to perform more &lt;strong&gt;robustly&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Useful to small datasets -&amp;gt; not used too frequently in deep learning&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;linear-classification&quot;&gt;Linear Classification&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Parametric Approach : summarize knowledge of training examples &amp;amp; stick all that knowledge into W.&lt;/li&gt;
  &lt;li&gt;Image -&amp;gt; f(x,W) -&amp;gt; N numbers giving class scores
    &lt;ul&gt;
      &lt;li&gt;W : parameters or weights&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;f(x,W) = Wx + b (# of classes = 10, input dimension = 3072)
    &lt;ul&gt;
      &lt;li&gt;f(x,W) : 10x1&lt;/li&gt;
      &lt;li&gt;W should be 10 x 3072&lt;/li&gt;
      &lt;li&gt;x : 3072x1&lt;/li&gt;
      &lt;li&gt;b : 10x1&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Wx&lt;/code&gt; gives classesâ€™ scores&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Bias
    &lt;ul&gt;
      &lt;li&gt;constant vector&lt;/li&gt;
      &lt;li&gt;Not interact with training set&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;data independent&lt;/strong&gt;, preferences for some classes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overview
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/112023440-98bf2e80-8b76-11eb-837b-fb2bc8758a44.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Interpretation of linear classifiers as &lt;strong&gt;template matching&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;1 class : 1 template (driven from training data)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hard cases for a linear classifier&lt;/li&gt;
  &lt;li&gt;Question
    &lt;ul&gt;
      &lt;li&gt;how can we tell &lt;strong&gt;whether this W is good or bad?&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yeonsoo Kim</name></author><category term="cs231n" /><category term="Deep Learning" /><category term="lecture note" /><category term="Computer Vision" /><summary type="html">Stanford CS231n 2017 ê°•ì˜ë¥¼ ë“£ê³  ê°œì¸ì ìœ¼ë¡œ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.</summary></entry><entry><title type="html">ì§€ë‚œ í•œ ë‹¬ì„ ëŒì•„ë³´ë©°, 2021ë…„ 2ì›” íšŒê³ </title><link href="http://localhost:4000/logs/2021/03/14/2021_February/" rel="alternate" type="text/html" title="ì§€ë‚œ í•œ ë‹¬ì„ ëŒì•„ë³´ë©°, 2021ë…„ 2ì›” íšŒê³ " /><published>2021-03-14T00:00:00+09:00</published><updated>2021-03-14T00:00:00+09:00</updated><id>http://localhost:4000/logs/2021/03/14/2021_February</id><content type="html" xml:base="http://localhost:4000/logs/2021/03/14/2021_February/">&lt;p&gt;2021ë…„ 2ì›” íšŒê³  ê¸€ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;ëª©ì°¨&quot;&gt;ëª©ì°¨&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;ETRI ì¸í„´ ì¢…ë£Œ&lt;/li&gt;
  &lt;li&gt;ì´í™”ë³´ì´ìŠ¤ ì¸í„°ë·°&lt;/li&gt;
  &lt;li&gt;AI/ë°ì´í„°ë¶„ì„ ë™ì•„ë¦¬ ê°œì„¤&lt;/li&gt;
  &lt;li&gt;3ì›”ì˜ ê°ì˜¤&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;etri-ì¸í„´-ì¢…ë£Œ&quot;&gt;ETRI ì¸í„´ ì¢…ë£Œ&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/111064641-7edb7700-84f8-11eb-9dfb-1e5add8fa87d.jpeg&quot; alt=&quot;DD64C628-9482-43AE-B258-D6D83EF86E7A_1_105_c&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ë²Œì¨ ë‘ ë‹¬ì´ ì§€ë‚˜ ì—°êµ¬ì—°ìˆ˜ìƒ ê¸°ê°„ì´ ëë‚¬ë‹¤. ì‹œê°„ì€ ë‚´ ìƒê°ë³´ë‹¤ë„ ë” ìœì‚´ê°™ì´ í˜ëŸ¬ê°„ë‹¤ëŠ” ê±¸ ëª¸ì†Œ ëŠë‚„ ìˆ˜ ìˆì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ê²°ë¡ ì ìœ¼ë¡œ ë§í•˜ë©´ ETRI ì¸í„´ì„ í•˜ê¸¸ ì°¸ ì˜í–ˆë‹¤.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ì¢‹ì€ ë™ê¸°ë“¤, ë©‹ìˆëŠ” ë°•ì‚¬ë‹˜ë“¤ê³¼ ì„ ë°°ë‹˜ë“¤, ì´  ëª¨ë‘ì™€ í•¨ê»˜ í•  ìˆ˜ ìˆì–´ ì˜ê´‘ì´ì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë˜ ê·¸ë™ì•ˆ ì˜ ëª°ëê¸°ì— ê´€ì‹¬ ì—†ë˜ ë¶„ì•¼ì— ëŒ€í•´ ìƒˆë¡­ê²Œ ì•Œê²Œ ë˜ëŠ” ì¾Œê°ì„ ëŠë‚„ ìˆ˜ ìˆì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;íŠ¹í—ˆëŠ” ì‘ì„±í–ˆê³  ë…¼ë¬¸ì€ ì‘ì„±í• ì§€ ì•ˆí• ì§€ ë¯¸ì§€ìˆ˜ì¸ ìƒíƒœë¡œ ë– ë‚¬ë‹¤. ë…¼ë¬¸ì„ ë‹¹ì—°íˆ ì¨ì•¼ëœë‹¤ê³  ìƒê°í–ˆì§€ë§Œ ì´ê²ƒì €ê²ƒ ìš°ë ¤ë˜ëŠ” ë¶€ë¶„ë“¤ì´ ìˆì–´ì„œ ì§€ê¸ˆë„ ê³ ë¯¼ ì¤‘ì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ë™ê¸°ë“¤í•œí…Œ íŠ¹íˆë‚˜ ê³ ë§ˆì› ë‹¤. ë¯¸ìˆ™í•˜ê³  íˆ´íˆ´ê±°ë¦¬ëŠ” ë‚´ ì„±ê²©ì„ ì¬ë°Œê²Œ ì˜ ë°›ì•„ì£¼ê³  ë˜ ì„œë¡œ ì—´ì •ì ì´ë¼ ëˆ„êµ¬ í•˜ë‚˜ ë‚™ì˜¤ë˜ì§€ ì•Šê³  ì •ë§ ì—´ì‹¬íˆ ì¼í•  ìˆ˜ ìˆì—ˆë‹¤. ë‚´ê°€ ì•„ëŠ” ETRI ì¸í„´ë“¤, ë‹¤ë¥¸ ë© ì—°êµ¬ì—°ìˆ˜ìƒë“¤ ì¤‘ì— ìš°ë¦¬ê°€ ê°€ì¥ ë°”ì˜ê²Œ ì¼í•˜ì§€ ì•Šì•˜ì„ê¹Œ ì‹¶ë‹¤. ì•¼ê·¼ê³¼ ì£¼ë§ ì¶œê·¼ì„ ìì£¼ í–ˆìœ¼ë‹ˆâ€¦ (ìš°ë¦¬ë¼ë¦¬ ë‹¤ìŒ ê¸°ìˆ˜ ì¸í„´ë“¤ì´ ë¶ˆìŒí•˜ë‹¤ê³  ê³„ì† ë§í–ˆìŒã…‹ã…‹)&lt;/p&gt;

&lt;p&gt;íŠ¹í—ˆë‚˜ ë…¼ë¬¸ì„ ë‚´ê¸°ì— ë‘ ë‹¬ì´ë¼ëŠ” ì‹œê°„ì€ ë§¤ìš° ì§§ì€ ì‹œê°„ì´ë¼ê³  ì—¬ê²¨ì¡Œë‹¤. ê·¸ë˜ì„œ ì‚¬ì‹¤ â€˜ì•„, ëª»í•´ë„ ê·¸ë§Œì´ì§€! ë°°ìš´ ê²Œ ë§ìœ¼ë‹ˆê¹Œâ€™ ì´ëŸ° ë¥˜ì˜ ìƒê°ì„ í–ˆì—ˆëŠ”ë° &lt;strong&gt;ìƒê°ë³´ë‹¤ í•  ë§Œí•˜ë‹¤.&lt;/strong&gt; ë°•ì‚¬ë‹˜ë“¤ê»˜ì„œ ì •ë§ ë§ì´ ë„ì™€ì£¼ì‹œê³  ì¸í”„ë¼ë„ ë¹µë¹µí•˜ë‹ˆ ë³¸ì¸ì˜ ì˜ì§€ë§Œ ìˆë‹¤ë©´ ëª» í•  ê¸°ê°„ì´ ì ˆëŒ€ ì•„ë‹ˆì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ETRI ì¸í„´ì— ëŒ€í•œ í›„ê¸°ëŠ” ì•„ì˜ˆ ë‹¤ë¥¸ ê¸€ë¡œ íŒŒì„œ ì“°ë ¤ê³  í•œë‹¤. ì–´ì°Œëë“  ë‚˜ëŠ” ETRI ì¸í„´ì„ ì ê·¹ ì¶”ì²œí•˜ëŠ” ì…ì¥ì´ë‹¤. ì´ ì •ë„ì˜ í™˜ê²½ê³¼ ì‚¬ëŒì„ ì¶©ì¡±ì‹œí‚¤ëŠ” íšŒì‚¬ê°€ ë§ì§€ ì•Šì„ ê±°ë¼ ìƒê°ë˜ê¸° ë•Œë¬¸ì´ë‹¤. ì‚¬ëŒë„ ì¢‹ê³  ì¼ë„ ì¢‹ë‹¤ë©´ ë§ˆë‹¤í•  ì´ìœ ê°€ ìˆì„ê¹Œ?&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;ì´í™”ë³´ì´ìŠ¤-ì¸í„°ë·°&quot;&gt;ì´í™”ë³´ì´ìŠ¤ ì¸í„°ë·°&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/111064661-9e729f80-84f8-11eb-8f2a-c0f4458f93b5.png&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-03-14 á„‹á…©á„’á…® 7 08 09&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ìš°ì—°ì°®ì€ ê¸°íšŒë¡œ ì´í™”ë³´ì´ìŠ¤ì— ì¸í„°ë·°ë¥¼ í•˜ê²Œ ë˜ì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;í˜„ì¬ ë‚˜ëŠ” ì•„ëŠ” ë™ìƒì´ë‘ ì´í™”ì—¬ëŒ€ AI/ë°ì´í„°ë¶„ì„ ë¶„ì•¼ ì˜¤ì¹´ë°©ê³¼ ë””ìŠ¤ì½”ë“œë¥¼ ê°™ì´ ìš´ì˜í•˜ê³  ìˆëŠ”ë° ì—¬ê¸°ì— ê³„ì‹  ì´í™”ë³´ì´ìŠ¤ ê¸°ìë‹˜ê»˜ì„œ ì½”ë¡œë‚˜ì‹œëŒ€ ë„¤íŠ¸ì›Œí‚¹ì— ëŒ€í•œ ì£¼ì œë¡œ ì¸í„°ë·°ë¥¼ ìš”ì²­í•´ì£¼ì…¨ë‹¤.&lt;/p&gt;

&lt;p&gt;êµ‰ì¥íˆ ì˜ê´‘ì´ì—ˆê³  ì‚¬ì‹¤ ë‚˜ëŠ” ë”±íˆ í•œ ì¼ì´ ì—†ëŠ”ë° ì–´ë–¤ ë‹µë³€ì„ ë“œë ¤ì•¼ í• ê¹Œ ë§ì€ ê³ ë¯¼ì„ í–ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë˜ ì´í™”ë³´ì´ìŠ¤ëŠ” í•™êµ ê³µì‹ ì˜ì ì‹ ë¬¸ì¸ì§€ë¼ ê½¤ ë¶€ë‹´ì´ ëœ ê²ƒë„ ì‚¬ì‹¤ì´ì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;í•˜ì§€ë§Œ ê¸°ìë²—ê»˜ì„œ êµ‰ì¥íˆ ë¶„ìœ„ê¸°ë¥¼ ì˜ í’€ì–´ì£¼ì…¨ê³ , ìš°ë¦¬ê°€ ë‹µë³€í•  ìˆ˜ ìˆì„ ì •ë„ì˜ ì¢‹ì€ ì§ˆë¬¸ë“¤ì„ ì£¼ì…”ì„œ ì¬ë°ŒëŠ” ì¸í„°ë·°ê°€ ê°€ëŠ¥í–ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ìš°ë¦¬ í•™êµ ë„¤íŠ¸ì›Œí‚¹ì˜ ì¥ì„ ì¡°ê¸ˆì´ë¼ë„ ë„“íˆê³  ì‹¶ì—ˆë˜ ë‚˜ì˜ ì‘ì€ ë°”ëŒì´ ì ì°¨ ë§ì€ ì´í™”ì¸ì—ê²Œ ë‹¿ëŠ” ê²ƒ ê°™ì•„ ë¿Œë“¯í–ˆë‹¤. ë˜ ë” ì—´ì‹¬íˆ (ê³µë¶€)í•´ì•¼ ê² ë‹¤ëŠ” ë™ê¸°ë¶€ì—¬ë„ í™•ì‹¤íˆ ëë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ ë„¤íŠ¸ì›Œí‚¹ì˜ ì¥ì´ ë³„ íƒˆì—†ì´, ë¶€ë”” ì˜¤ë˜ ê°ˆ ìˆ˜ ìˆìœ¼ë©´ ì¢‹ê² ë‹¤.&lt;/p&gt;

&lt;p&gt;ê¸°ì‚¬ì˜ ì „ë¬¸ì€ &lt;a href=&quot;https://evoice.ewha.ac.kr/news/articleView.html?idxno=10596&quot;&gt;ì—¬ê¸°&lt;/a&gt;ì„œ ë³¼ ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;aië°ì´í„°ë¶„ì„-ë™ì•„ë¦¬-ê°œì„¤&quot;&gt;AI/ë°ì´í„°ë¶„ì„ ë™ì•„ë¦¬ ê°œì„¤&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/111064679-b813e700-84f8-11eb-929f-9024599f1417.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì‘ë…„ë¶€í„° ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ë¶„ì•¼ í•™íšŒë¥¼ í•˜ë‚˜ ë§Œë“¤ê¹Œ ê³ ë¯¼í•˜ê¸´ í–ˆìœ¼ë‚˜, ì¡¸í”„ ë“± ì‚¬ì • ìƒ ë¶€ë‹´ì´ ë§ì´ ë˜ëŠ” ì¼ì€ ì‚¬ì‹¤ì¸ì§€ë¼ ê°€ëŠ¥í•˜ë©´ í”¼í•˜ë ¤ê³  í–ˆë‹¤. í•˜ì§€ë§Œ ê²°êµ­ ë§Œë“¤ê²Œ ë˜ì—ˆë‹¤â€¦ ã…‹ã…‹ã…‹ã…‹ ì‹¬ì§€ì–´ ì¡¸í”„ ë©”ì´íŠ¸ê¹Œì§€ ê¼¬ì…”ì„œ ê°™ì´~&lt;/p&gt;

&lt;p&gt;ìš°ë¦¬ í•™êµ ì‚¬ëŒë“¤ì´ ì´ ë¶„ì•¼ì— ë” ë§ì´ ì§„ì¶œí•˜ê³ , ë” ë§ì´ ê¿ˆì„ ê¿¨ìœ¼ë©´ ì¢‹ê² ë‹¤ëŠ” ìƒê°ì€ í•­ìƒ ê°–ê³  ìˆì—ˆê¸° ë•Œë¬¸ì— ì´ ë™ì•„ë¦¬ë¥¼ ë°˜ë“œì‹œ ì˜ í‚¤ìš°ê³  ì‹¶ë‹¤. ì‚¬ì‹¤ ì–´ë ¤ìš¸ ê²ƒì´ ì—†ëŠ” ê²Œ ë‚˜í•œí…ŒëŠ” ë“ ë“ í•œ ì„ì›ì§„ë“¤ë„ ìˆê³ , ë˜ ì—´ì • ìˆëŠ” ë¶€ì›ë“¤ê¹Œì§€ ìˆë‹¤. ë‚´ê°€ ë°©í–¥ì„±ë§Œ ìƒì§€ ì•Šìœ¼ë©´ ì¶©ë¶„íˆ ëœë‹¤.&lt;/p&gt;

&lt;p&gt;ë‚´ê°€ ì–´ì—¿í•œ ì‚¬íšŒì¸ì´ ë˜ì–´ í›„ë°°ë‹˜ë“¤ì„ ë„ì™€ì¤„ ìˆ˜ ìˆëŠ” ìœ„ì¹˜ì— ê°ˆ ë•Œê¹Œì§€ ì´ ë™ì•„ë¦¬ê°€ ì­‰ ì´ì–´ì ¸ë‚˜ê°”ìœ¼ë©´ ì¢‹ê² ë‹¤.&lt;/p&gt;

&lt;p&gt;êµ‰ì¥íˆ ë¹¡ì„¼ ì»¤ë¦¬í˜ëŸ¼ìœ¼ë¡œ ì§„í–‰í•˜ëŠ”ë°, íƒˆì£¼ì ì—†ì´ ëª¨ë‘ê°€ ì„±ì¥í•  ìˆ˜ ìˆëŠ” í•œ í•´ê°€ ë˜ê¸¸!&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;3ì›”ì˜-ê°ì˜¤&quot;&gt;3ì›”ì˜ ê°ì˜¤&lt;/h3&gt;

&lt;p&gt;ì‚¬ì‹¤ ì´ ê¸€ì„ ì“°ëŠ” ì‹œì ì´ 3/14ì¼ì´ê¸´ í•œë°â€¦ 3ì›”ì˜ í•  ì¼ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ì¡¸ì—… í”„ë¡œì íŠ¸ ì£¼ì œ í™•ì •&lt;/li&gt;
  &lt;li&gt;ê°•ì˜ ì—´ì‹¬íˆ ë“£ê¸°, ê³¼ì œ ì˜ í•˜ê¸°&lt;/li&gt;
  &lt;li&gt;ë™ì•„ë¦¬ ê³µë¶€ ì˜ í•˜ê¸°&lt;/li&gt;
  &lt;li&gt;ì´êµìˆ˜ë‹˜ ê°•ì˜, ë…¼ë¬¸ ë”°ë¼ ê°€ê¸°&lt;/li&gt;
  &lt;li&gt;ë…¼ë¬¸ ìŠ¤í„°ë””&lt;/li&gt;
  &lt;li&gt;ê±´ê°• ì±™ê¸°ê¸°&lt;/li&gt;
  &lt;li&gt;(ETRI ë…¼ë¬¸ ì‘ì„±?) -&amp;gt; ì™„ì „ ë¯¸ì •.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yeonsoo Kim</name></author><category term="ì›”ê°„ íšŒê³ " /><summary type="html">2021ë…„ 2ì›” íšŒê³  ê¸€ì…ë‹ˆë‹¤.</summary></entry><entry><title type="html">ì§€ë‚œ í•œ ë‹¬ì„ ëŒì•„ë³´ë©°, 2021ë…„ 1ì›” íšŒê³ </title><link href="http://localhost:4000/logs/2021/02/14/2021_January/" rel="alternate" type="text/html" title="ì§€ë‚œ í•œ ë‹¬ì„ ëŒì•„ë³´ë©°, 2021ë…„ 1ì›” íšŒê³ " /><published>2021-02-14T00:00:00+09:00</published><updated>2021-02-14T00:00:00+09:00</updated><id>http://localhost:4000/logs/2021/02/14/2021_January</id><content type="html" xml:base="http://localhost:4000/logs/2021/02/14/2021_January/">&lt;p&gt;2021ë…„ 1ì›” íšŒê³  ê¸€ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;ëª©ì°¨&quot;&gt;ëª©ì°¨&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;ETRI 1ê°œì›” ì°¨&lt;/li&gt;
  &lt;li&gt;ë°ì´í„° ë¶„ì„ ìŠ¤í„°ë”” ì¢…ë£Œ&lt;/li&gt;
  &lt;li&gt;ê¸°íƒ€&lt;/li&gt;
  &lt;li&gt;í•œ ë‹¬ ë™ì•ˆ í•œ ê³µë¶€&lt;/li&gt;
  &lt;li&gt;ë‚¨ì€ ë°©í•™ ë™ì•ˆ í•  ì¼&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;1ï¸âƒ£-etri-1ê°œì›”-ì°¨&quot;&gt;1ï¸âƒ£ ETRI 1ê°œì›” ì°¨&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107854608-48123280-6e60-11eb-9221-aeba67c94e38.jpeg&quot; alt=&quot;E11D4A80-51E2-4D70-8452-DDB5C3B6B70D_1_105_c&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì–´ëŠìƒˆ ETRI ì¸í„´ í•œ ë‹¬ì´ ì§€ë‚¬ë‹¤. (ì§€ê¸ˆì€ ì•½ 2ì£¼ì •ë„ë°–ì— ì•ˆë‚¨ì•˜ì§€ë§Œâ€¦)
í•œ ë‹¬ê°„ í•œ ì¼ì„ ì •ë¦¬í•´ë³´ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Active Learning ê°œë… ê³µë¶€ ë° ë…¼ë¬¸ ë¦¬ë”©&lt;/li&gt;
  &lt;li&gt;Active Learning - Uncertainty Sampling Implementation&lt;/li&gt;
  &lt;li&gt;ì‹¤í—˜ì— í•„ìš”í•œ ë°ì´í„°ë“¤ ë¼ë²¨ë§&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ì´ì „ê¹Œì§€ ë‚˜ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ ìì²´ì— ê´€ì‹¬ì´ ìˆì—ˆë˜ì§€ë¼ Active Learningì´ë‘ Human in the Loop ê°œë…ì„ ì´ë²ˆì— ìƒˆë¡œ ì•Œì•˜ë‹¤.
Active Learningì€ ìš”ì•½í•˜ìë©´ ì ì€ ë°ì´í„° ìˆ˜ë¡œ ë†’ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆëŠ” ê²ƒì´ ëª©ì ì¸ë°, ì´ ë•Œ &lt;strong&gt;ê·¸ ì ì€ ë°ì´í„°ë“¤ì„ ë¬´ìŠ¨ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìƒ˜í”Œë§í•˜ëŠ”ê°€&lt;/strong&gt;ì— ê´€í•œ ì´ì•¼ê¸°ì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ì±…ì„ë‹˜ê»˜ì„œ ë ˆí¼ëŸ°ìŠ¤í•  ë§Œí•œ ë…¼ë¬¸, ì•„í‹°í´ì„ ì£¼ì…”ì„œ ì‹¤í—˜ì˜ ê°ˆí”¼ë¥¼ ë¹ ë¥´ê²Œ ì¡ì„ ìˆ˜ ìˆì—ˆë‹¤.
ë˜ ë°ì´í„°ë¥¼ ë°›ê¸°ê¹Œì§€ ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë ¤ì„œ ê·¸ ë™ì•ˆ Uncertainty Samplingì„ êµ¬í˜„í•˜ê³  ì˜¤í”ˆ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‹¤í—˜í•´ë³´ì•˜ë‹¤.&lt;/p&gt;

&lt;p&gt;1ì›”ì´ ê°€ì¥ í˜ë“¤ì—ˆë˜ ì´ìœ ëŠ” ë¼ë²¨ë§^_^â€¦ ì¸ë°â€¦
ë‚˜ëŠ” ì´ìª½ì˜ ëª¨ë“  ì¼ì„ ì°¸ì—¬í•˜ê¸° ì „ì— ë¼ë²¨ë§ ì¼ì„ í•˜ëŠ”ì§€ ì•ˆí•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ë¼ë²¨ë§ ì—…ë¬´ê°€ ë‚€ë‹¤ë©´ ê·¸ëƒ¥ ì•ˆí•œë‹¤^^â€¦
ê·¸ë˜ì„œ ETRI ì§€ì›í•  ë•Œë„ ë¼ë²¨ë§í•´ì•¼í•  ê²ƒ ê°™ì€ ë¶€ì„œëŠ” ë‹¤ ì œì™¸í•˜ê³  ìƒê°í–ˆëŠ”ë° ã… _ã…  ë‚´ê°€ ì§€ì›í•œ ê³³ì—ì„œ ê°‘ìê¸° ë¼ë²¨ë§ì„ ì‹œí‚¤ì‹¤ ì¤„ì€ ëª°ëë‹¤..!(ì–˜ê¸° ì—†ì—ˆì–ì•„ìš”!!)&lt;/p&gt;

&lt;p&gt;ì•„ë¬´íŠ¼ 2ì£¼ê°„ ë¼ë²¨ë§ì„ í•˜ê³ ,, ë˜ 112 ë°ì´í„°ë¼ ê·¸ëŸ°ì§€ ì •ì‹  í”¼íí•´ì§ˆ ë§Œí•œ ë‚´ìš©ì´ ë§ì•˜ë‹¤ã…‹ã…‹ã…‹ ë‚´ê°€ ë‘”í•˜ê³  ë©˜íƒˆì´ ê´œì°®ì€ í¸ì´ë¼ ë‹¤í–‰ì´ì—ˆì§€, ê°ì •ì´ì… ì˜í•˜ëŠ” ì‚¬ëŒì´ ë´¤ìœ¼ë©´ í˜ë“¤ì—ˆì„ ê²ƒ ê°™ë‹¤.&lt;/p&gt;

&lt;p&gt;2ì›”ë¶€í„°ëŠ” ì‹¤í—˜ì„ ì‹œì‘í–ˆëŠ”ë°, ê·¸ê±´ 2ì›” íšŒê³ ì— ê¸°ë¡í•  ê²ƒ.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;2ï¸âƒ£ë°ì´í„°-ë¶„ì„-ìŠ¤í„°ë””-ì¢…ë£Œ&quot;&gt;2ï¸âƒ£ë°ì´í„° ë¶„ì„ ìŠ¤í„°ë”” ì¢…ë£Œ&lt;/h3&gt;

&lt;p&gt;10ì›”ì— ëª¨ì§‘í•˜ì—¬ ì§„í–‰í–ˆë˜ ë°ì´í„° ë¶„ì„ ìŠ¤í„°ë””ê°€ ë“œë””ì–´ ë§ˆë¬´ë¦¬ ëë‹¤!
í›„ë°˜ë¶€ì—ëŠ” ë‚´ê°€ ì •ì‹ ì´ ì—†ì–´ì„œ ê³µë¶€ë¥¼ ë§ì´ ëª» í–ˆì§€ë§Œã… ã…  ê·¸ë˜ë„ ì¢‹ì€ ë²—ë“¤ì„ ë§Œë‚˜ì„œ ë„ˆë¬´ ê¸°ë¶„ì´ ì¢‹ì•˜ë‹¤ :)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107866777-7a02b380-6eb7-11eb-9dc2-135c04404f49.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;â€˜íŒŒì´ì¬ ë¨¸ì‹ ëŸ¬ë‹ ì™„ë²½ ê°€ì´ë“œâ€™ ë¼ëŠ” ì±…ì€ ë‚´ê°€ ì´ì „ì— í•œ ë²ˆ ì½ì€ ì±…ì´ê¸´ í•˜ì§€ë§Œ, ì´ë ‡ê²Œ ë²—ë“¤ì´ë‘ ë‹¤ì‹œ ì„¸ë¯¸ë‚˜ í˜•íƒœë¡œ ê³µë¶€í•˜ë‹ˆê¹Œ ë˜ ìƒˆë¡­ê²Œ ì™€ë‹¿ì•˜ë‹¤. ê³µë¶€ëŠ” ì—­ì‹œ í•  ë•Œë§ˆë‹¤ ìƒˆë¡œìš´ ê±´ê°€?
ì´ì „ì—ëŠ” ì•™ìƒë¸”, ë¶€ìŠ¤íŒ… ê¸°ë²•ì„ ê¹Šê²Œ ìƒê°í•˜ì§€ ì•Šê³  ê³µë¶€í–ˆëŠ”ë° ì‹¤ì œ ìºê¸€ì´ë‚˜ ë°ì´ì½˜ê³¼ ê°™ì€ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì»´í”¼í‹°ì…˜ì—ì„œëŠ” ë§¤ìš° ì˜ ì“°ì´ëŠ” ê¸°ë²•ë“¤ì´ë‹¤. ì•„ë¬´íŠ¼ ì´ëŸ¬í•œ ë‚´ìš©ë“¤ì„ ì„¸ë¯¸ë‚˜ë¥¼ ì¤€ë¹„í•˜ë©´ì„œ ê³µë¶€í•  ìˆ˜ ìˆì–´ì„œ ì¢‹ì•˜ë‹¤.&lt;/p&gt;

&lt;p&gt;ìš°ë¦¬ í•™êµ ë²—ë“¤ì´ ë°ì´í„° ë¶„ì„ê³¼ ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ì— ë§ì´ ë§ì´ ì§„ì…í–ˆìœ¼ë©´ ì¢‹ê² ë‹¤ :)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;3ï¸âƒ£-ê¸°íƒ€&quot;&gt;3ï¸âƒ£ ê¸°íƒ€&lt;/h3&gt;

&lt;p&gt;ì²« ìì·¨ ìƒí™œë¡œ ì¸í•´ ëª¸ê³¼ ë§ˆìŒì´ ì¡°ê¸ˆ í˜ë“¤ì—ˆì—ˆë‹¤ ã… ã… &lt;/p&gt;

&lt;p&gt;ê·¸ë˜ì„œ 1ì›”ì—ë§Œ í˜¸ìº‰ìŠ¤ë¥¼ ë‘ ë²ˆ í–ˆë‹¤!&lt;/p&gt;

&lt;p&gt;ì²« ë²ˆì§¸ í˜¸ìº‰ìŠ¤ëŠ” ì›Œì»¤íì´ë‹¤. ì•„ë¹ ê°€ ì›Œì»¤í ìª½ì—ì„œ ë°”ìš°ì²˜ë‘ ì¹´ë“œë“¤ì„ ë°›ì•„ì„œ ì›Œì»¤í ìŠ¤ìœ„íŠ¸ë£¸ì—ì„œ ë¬µì„ ìˆ˜ ìˆì—ˆë‹¤! ì—„ë§ˆì•„ë¹  // ë‚˜ ì´ë ‡ê²Œ í•´ì„œ ê°ê° ë°© 1ê°œì”©ì„ ì¼ë‹¤. ë‚˜ í˜¼ì ìŠ¤ìœ„íŠ¸ë£¸ ì“´ ì ì€ ì²˜ìŒì´ë‹¤ ã…ã…ã…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107870217-cd85f900-6ed9-11eb-9bc6-1374187ccc5b.jpeg&quot; alt=&quot;90B06C1A-7396-41AC-8EF6-A23F6D707C44_1_105_c&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107870215-c6f78180-6ed9-11eb-9975-a4bc47ed7582.jpeg&quot; alt=&quot;FE0403FB-98D0-4BD6-8DF9-B343FF2D1FCD_1_105_c&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107870223-dd054200-6ed9-11eb-9666-b3682e317ec1.jpeg&quot; alt=&quot;E001E8C5-7F44-4A72-9A27-56E05590520B_1_105_c&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107870251-e8586d80-6ed9-11eb-897a-82fcea7590cb.jpeg&quot; alt=&quot;30019A7C-B51E-467C-945C-2EC7F62D30C1_1_105_c&quot; /&gt;&lt;/p&gt;

&lt;p&gt;í•œê°•ì´ ë°”ë¡œ ë³´ì´ëŠ” ê²ƒë„ ë§ˆìŒì— ë“¤ì—ˆê³ , 4ì¸µì— ìŠ¤ì¹´ì´ì•¼ë“œë„ êµ‰ì¥íˆ ì¢‹ì•˜ë‹¤. ì˜¤í”ˆ ì‹œê°„ì— ë°”ë¡œ ê°„ ê±°ë¼ ì¡±ìš•ë„ í–ˆë‹¤!&lt;/p&gt;

&lt;p&gt;ë‘ ë²ˆì§¸ í˜¸ìº‰ìŠ¤ëŠ” ì—„ë§ˆê°€ ëŒ€ì „ì— ì™”ì„ ë•Œ íœ´ê°€ë‚´ê³  ê°„ â€˜ëŒ€ì „ ë¡¯ë°ì‹œí‹°í˜¸í…”â€™ì´ë‹¤. ì´ ë•Œ ì •ë§ ì„œìš¸ ê³µí™”êµ­ì´ë¼ëŠ” ê²ƒì„ ëŠê¼ˆëŠ”ë°, ëŒ€ì „ì€ ì œì¼ ì¢‹ì€ í˜¸í…”ì´ 4ì„±ê¸‰ì´ë‹¤ã…œã…œ ì•„ë¬´íŠ¼ ì´ ë•Œì¯¤ì— í˜ë“¤ì—ˆë˜ ì¼ë“¤ì´ ë§ì•˜ì–´ì„œ íœ´ê°€ë¥¼ ë‚´ê³  í˜¸ìº‰ìŠ¤ë¥¼ ê°”ë‹¤. ê²°ê³¼ì ìœ¼ë¡œëŠ” ë°¤ì— ì¼ì„ í•˜ê¸´ í–ˆì§€ë§Œ â€¦^^&lt;/p&gt;

&lt;p&gt;ì—¬ê¸°ëŠ” í•œê°•ë·°ëŠ” ì•„ë‹ˆê³  ê°‘ì²œ?ë·° ã…‹ã…‹ã…‹ã…‹ ë˜ê²Œ ê¹”ë”í–ˆê³ , ê°€ì„±ë¹„ ìˆëŠ” í˜¸í…”ì´ì—ˆë‹¤!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107870334-982ddb00-6eda-11eb-9aa1-bd2c23f6b77b.jpeg&quot; alt=&quot;B635E2B2-B96E-479A-8CD4-6C709BBA2C70_1_105_c&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107870332-96fcae00-6eda-11eb-9b28-514afb771b8b.jpeg&quot; alt=&quot;B9009AFF-7402-49E0-B58E-E85AABF755D1_1_105_c&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ê·¸ë¦¬ê³  ë°”ë¡œ ë§ì€ í¸ì— ì„±ì‹¬ë‹¹ì´ ìˆì–´ì„œ ë°”ë¡œ ë‹¬ë ¤ê°”ë‹¤. ë‚˜ë‘ ìš°ë¦¬ ì—„ë§ˆëŠ” ë¹µì„ ë§¤ìš° ì¢‹ì•„í•˜ê¸° ë•Œë¬¸ì—â€¦ ì‚¬ê³  ì‹¶ì€ ë¹µë“¤ì´ ë¬´ì²™ì´ë‚˜ ë§ì•˜ì§€ë§Œ ë‚˜ë¦„ ì ˆì œí•˜ë©° ê³¨ëë‹¤ ã…‹ã…‹ ì´ˆì½” íŠ€ê¹€ ì†Œë³´ë¡œë¥¼ ë¨¹ì—ˆëŠ”ë° ìƒê°ë³´ë‹¤ íŒ¥ë§›ì´ ë§ì´ ë‚˜ì„œ ì•„ì‰¬ì› ë‹¤ã… ã…  ëª…ë¬¼ì´ë¼ëŠ” ëª…ë€ ë°”ê²ŒíŠ¸ëŠ” ì €ë…ì´ë‘ ì•„ì¹¨ ëª¨ë‘ sold outâ€¦&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107870326-8cdaaf80-6eda-11eb-8e15-6d41c70f971d.jpeg&quot; alt=&quot;576C4EBC-F28D-4ECE-9432-A16B8C44D9A8_1_105_c&quot; /&gt;&lt;/p&gt;

&lt;p&gt;íœ´ê°€ ë‚ ì—ëŠ” ëŒ€ì „ í˜„ëŒ€í”„ë¦¬ë¯¸ì—„ì•„ìš¸ë ›ë„ ê°”ë‹¤! ìƒê¸´ì§€ ì–¼ë§ˆ ì•ˆëœ ê³³ì´ë¼ êµ‰ì¥íˆ ê¹”ë”í–ˆë‹¤. ë˜ í‰ì¼ ë‚®ì´ë¼ ê·¸ëŸ°ê°€ ì‚¬ëŒë“¤ì´ ë³„ë¡œ ì—†ì–´ì„œ ë§¤ìš° ì¢‹ì•˜ë‹¤!
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107870367-24400280-6edb-11eb-9495-41ffbae82b11.jpeg&quot; alt=&quot;45D7BCF6-C915-4251-A660-7882422F783E_1_105_c&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì•„ë¬´íŠ¼ 1ì›” ì¤‘ìˆœë¶€í„°ëŠ” ê°€ì¡±ë“¤ì´ ë‚´ ê¸°ë¶„ì„ í’€ì–´ì£¼ë ¤ê³  ì •ë§ ë§ì´ ë„ì™€ì¤¬ë‹¤. ì—­ì‹œ ê°€ì¡±ì´ ìµœê³ ì•¼ &amp;lt;3
(ì¹œêµ¬ë“¤ë„ ìµœê³ ì•¼)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;4ï¸âƒ£í•œ-ë‹¬-ë™ì•ˆ-í•œ-ê³µë¶€&quot;&gt;4ï¸âƒ£í•œ ë‹¬ ë™ì•ˆ í•œ ê³µë¶€&lt;/h3&gt;

&lt;p&gt;1ì›” ë™ì•ˆ í•œ ê³µë¶€ë¥¼ ì •ë¦¬í•´ë³´ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Active Learning (ì¸í„´ ì—…ë¬´)&lt;/li&gt;
  &lt;li&gt;ë…¼ë¬¸ ìŠ¤í„°ë””(Transformer, GAN, BERT, StarGAN, GPT-1)&lt;/li&gt;
  &lt;li&gt;â€¦&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ë”±íˆ ê³µë¶€ë¥¼ í•œ ê±´ ë³„ë¡œ ì—†êµ¬ë‚˜. ë°˜ì„±â€¦
ì§ì¥ì„ ë‹¤ë‹ˆë©´ì„œ ê³µë¶€í•˜ëŠ” ì‚¬ëŒë“¤ì´ ì •ë§ ëŒ€ë‹¨í•˜ê²Œ ë³´ì¸ë‹¤. ë‚˜ëŠ” ì¸í„´ì¼ ë¿ì¸ë°ë„ í‡´ê·¼í•˜ê³  ì˜¤ë©´ ë»—ê¸° ì¼ìˆ˜ì´ë‹¤ ã… ã…  ì²´ë ¥ì„ ê¸¸ëŸ¬ì•¼ í•  ê²ƒ ê°™ë‹¤&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;5ï¸âƒ£ë‚¨ì€-ë°©í•™-ë™ì•ˆ-í• -ì¼&quot;&gt;5ï¸âƒ£ë‚¨ì€ ë°©í•™ ë™ì•ˆ í•  ì¼&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;AI/ë°ì´í„°ë¶„ì„ ë™ì•„ë¦¬ ëª¨ì§‘ ë° ì„ ë°œ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ì›ë˜ ë§Œë“¤ê¹Œ ë§ê¹Œ ê³ ë¯¼í–ˆëŠ”ë° ì–´ì°Œì €ì°Œ ë§Œë“¤ê²Œ ëë‹¤(í˜„ì¬ 2ì›” ìƒí™©)
ì´ë¯¸ í•˜ê²Œ ëœ ê±°, ì±…ì„ê° ìˆê³  ì´í™”ì˜ ì—­ì‚¬ì— í•œ ì¤„ì„ ê¸‹ëŠ” ë™ì•„ë¦¬ê°€ ë˜ë„ë¡ ì—´ì‹¬íˆ ë…¸ë ¥í•  ê²ƒì´ë‹¤.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ë…¼ë¬¸ ìŠ¤í„°ë””&lt;/li&gt;
  &lt;li&gt;íŠ¹í—ˆ ì‘ì„±&lt;/li&gt;
  &lt;li&gt;ë…¼ë¬¸ ì‘ì„±&lt;/li&gt;
  &lt;li&gt;ì—¬ìœ ê°€ ëœë‹¤ë©´, Pix2Pix êµ¬í˜„&lt;/li&gt;
  &lt;li&gt;ê±´ê°•í•œ ìŒì‹ ë¨¹ê¸°(ì‹ë‹¨ ê´€ë¦¬) &amp;amp; ìì£¼ ê±·ê¸°&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;íŠ¹í—ˆ ì‘ì„±ê³¼ ë…¼ë¬¸ ì‘ì„±ì´ ê°€ì¥ ì£¼ê°€ ë  ë“¯ í•˜ë‹¤. ê³¼ì—° 2ì›”ì•ˆì— ë‹¤ í•  ìˆ˜ ìˆì„ê¹Œ? í•´ì•¼ë§Œ í•œë‹¤ã… &lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="ì›”ê°„ íšŒê³ " /><summary type="html">2021ë…„ 1ì›” íšŒê³  ê¸€ì…ë‹ˆë‹¤.</summary></entry><entry><title type="html">Ubuntu 18.04 clean, ***/*** files, ***/*** blocks ë¬¸ì œí•´ê²°</title><link href="http://localhost:4000/troubleshooting/2021/01/26/Ubuntu-clean-files-blocks-trouble/" rel="alternate" type="text/html" title="Ubuntu 18.04 clean, ***/*** files, ***/*** blocks ë¬¸ì œí•´ê²°" /><published>2021-01-26T00:00:00+09:00</published><updated>2021-01-26T00:00:00+09:00</updated><id>http://localhost:4000/troubleshooting/2021/01/26/Ubuntu-clean-files-blocks-trouble</id><content type="html" xml:base="http://localhost:4000/troubleshooting/2021/01/26/Ubuntu-clean-files-blocks-trouble/">&lt;p&gt;Ubuntu 18.04ì—ì„œ clean, ***/*** files, ***/*** blocks ì—ëŸ¬ê°€ ë–´ì„ ë•Œ ê³ ì¹˜ëŠ” ë°©ë²•ì„ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;ì¡ì†Œë¦¬&quot;&gt;ì¡ì†Œë¦¬&lt;/h2&gt;

&lt;p&gt;ì˜¤ëŠ˜ ì—°êµ¬ì‹¤ì—ì„œ ì •ë§ ë‹¹í™©í•œ ì—ëŸ¬ë¥¼ ë§ˆì£¼ì³¤ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì‚¬ê±´ì˜ ì—­ì‹œë‚˜ CUDAì—ì„œ ì‹œì‘â€¦ ì €ë²ˆì£¼ê¹Œì§€ë§Œ í•´ë„ ì˜ ëë˜ gpuì‚¬ìš©ì´ ì˜¤ëŠ˜ í•˜ë ¤ë‹ˆ ê°‘ìê¸° ì°¾ì„ ìˆ˜ ì—†ë‹¤(?)ëŠ” ì—ëŸ¬ê°€ ë–´ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì¹¨ì°©í•˜ê²Œ ê·¸ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ êµ¬ê¸€ë§í•´ì„œ ìŠ¤íƒì˜¤ë²„í”Œë¡œìš°ì—ì„œ í•˜ë¼ëŠ” ëŒ€ë¡œ í•˜ê³  ì¬ë¶€íŒ…í–ˆëŠ”ë° ì•„ë˜ì™€ ê°™ì€ ì—ëŸ¬ê°€ ëœ¨ê³  ë¬´í•œ ë¸”ë™ìŠ¤í¬ë¦°ì´ì—ˆìŠµë‹ˆë‹¤ ã…œã…œ.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/fBYUJ.jpg&quot; alt=&quot;&quot; /&gt; &lt;a href=&quot;https://askubuntu.com/questions/1222496/system-wont-boot-stuck-at-dev-sda5-clean-xxxx-xxxx-files-yyyy-yyyy-blocks&quot;&gt;ì´ë¯¸ì§€ ì¶œì²˜&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ì§„ì‹¬ ì €ê±° ë´¤ì„ ë•Œ ëˆˆë¬¼ì´ ì°”ë” ë‚˜ì˜¬ ë»” í–ˆëŠ”ë°ìš” ..ã…‹ã…‹ã…‹ ìŠ¤íƒì˜¤ë²„í”Œë¡œìš°ë³´ë‹ˆê¹Œ ì•„ì˜ˆ ìš°ë¶„íˆ¬ë¥¼ ì¬ì„¤ì¹˜ í•´ì•¼ í•  ìˆ˜ë„ ìˆë‹¤ëŠ” ê¸€ì„ ë³´ê³ â€¦ ì§„ì§œ ì˜¨ëª¸ì´ ì˜¤ì‹¹í•´ì¡ŒìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;ìš°ì„  ë‹¤ì‹œ ë¶€íŒ…í•˜ê³  recovery mode (safe mode) ë¡œ ë“¤ì–´ê°€ì•¼ í•©ë‹ˆë‹¤. ë¶€íŒ…ë  ë•Œ ë¬´í•œ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Shift&lt;/code&gt; ëˆ„ë¥´ì‹­ì‹œì˜¤. (ìš°ë¶„íˆ¬ 18.04 ê¸°ì¤€)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Advanced options for Ubuntu&lt;/code&gt; ë¥¼ í´ë¦­í•˜ê³ , &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;recovery mode&lt;/code&gt;ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤.
ì œê°€ ìº¡ì³í•  ìƒí™©ì€ ì•„ë‹ˆì—ˆë˜ì§€ë¼ ë‹¤ ì¸í„°ë„·ì—ì„œ ë“¤ê³  ì˜¤ëŠ” ì  ì´í•´ ë¶€íƒë“œë¦½ë‹ˆë‹¤^^; &lt;a href=&quot;https://askubuntu.com/questions/92556/how-do-i-boot-into-a-root-shell&quot;&gt;ì¶œì²˜&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/01e8n.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://i.stack.imgur.com/UP5j7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ì´ì œ ë¦¬ì»¤ë²„ë¦¬ ëª¨ë“œì— ë“¤ì–´ì™”ëŠ”ë°ìš”. ì—¬ê¸°ì„œ ë°©í–¥í‚¤ë¥¼ ì‚¬ìš©í•´ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt;ë¡œ ì´ë™í•˜ê³  Enterí•©ë‹ˆë‹¤. ì´ì œ í„°ë¯¸ë„ì„ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤! 
&lt;img src=&quot;https://i.stack.imgur.com/tHkmh.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ì´ì œ í„°ë¯¸ë„ì—ì„œ&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt-get purge nvidia*&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;ë¥¼ ì¹˜ë©´ ë©ë‹ˆë‹¤ë§Œ, ì—¬ê¸°ì„œ ì €ëŠ”&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dpkg was interrupted, you must manually run 'sudo dpkg --configure -a' to correct the problem
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ë¼ëŠ” ì—ëŸ¬ê°€ ë˜ ëœ¹ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì—¬ê¸°ì„œ ì œ ë¶€íŒ… ì‹¤íŒ¨ì˜ ì›ì¸ì„ ìœ ì¶”í•´ë³¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì¬ë¶€íŒ… ì „ì— í–ˆë˜ ë™ì‘ë“¤ì—ì„œ ë­”ê°€ë¥¼ ê±´ë“œë ¸ê³  ê·¸ ê²°ê³¼ dpkg íŒ¨í‚¤ì§€ê°€ ì œëŒ€ë¡œ êµ¬ì„±ë˜ì§€ ì•Šì•˜ë˜ ê²ë‹ˆë‹¤.(maybeâ€¦.)&lt;/p&gt;

&lt;p&gt;ì•„ë¬´íŠ¼ ì´ ì—ëŸ¬ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì‰½ê²Œ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì—ëŸ¬ ë©”ì‹œì§€ëŒ€ë¡œ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo dpkg --configure -a&lt;/code&gt; ë¥¼ í•´ì£¼ë©´ ë©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ì œ ë‹¤ì‹œ&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt-get purge nvidia*&lt;/code&gt;
ë¥¼ í•˜ê²Œ ë˜ë©´ ì˜ ì„¤ì¹˜ ë©ë‹ˆë‹¤.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reboot&lt;/code&gt; ë¥¼ ì¨ì„œ ì¬ë¶€íŒ…í•©ë‹ˆë‹¤ -&amp;gt; ì„±ê³µ!&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;ì´ ì¼ì€ ì €ì—ê²Œ ìˆì–´ ì •ë§ ë”ì°í–ˆë˜ ìˆœê°„ì´ì—ˆìŠµë‹ˆë‹¤. í‡´ì‚¬í•´ì•¼í•˜ë‚˜ ìƒê°ë„ í–ˆì–´ìš” ã…‹ã…‹ã…‹ ì•„ë¬´íŠ¼ ë¹ ë¥´ê²Œ ê³ ì¹˜ê³  í‡´ê·¼í–ˆìŠµë‹ˆë‹¤^^.. í˜¹ì‹œ êµ¬ê¸€ë§í•˜ë‹¤ ì €ì™€ ê°™ì€ ì—ëŸ¬ë¥¼ ë°œê²¬í•˜ì‹œê²Œ ëœë‹¤ë©´, ìœ„ ê³¼ì •ì„ í†µí•´ ê³ ì¹˜ê¸¸ ë°”ë¼ê² ìŠµë‹ˆë‹¤.&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Ubuntu" /><category term="Troubleshooting" /><summary type="html">Ubuntu 18.04ì—ì„œ clean, ***/*** files, ***/*** blocks ì—ëŸ¬ê°€ ë–´ì„ ë•Œ ê³ ì¹˜ëŠ” ë°©ë²•ì„ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.</summary></entry><entry><title type="html">Pre-training of Deep Bidirectional Transformers for Language Understanding(BERT)</title><link href="http://localhost:4000/paper%20review/2021/01/25/BERT/" rel="alternate" type="text/html" title="Pre-training of Deep Bidirectional Transformers for Language Understanding(BERT)" /><published>2021-01-25T00:00:00+09:00</published><updated>2021-01-25T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2021/01/25/BERT</id><content type="html" xml:base="http://localhost:4000/paper%20review/2021/01/25/BERT/">&lt;p&gt;Pre-training of Deep Bidirectional Transformers for Language Understanding(BERT) ì„ ì½ê³  ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;bidirectional&lt;/strong&gt;ì´ë¼ëŠ” ì ì´ ì´ì „ ì—°êµ¬ë“¤ê³¼ì˜ ê°€ì¥ í° ì°¨ì´ì ì´ë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create SOTA ~...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;fine-tuningì´ ë§¤ìš° ìš©ì´í•œ ëª¨ë¸ì´ë‹¤&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ì—¬ëŸ¬ ê°€ì§€ taskì—ì„œ SOTAë¥¼ ê¸°ë¡í–ˆìŒ.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Applying pre-trained language representations to downstream tasks&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feature-based&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;ELMo&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fine-tuning&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;GPT&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;limitation-of-unidirectional-approach&quot;&gt;Limitation of Unidirectional approach&lt;/h3&gt;

&lt;p&gt;ë‘ ì ‘ê·¼ë²• ëª¨ë‘ &lt;strong&gt;unidirectional&lt;/strong&gt; ë°©ë²•ì„ ì‚¬ìš©í•¨.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;But, unidirectional ë°©ë²•ì€ í•œê³„ì ì´ ëšœë ·í•¨.&lt;/li&gt;
  &lt;li&gt;GPT(1)ì²˜ëŸ¼ left-to-right architectureë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´, ëª¨ë“  tokenë“¤ì€ ì´ì „ì˜ tokenë“¤ì—ë§Œ self-attentioní•  ìˆ˜ ìˆê²Œ ëœë‹¤.
    &lt;ul&gt;
      &lt;li&gt;ì´ëŠ” token-level task (ì˜ˆë¥¼ ë“¤ì–´ QA)ë“¤ì— ë¶€ì í•©í•˜ë‹¤. ì´ëŸ¬í•œ íƒœìŠ¤í¬ë“¤ì€ ì–‘ ë°©í–¥ì„ ë´ì„œ &lt;strong&gt;context&lt;/strong&gt;ë¥¼ ì½ì–´ì•¼í•˜ê¸° ë•Œë¬¸.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;masked-langauge-modelmlm&quot;&gt;Masked Langauge Model(MLM)&lt;/h3&gt;

&lt;p&gt;BERTì—ì„œëŠ” pre-training í•  ë•Œ, ìœ„ì—ì„œ ì–¸ê¸‰í•œ unidirectionality constraintë¥¼ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Masked Language Model(MLM)&lt;/code&gt; ìœ¼ë¡œ ì™„í™”(?)í–ˆìŒ.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MLMì€ input í† í°ë“¤ë¡œë¶€í„° &lt;strong&gt;randomly mask&lt;/strong&gt;í•¨.
    &lt;ul&gt;
      &lt;li&gt;ì´ê²ƒì˜ ëª©ì ì€, &lt;strong&gt;ì˜¤ì§ contextë§Œ ê°€ì§€ê³ &lt;/strong&gt; maskëœ original vocab.ì„ ë§ì¶”ëŠ” ê²ƒ.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Left-to-right LM pre-trainingê³¼ ë‹¬ë¦¬, MLMì˜ ëª©ì  í•¨ìˆ˜ëŠ” ì–‘ ë°©í–¥ì˜ representationsë¥¼ ì´í•´í•  ìˆ˜ ìˆë‹¤.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  which allows us to pre-train a deep bidirectional Transformer
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;next-sentence-prediction&quot;&gt;Next Sentence Prediction&lt;/h3&gt;

&lt;p&gt;jointly pre-train text-pair representation&lt;/p&gt;

&lt;h2 id=&quot;bert&quot;&gt;BERT&lt;/h2&gt;

&lt;p&gt;&lt;img width=&quot;766&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-01-25 á„‹á…©á„’á…® 9 09 09&quot; src=&quot;https://user-images.githubusercontent.com/48315997/105704076-91eab580-5f51-11eb-8b52-16c48146bd50.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;pre-training&quot;&gt;Pre-training&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;During pre-training, the model is trained on unlabeled data over different pre-training tasks.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Pre-training ë‹¨ê³„ì—ì„œ ëª¨ë¸ì€ &lt;strong&gt;unlabeled data&lt;/strong&gt;ë¥¼ ê°€ì§€ê³  í•™ìŠµëœë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TWO Unsupervised taskë¥¼ ì‚¬ìš©í•¨.&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;task-1-masked-lm&quot;&gt;Task #1. Masked LM&lt;/h4&gt;

&lt;p&gt;deep bidirectional representationì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´, input tokenì—ì„œ ëœë¤ìœ¼ë¡œ ëª‡ %ì •ë„ë¥¼ maskingí•˜ê³  ì´ masked tokenë“¤ì„ predictí•˜ë„ë¡ í•™ìŠµì‹œí‚´.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;[MASK]&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Random&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Unchanged&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;task-2-next-sentence-prediction-nsp&quot;&gt;Task #2. Next Sentence Prediction (NSP)&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;ë‘ ë¬¸ì¥ ì‚¬ì´ì˜ ê´€ê³„(relationship)ë¥¼ ì´í•´&lt;/strong&gt;í•˜ê¸° ìœ„í•´ í•™ìŠµí•˜ëŠ” taskì´ë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IsNext&lt;/code&gt;ì¸ì§€ ì•„ë‹Œì§€(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NotNext&lt;/code&gt;) ë¶„ë¥˜í•˜ëŠ” binary classification ë¬¸ì œì„.&lt;/p&gt;

&lt;h3 id=&quot;fine-tuning-bert&quot;&gt;Fine-tuning BERT&lt;/h3&gt;

&lt;p&gt;ì ì ˆí•œ Inputê³¼ Outputì„ ë„£ìœ¼ë©´, single textë‚˜ text pair ëª¨ë‘ ê°€ëŠ¥.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="BERT" /><category term="Attention" /><category term="Transformer" /><summary type="html">Pre-training of Deep Bidirectional Transformers for Language Understanding(BERT) ì„ ì½ê³  ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.</summary></entry></feed>