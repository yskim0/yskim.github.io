<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-01-26T21:44:57+09:00</updated><id>http://localhost:4000/atom.xml</id><title type="html">Yeonsoo Kimâ€™s blog</title><author><name>Yeonsoo Kim</name></author><entry><title type="html">Ubuntu 18.04 clean, ***/*** files, ***/*** blocks ë¬¸ì œí•´ê²°</title><link href="http://localhost:4000/troubleshooting/2021/01/26/d/" rel="alternate" type="text/html" title="Ubuntu 18.04 clean, ***/*** files, ***/*** blocks ë¬¸ì œí•´ê²°" /><published>2021-01-26T00:00:00+09:00</published><updated>2021-01-26T00:00:00+09:00</updated><id>http://localhost:4000/troubleshooting/2021/01/26/d</id><content type="html" xml:base="http://localhost:4000/troubleshooting/2021/01/26/d/">&lt;p&gt;Ubuntu 18.04ì—ì„œ clean, ***/*** files, ***/*** blocks ì—ëŸ¬ê°€ ë–´ì„ ë•Œ ê³ ì¹˜ëŠ” ë°©ë²•ì„ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;ì¡ì†Œë¦¬&quot;&gt;ì¡ì†Œë¦¬&lt;/h2&gt;

&lt;p&gt;ì˜¤ëŠ˜ ì—°êµ¬ì‹¤ì—ì„œ ì •ë§ ë‹¹í™©í•œ ì—ëŸ¬ë¥¼ ë§ˆì£¼ì³¤ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì‚¬ê±´ì˜ ì—­ì‹œë‚˜ CUDAì—ì„œ ì‹œì‘â€¦ ì €ë²ˆì£¼ê¹Œì§€ë§Œ í•´ë„ ì˜ ëë˜ gpuì‚¬ìš©ì´ ì˜¤ëŠ˜ í•˜ë ¤ë‹ˆ ê°‘ìê¸° ì°¾ì„ ìˆ˜ ì—†ë‹¤(?)ëŠ” ì—ëŸ¬ê°€ ë–´ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì¹¨ì°©í•˜ê²Œ ê·¸ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ êµ¬ê¸€ë§í•´ì„œ ìŠ¤íƒì˜¤ë²„í”Œë¡œìš°ì—ì„œ í•˜ë¼ëŠ” ëŒ€ë¡œ í•˜ê³  ì¬ë¶€íŒ…í–ˆëŠ”ë° ì•„ë˜ì™€ ê°™ì€ ì—ëŸ¬ê°€ ëœ¨ê³  ë¬´í•œ ë¸”ë™ìŠ¤í¬ë¦°ì´ì—ˆìŠµë‹ˆë‹¤ ã…œã…œ.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/fBYUJ.jpg&quot; alt=&quot;&quot; /&gt; &lt;a href=&quot;https://askubuntu.com/questions/1222496/system-wont-boot-stuck-at-dev-sda5-clean-xxxx-xxxx-files-yyyy-yyyy-blocks&quot;&gt;ì´ë¯¸ì§€ ì¶œì²˜&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ì§„ì‹¬ ì €ê±° ë´¤ì„ ë•Œ ëˆˆë¬¼ì´ ì°”ë” ë‚˜ì˜¬ ë»” í–ˆëŠ”ë°ìš” ..ã…‹ã…‹ã…‹ ìŠ¤íƒì˜¤ë²„í”Œë¡œìš°ë³´ë‹ˆê¹Œ ì•„ì˜ˆ ìš°ë¶„íˆ¬ë¥¼ ì¬ì„¤ì¹˜ í•´ì•¼ í•  ìˆ˜ë„ ìˆë‹¤ëŠ” ê¸€ì„ ë³´ê³ â€¦ ì§„ì§œ ì˜¨ëª¸ì´ ì˜¤ì‹¹í•´ì¡ŒìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;ìš°ì„  ë‹¤ì‹œ ë¶€íŒ…í•˜ê³  recovery mode (safe mode) ë¡œ ë“¤ì–´ê°€ì•¼ í•©ë‹ˆë‹¤. ë¶€íŒ…ë  ë•Œ ë¬´í•œ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Shift&lt;/code&gt; ëˆ„ë¥´ì‹­ì‹œì˜¤. (ìš°ë¶„íˆ¬ 18.04 ê¸°ì¤€)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Advanced options for Ubuntu&lt;/code&gt; ë¥¼ í´ë¦­í•˜ê³ , &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;recovery mode&lt;/code&gt;ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤.
ì œê°€ ìº¡ì³í•  ìƒí™©ì€ ì•„ë‹ˆì—ˆë˜ì§€ë¼ ë‹¤ ì¸í„°ë„·ì—ì„œ ë“¤ê³  ì˜¤ëŠ” ì  ì´í•´ ë¶€íƒë“œë¦½ë‹ˆë‹¤^^; &lt;a href=&quot;https://askubuntu.com/questions/92556/how-do-i-boot-into-a-root-shell&quot;&gt;ì¶œì²˜&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/01e8n.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://i.stack.imgur.com/UP5j7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ì´ì œ ë¦¬ì»¤ë²„ë¦¬ ëª¨ë“œì— ë“¤ì–´ì™”ëŠ”ë°ìš”. ì—¬ê¸°ì„œ ë°©í–¥í‚¤ë¥¼ ì‚¬ìš©í•´ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt;ë¡œ ì´ë™í•˜ê³  Enterí•©ë‹ˆë‹¤. ì´ì œ í„°ë¯¸ë„ì„ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤! 
&lt;img src=&quot;https://i.stack.imgur.com/tHkmh.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ì´ì œ í„°ë¯¸ë„ì—ì„œ&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt-get purge nvidia*&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;ë¥¼ ì¹˜ë©´ ë©ë‹ˆë‹¤ë§Œ, ì—¬ê¸°ì„œ ì €ëŠ”&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dpkg was interrupted, you must manually run 'sudo dpkg --configure -a' to correct the problem
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ë¼ëŠ” ì—ëŸ¬ê°€ ë˜ ëœ¹ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì—¬ê¸°ì„œ ì œ ë¶€íŒ… ì‹¤íŒ¨ì˜ ì›ì¸ì„ ìœ ì¶”í•´ë³¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì¬ë¶€íŒ… ì „ì— í–ˆë˜ ë™ì‘ë“¤ì—ì„œ ë­”ê°€ë¥¼ ê±´ë“œë ¸ê³  ê·¸ ê²°ê³¼ dpkg íŒ¨í‚¤ì§€ê°€ ì œëŒ€ë¡œ êµ¬ì„±ë˜ì§€ ì•Šì•˜ë˜ ê²ë‹ˆë‹¤.(maybeâ€¦.)&lt;/p&gt;

&lt;p&gt;ì•„ë¬´íŠ¼ ì´ ì—ëŸ¬ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì‰½ê²Œ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì—ëŸ¬ ë©”ì‹œì§€ëŒ€ë¡œ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo dpkg --configure -a&lt;/code&gt; ë¥¼ í•´ì£¼ë©´ ë©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ì œ ë‹¤ì‹œ&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt-get purge nvidia*&lt;/code&gt;
ë¥¼ í•˜ê²Œ ë˜ë©´ ì˜ ì„¤ì¹˜ ë©ë‹ˆë‹¤.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reboot&lt;/code&gt; ë¥¼ ì¨ì„œ ì¬ë¶€íŒ…í•©ë‹ˆë‹¤ -&amp;gt; ì„±ê³µ!&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;ì´ ì¼ì€ ì €ì—ê²Œ ìˆì–´ ì •ë§ ë”ì°í–ˆë˜ ìˆœê°„ì´ì—ˆìŠµë‹ˆë‹¤. í‡´ì‚¬í•´ì•¼í•˜ë‚˜ ìƒê°ë„ í–ˆì–´ìš” ã…‹ã…‹ã…‹ ì•„ë¬´íŠ¼ ë¹ ë¥´ê²Œ ê³ ì¹˜ê³  í‡´ê·¼í–ˆìŠµë‹ˆë‹¤^^.. í˜¹ì‹œ êµ¬ê¸€ë§í•˜ë‹¤ ì €ì™€ ê°™ì€ ì—ëŸ¬ë¥¼ ë°œê²¬í•˜ì‹œê²Œ ëœë‹¤ë©´, ìœ„ ê³¼ì •ì„ í†µí•´ ê³ ì¹˜ê¸¸ ë°”ë¼ê² ìŠµë‹ˆë‹¤.&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Ubuntu" /><category term="Troubleshooting" /><summary type="html">Ubuntu 18.04ì—ì„œ clean, ***/*** files, ***/*** blocks ì—ëŸ¬ê°€ ë–´ì„ ë•Œ ê³ ì¹˜ëŠ” ë°©ë²•ì„ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.</summary></entry><entry><title type="html">Pre-training of Deep Bidirectional Transformers for Language Understanding(BERT)</title><link href="http://localhost:4000/paper%20review/2021/01/25/BERT/" rel="alternate" type="text/html" title="Pre-training of Deep Bidirectional Transformers for Language Understanding(BERT)" /><published>2021-01-25T00:00:00+09:00</published><updated>2021-01-25T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2021/01/25/BERT</id><content type="html" xml:base="http://localhost:4000/paper%20review/2021/01/25/BERT/">&lt;p&gt;Pre-training of Deep Bidirectional Transformers for Language Understanding(BERT) ì„ ì½ê³  ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;bidirectional&lt;/strong&gt;ì´ë¼ëŠ” ì ì´ ì´ì „ ì—°êµ¬ë“¤ê³¼ì˜ ê°€ì¥ í° ì°¨ì´ì ì´ë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create SOTA ~...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;fine-tuningì´ ë§¤ìš° ìš©ì´í•œ ëª¨ë¸ì´ë‹¤&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ì—¬ëŸ¬ ê°€ì§€ taskì—ì„œ SOTAë¥¼ ê¸°ë¡í–ˆìŒ.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Applying pre-trained language representations to downstream tasks&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feature-based&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;ELMo&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fine-tuning&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;GPT&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;limitation-of-unidirectional-approach&quot;&gt;Limitation of Unidirectional approach&lt;/h3&gt;

&lt;p&gt;ë‘ ì ‘ê·¼ë²• ëª¨ë‘ &lt;strong&gt;unidirectional&lt;/strong&gt; ë°©ë²•ì„ ì‚¬ìš©í•¨.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;But, unidirectional ë°©ë²•ì€ í•œê³„ì ì´ ëšœë ·í•¨.&lt;/li&gt;
  &lt;li&gt;GPT(1)ì²˜ëŸ¼ left-to-right architectureë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´, ëª¨ë“  tokenë“¤ì€ ì´ì „ì˜ tokenë“¤ì—ë§Œ self-attentioní•  ìˆ˜ ìˆê²Œ ëœë‹¤.
    &lt;ul&gt;
      &lt;li&gt;ì´ëŠ” token-level task (ì˜ˆë¥¼ ë“¤ì–´ QA)ë“¤ì— ë¶€ì í•©í•˜ë‹¤. ì´ëŸ¬í•œ íƒœìŠ¤í¬ë“¤ì€ ì–‘ ë°©í–¥ì„ ë´ì„œ &lt;strong&gt;context&lt;/strong&gt;ë¥¼ ì½ì–´ì•¼í•˜ê¸° ë•Œë¬¸.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;masked-langauge-modelmlm&quot;&gt;Masked Langauge Model(MLM)&lt;/h3&gt;

&lt;p&gt;BERTì—ì„œëŠ” pre-training í•  ë•Œ, ìœ„ì—ì„œ ì–¸ê¸‰í•œ unidirectionality constraintë¥¼ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Masked Language Model(MLM)&lt;/code&gt; ìœ¼ë¡œ ì™„í™”(?)í–ˆìŒ.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MLMì€ input í† í°ë“¤ë¡œë¶€í„° &lt;strong&gt;randomly mask&lt;/strong&gt;í•¨.
    &lt;ul&gt;
      &lt;li&gt;ì´ê²ƒì˜ ëª©ì ì€, &lt;strong&gt;ì˜¤ì§ contextë§Œ ê°€ì§€ê³ &lt;/strong&gt; maskëœ original vocab.ì„ ë§ì¶”ëŠ” ê²ƒ.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Left-to-right LM pre-trainingê³¼ ë‹¬ë¦¬, MLMì˜ ëª©ì  í•¨ìˆ˜ëŠ” ì–‘ ë°©í–¥ì˜ representationsë¥¼ ì´í•´í•  ìˆ˜ ìˆë‹¤.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  which allows us to pre-train a deep bidirectional Transformer
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;next-sentence-prediction&quot;&gt;Next Sentence Prediction&lt;/h3&gt;

&lt;p&gt;jointly pre-train text-pair representation&lt;/p&gt;

&lt;h2 id=&quot;bert&quot;&gt;BERT&lt;/h2&gt;

&lt;p&gt;&lt;img width=&quot;766&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2021-01-25 á„‹á…©á„’á…® 9 09 09&quot; src=&quot;https://user-images.githubusercontent.com/48315997/105704076-91eab580-5f51-11eb-8b52-16c48146bd50.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;pre-training&quot;&gt;Pre-training&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;During pre-training, the model is trained on unlabeled data over different pre-training tasks.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Pre-training ë‹¨ê³„ì—ì„œ ëª¨ë¸ì€ &lt;strong&gt;unlabeled data&lt;/strong&gt;ë¥¼ ê°€ì§€ê³  í•™ìŠµëœë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TWO Unsupervised taskë¥¼ ì‚¬ìš©í•¨.&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;task-1-masked-lm&quot;&gt;Task #1. Masked LM&lt;/h4&gt;

&lt;p&gt;deep bidirectional representationì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´, input tokenì—ì„œ ëœë¤ìœ¼ë¡œ ëª‡ %ì •ë„ë¥¼ maskingí•˜ê³  ì´ masked tokenë“¤ì„ predictí•˜ë„ë¡ í•™ìŠµì‹œí‚´.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;[MASK]&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Random&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Unchanged&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;task-2-next-sentence-prediction-nsp&quot;&gt;Task #2. Next Sentence Prediction (NSP)&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;ë‘ ë¬¸ì¥ ì‚¬ì´ì˜ ê´€ê³„(relationship)ë¥¼ ì´í•´&lt;/strong&gt;í•˜ê¸° ìœ„í•´ í•™ìŠµí•˜ëŠ” taskì´ë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IsNext&lt;/code&gt;ì¸ì§€ ì•„ë‹Œì§€(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NotNext&lt;/code&gt;) ë¶„ë¥˜í•˜ëŠ” binary classification ë¬¸ì œì„.&lt;/p&gt;

&lt;h3 id=&quot;fine-tuning-bert&quot;&gt;Fine-tuning BERT&lt;/h3&gt;

&lt;p&gt;ì ì ˆí•œ Inputê³¼ Outputì„ ë„£ìœ¼ë©´, single textë‚˜ text pair ëª¨ë‘ ê°€ëŠ¥.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="BERT" /><category term="Attention" /><category term="Transformer" /><summary type="html">Pre-training of Deep Bidirectional Transformers for Language Understanding(BERT) ì„ ì½ê³  ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.</summary></entry><entry><title type="html">2020ë…„ì„ ëŒì•„ë³´ë©°, 2020 YEAR in REVIEW</title><link href="http://localhost:4000/year%20in%20review/2020/12/31/2020_YEAR_in_REVIEW/" rel="alternate" type="text/html" title="2020ë…„ì„ ëŒì•„ë³´ë©°, 2020 YEAR in REVIEW" /><published>2020-12-31T00:00:00+09:00</published><updated>2020-12-31T00:00:00+09:00</updated><id>http://localhost:4000/year%20in%20review/2020/12/31/2020_YEAR_in_REVIEW</id><content type="html" xml:base="http://localhost:4000/year%20in%20review/2020/12/31/2020_YEAR_in_REVIEW/">&lt;p&gt;2020ë…„ ì €ì—ê²Œ ìˆì—ˆë˜ ì¼ì„ ì§§ê²Œ íšŒê³ í•˜ëŠ” ê¸€ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;ëª©ì°¨&quot;&gt;ëª©ì°¨&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;ìˆ˜í™”ì¸ì‹ ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜ í”„ë¡œì íŠ¸ ì™„ì„±&lt;/li&gt;
  &lt;li&gt;ë°ì´í„° ë¶„ì„ ê³µë¶€&lt;/li&gt;
  &lt;li&gt;SKT AI Fellowship 2ê¸° ì§€ì›ê³¼ íƒˆë½&lt;/li&gt;
  &lt;li&gt;ë…¼ë¬¸ ìŠ¤í„°ë””&lt;/li&gt;
  &lt;li&gt;HAICON 2020 ì…ì„ &lt;/li&gt;
  &lt;li&gt;1,2í•™ê¸° ëª¨ë‘ 4ì ëŒ€ë¡œ ë§ˆë¬´ë¦¬&lt;/li&gt;
  &lt;li&gt;2021ë…„ ëª©í‘œ&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;1ï¸âƒ£-ìˆ˜í™”ì¸ì‹-ì›¹-ì–´í”Œë¦¬ì¼€ì´ì…˜-í”„ë¡œì íŠ¸-ì™„ì„±&quot;&gt;1ï¸âƒ£ ìˆ˜í™”ì¸ì‹ ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜ í”„ë¡œì íŠ¸ ì™„ì„±&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/103408719-15112980-4ba7-11eb-8161-5ec2c9fbcbb4.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;DSC Ewhaì—ì„œ ìš°ë¦¬íŒ€ì´ ì§„í–‰í•˜ë˜ í”„ë¡œì íŠ¸ëŠ” &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ìˆ˜í™” ì¸ì‹&lt;/code&gt;ê³¼ ê´€ë ¨ëœ í”„ë¡œì íŠ¸ì˜€ë‹¤.&lt;/p&gt;

&lt;p&gt;ì •í™•íˆëŠ” â€œë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì´ìš©í•œ ìˆ˜í™” êµìœ¡ ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜â€.&lt;/p&gt;

&lt;p&gt;ì‰½ì§€ ì•Šì€ í”„ë¡œì íŠ¸ì˜€ê¸°ì— êµ‰ì¥íˆ ë‹¤ì‚¬ë‹¤ë‚œí–ˆì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë”¥ëŸ¬ë‹ì„ ì•„ì§ ìì£¼ ë‹¤ë£¨ì§€ ì•Šì€ íŒ€ì›ë“¤ì´ ë§ì•˜ê¸°ì—(ë‚˜ í¬í•¨) ì²˜ìŒë¶€í„° ì´ë¡  ê³µë¶€ë„ í•´ë³´ê³ , ìˆ˜í™” ì¸ì‹ ê´€ë ¨ ë…¼ë¬¸ë„ ì½ì–´ë³´ê³ , ê´€ë ¨ í”„ë¡œì íŠ¸ë“¤ë„ ë§ì´ ì¡°ì‚¬í–ˆì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;í”„ë¡œì íŠ¸ ì´ˆê¸°ì— ì´ë ‡ê²Œ ë§ì´ ê³ ìƒí–ˆê¸° ë•Œë¬¸ì— ìƒëŒ€ì ìœ¼ë¡œ ìš°ë¦¬ íŒ€ì€ ì¤‘í•˜ë°˜ë¶€í„°ëŠ” í•  ì¼ì´ ì—†ì—ˆë‹¤ :)&lt;/p&gt;

&lt;p&gt;íŒ€ì›ë“¤ì´ ì—´ì •, ëˆê¸°, ì˜ì§€, ì‹¤ë ¥ê¹Œì§€ ìˆì–´ì„œ ë•ë¶„ì— ì´ í”„ë¡œì íŠ¸ë¥¼ ìˆ˜ì›”í•˜ê²Œ(?) ì™„ìˆ˜í•  ìˆ˜ ìˆì—ˆë˜ ê²ƒ ê°™ë‹¤.&lt;/p&gt;

&lt;p&gt;êµ‰ì¥íˆ ë‹¤ì‚¬ë‹¤ë‚œí•œ ì¼ì´ ë§ì•˜ì§€ë§Œâ€¦^^ ê·¸ ì´ì•¼ê¸°ë“¤ì€ ì„œë¡œì˜ ê°€ìŠ´ ì†ì— ë¬»ì–´ë‘ê¸°ë¡œ í•˜ê³  ã…‹ã…‹ã…‹&lt;/p&gt;

&lt;p&gt;ìš°ë¦¬ë§Œì˜ ìˆ˜í™” ì¸ì‹ ëª¨ë¸ì„ ë§Œë“œëŠë¼ ê°€ì¥ ë§ì€ ì‹œê°„ì„ ìŸì•„ ë¶€ì—ˆì—ˆë‹¤. í•˜ì§€ë§Œ ì´ë•ë¶„ì— ë‚˜ëŠ” Object Detectionì— ëŒ€í•œ ê³µë¶€ì™€ ê´€ë ¨ ê²½í—˜ì„ ìŒ“ì„ ìˆ˜ ìˆì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;YOLO ë“± ì—¬ëŸ¬ ëª¨ë¸ë“¤ì„ ì§ì ‘ ëŒë ¤ë³´ê¸°ë„ í•˜ê³ , ì§ì ‘ ë§Œë“¤ì–´ë³´ê¸°ë„ í•˜ì˜€ë‹¤. ê²°ê³¼ì ìœ¼ë¡œëŠ” ìš°ë¦¬ íŒ€ì˜ ã…ˆã…‡ì–¸ë‹ˆê°€ ë§Œë“  ëª¨ë¸ì„ ì¼ì§€ë§Œ ğŸ˜†&lt;/p&gt;

&lt;p&gt;ì•„ë¬´íŠ¼ ì´ í”„ë¡œì íŠ¸ë¡œ ì¸í•´ ë‚˜ëŠ” &lt;strong&gt;ì¢‹ì€ ì‚¬ëŒë“¤ë„ ì–»ì—ˆê³ , ë”¥ëŸ¬ë‹ì— ëŒ€í•œ í˜¸ê°ê³¼ í¥ë¯¸, ìì‹ ê° ë“±ì„ ëª¨ë‘ ì–»ì„ ìˆ˜ ìˆì—ˆë‹¤.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;í™•ì‹¤íˆ í”„ë¡œì íŠ¸ë¥¼ í•œë²ˆ í•˜ê³  ë‚˜ë©´ ë§ì€ ì„±ì¥ì„ ì´ë£¨ëŠ” ê²ƒ ê°™ë‹¤. ë‚´ë…„ì˜ ë‚˜ëŠ” ì¡¸ì—… í”„ë¡œì íŠ¸ë¥¼ í†µí•´ í›¨ì”¬ ë” ì„±ì¥í•  ìˆ˜ ìˆê¸°ë¥¼.&lt;/p&gt;

&lt;h3 id=&quot;2ï¸âƒ£-ë°ì´í„°-ë¶„ì„-ê³µë¶€&quot;&gt;2ï¸âƒ£ ë°ì´í„° ë¶„ì„ ê³µë¶€&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/103408999-23137a00-4ba8-11eb-888e-4532bc570e1e.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ë°ì´í„° ë¶„ì„ ê³µë¶€ë¥¼ í•˜ê²Œ ëœ ê³„ê¸°ëŠ” ì¢€ ì†ë¬¼ì ì´ë‹¤.&lt;/p&gt;

&lt;p&gt;AI ë¦¬ì„œì³/ì—”ì§€ë‹ˆì–´ ì§êµ°ì„ ëª¨ì§‘í•  ë•Œ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ìš°ëŒ€ì‚¬í•­&lt;/code&gt;ì— ìºê¸€ê³¼ ê°™ì€ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì»´í”¼í‹°ì…˜ê³¼ ê´€ë ¨ëœ ìˆ˜ìƒ ê²½í—˜ì„ ì ì–´ë†¨ê¸° ë•Œë¬¸ ^^;;&lt;/p&gt;

&lt;p&gt;ìºê¸€ê°™ì€ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ëŒ€íšŒ ìƒê¸ˆë„ ì–´ë§ˆì–´ë§ˆí•˜ê³  ë§ì´ë‹¤ã…‹ã…‹ã…‹.&lt;/p&gt;

&lt;p&gt;ìºê¸€ëŸ¬ë“¤ì˜ ì¶”ì²œì„ ë°›ì•„ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;íŒŒì´ì¬ ë¨¸ì‹ ëŸ¬ë‹ ì™„ë²½ ê°€ì´ë“œ&lt;/code&gt; (ë¶€ì œ: ê³µë£¡ì±…)ì„ ê³µë¶€í•˜ê¸° ì‹œì‘í–ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì¸í”„ëŸ°ì—ì„œ ê°•ì˜ ë“¤ìœ¼ë©´ì„œ í—ˆê²ì§€ê² ì§„ë„ë¥¼ ëºë˜ ê¸°ì–µì´ ë‚œë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ ë•Œ ì½”ë¡œë‚˜ê°€ ë§‰ ìœ í–‰í•˜ê¸° ì‹œì‘ì¸ ë•Œë¼ ì¹œêµ¬ë‘ ê³µë¶€ë‚˜ í•˜ìëŠ” ìƒê°ìœ¼ë¡œ ì‹œì‘í–ˆì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì†ë¬¼ì ìœ¼ë¡œ ì‹œì‘í•œ ê³µë¶€ì´ì§€ë§Œ, ë‚˜ëŠ” ë”¥ëŸ¬ë‹ì„ í¬í•¨í•œ ì¸ê³µì§€ëŠ¥ì— ê´€ì‹¬ì´ ë§ì•˜ê¸° ë•Œë¬¸ì— ì‹¤ìƒí™œ ë°ì´í„°ì— ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©ì‹œí‚¤ëŠ” ê²ƒì´ ë‚˜ì—ê²Œ í° í¥ë¯¸ë¡œ ë‹¤ê°€ì™”ë‹¤.&lt;/p&gt;

&lt;p&gt;ì§€ê¸ˆì€ ë¬¼ë¡  ë°ì´í„° ë¶„ì„ì´ ì´ëŠ” ì „ë¶€ëŠ” ì•„ë‹ˆë¼ëŠ” ê²ƒì€ ì•Œê³  ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì–´ì¨Œë“  ì´ ëŠë‚Œ? ì´ ìƒê° ë•ë¶„ì— ê³µë¶€ë¥¼ ìˆ˜ì›”í•˜ê²Œ í•  ìˆ˜ ìˆì—ˆê³ , ìºê¸€ í•„ì‚¬ ì»¤ë¦¬í˜ëŸ¼ì„ ë”°ë¼ ê°€ë©´ì„œ(ì™„ì „ ì¡°ê¸ˆ ë”°ë¼ê°”ë‹¤ ã…‹ã…‹ã…‹ ê·¸ë•Œê°€ ê°œê°•í•  ë•Œ ì¯¤ ë˜ì–´ì„œ..) ë‚˜ì¤‘ì— ê¼­ ìºê¸€ì— ì°¸ê°€í•´ì„œ ìƒì„ ë°›ê³  ì‹¶ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ëŠ” ì¢€ ì´ë”° ì„œìˆ í•  HAICONì— ì…ìƒí•  ìˆ˜ ìˆê²Œ ëœ ê³„ê¸°ê°€ ëœë‹¤ã…ã…&lt;/p&gt;

&lt;h3 id=&quot;3ï¸âƒ£-skt-ai-fellowship-2ê¸°-ì§€ì›ê³¼-íƒˆë½&quot;&gt;3ï¸âƒ£ SKT AI Fellowship 2ê¸° ì§€ì›ê³¼ íƒˆë½&lt;/h3&gt;

&lt;!-- ![image](https://user-images.githubusercontent.com/48315997/103409204-00359580-4ba9-11eb-8ed6-6fa981f29d59.png) --&gt;
&lt;p&gt;&lt;img width=&quot;550&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103409204-00359580-4ba9-11eb-8ed6-6fa981f29d59.png&quot; /&gt;
ì„œë¥˜ í•©ê²© ğŸ˜†
&lt;!-- ![image](https://user-images.githubusercontent.com/48315997/103409209-088dd080-4ba9-11eb-8f5a-787e4b3e0068.png) --&gt;
&lt;img width=&quot;550&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103409209-088dd080-4ba9-11eb-8f5a-787e4b3e0068.png&quot; /&gt;
ë©´ì ‘ íƒˆë½ ğŸ˜‚&lt;/p&gt;

&lt;p&gt;======&lt;/p&gt;

&lt;p&gt;ì–´ì©Œë‹¤ ì•Œê²Œ ëœ SKT AI Fellowship.&lt;/p&gt;

&lt;p&gt;SKTì—ì„œ ê³µëŒ€ ëŒ€í•™(ì›)ìƒì—ê²Œ ì¸ê³µì§€ëŠ¥ê³¼ ê´€ë ¨ëœ ëª‡ëª‡ í”„ë¡œì íŠ¸ë¥¼ ì œì‹œí•˜ê³ , í° ê·œëª¨ë¡œ ì§€ì›í•´ì£¼ëŠ” í”„ë¡œê·¸ë¨ì´ì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ìˆ˜í™”ì¸ì‹ í”„ë¡œì íŠ¸ë¥¼ ê°™ì´í•œ íŒ€ì›ë“¤ì—ê²Œ ë¬¼ì–´ë³¸ í›„, ê°™ì´ ì¤€ë¹„í•˜ê¸°ë¡œ í–ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì£¼ì œëŠ” ë‚´ê°€ ì¢‹ì•„í•˜ëŠ”(ã…‹ã…‹ã…‹) ë¦¬ê·¸ì˜¤ë¸Œë ˆì „ë“œì˜ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;í•˜ì´ë¼ì´íŠ¸ ìë™ ì¶”ì¶œ&lt;/code&gt; ì—°êµ¬ í”„ë¡œì íŠ¸ì˜€ë‹¤.&lt;/p&gt;

&lt;p&gt;ë¦¬ê·¸ì˜¤ë¸Œë ˆì „ë“œë¼ ë”± ì§€ì¹­í•˜ì§€ëŠ” ì•Šê³ , ìŠ¤í¬ì¸ ë¼ê³  ë§í–ˆì§€ë§Œ ë‚´ê°€ ì›Œë‚™ ë¡¤ì„ ì¦ê²¨ë³´ê¸° ë•Œë¬¸ì— ì´ ì£¼ì œë¡œ ì–´í•„ì„ ê°•í•˜ê²Œ í–ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ê´€ë ¨ ì—°êµ¬ ë…¼ë¬¸ë“¤, ë‹¤ë¥¸ ìŠ¤í¬ì¸ ë“¤(ex.ì•¼êµ¬)ì— ì ìš©ëœ í•˜ì´ë¼ì´íŠ¸AI ê¸°ìˆ  ë“±ì„ ì¡°ì‚¬í•˜ì—¬ ìš°ë¦¬ë„ ì•„ì´ë””ì–´ë¥¼ ëƒˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ë•ŒëŠ” ì •ë§ ë³„ ê¸°ëŒ€ ì•ˆí–ˆëŠ”ë°, ë©´ì ‘ê¹Œì§€ ì˜¬ë¼ê°”ë‹¤! ë©´ì ‘ì€ ê° ì£¼ì œë‹¹ ìµœì¢… 3íŒ€(ìš°ë¦¬ ì£¼ì œëŠ” ê·¸ë¬ëŠ”ë° í™•ì‹¤í•˜ì§„ ì•Šë‹¤)ì´ ì˜¬ë¼ê°€ë©°, ì´ ì¤‘ 1íŒ€ì´ ìµœì¢… íŒ€ìœ¼ë¡œ ì„ ì •ëœë‹¤.&lt;/p&gt;

&lt;p&gt;ìƒê°ì§€ë„ ëª»í•œ ê²°ê³¼ì— ìš°ë¦¬ëŠ” ë¹ ë¥´ê²Œ ë©´ì ‘ ëŒ€ë¹„ë¥¼ í–ˆë‹¤. ìŠ¤í”„ë ˆë“œì‹œíŠ¸ì— ì˜ˆìƒ ì§ˆë¬¸ê³¼ ê·¸ì— ëŒ€í•œ ë‹µë³€ë“¤ì„ ëª¨ë‘ ì¤€ë¹„í–ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë˜ í”„ë¡œì íŠ¸ë¥¼ ì–´ë–»ê²Œ ì§„í–‰í•  ê²ƒì¸ì§€ ë°œí‘œí•´ì•¼ í–ˆëŠ”ë° ì´ëŠ” ë‚´ê°€ ë§¡ì•˜ì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì½”ë¡œë‚˜ë¡œ ì¸í•´ ì‹¤ì œ ë©´ì ‘ì€ Zoomìœ¼ë¡œ ì§„í–‰ë˜ì—ˆë‹¤. ë©´ì ‘ê´€ì€ 3ëª…ì´ì—ˆê³  ì¹œì ˆí•˜ê³  í¸í•˜ê²Œ í•´ì£¼ì…¨ë‹¤. ë¬¼ë¡  ê¸°ìˆ ì ì¸ ì–˜ê¸°ì— ëŒ€í•´ì„œëŠ” ë‚ ì¹´ë¡­ê³  ì˜ˆë¦¬í•œ ì§ˆë¬¸, ë¹„íŒë“¤ì„ í•´ì£¼ì…¨ë‹¤.&lt;/p&gt;

&lt;p&gt;ì‚¬ì‹¤ ë©´ì ‘ ë¶„ìœ„ê¸°ê°€ êµ‰ì¥íˆ ì¢‹ì•˜ê³ , ìš°ë¦¬ê°€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì˜ í–ˆê¸°ì— ê¸°ëŒ€ë¥¼ í•˜ê³  ìˆë˜ ìƒíƒœì˜€ë‹¤ã… ã… &lt;/p&gt;

&lt;p&gt;í•˜ì§€ë§Œ ì•„ì‰½ê²Œë„ íƒˆë½ í†µë³´ë¥¼ ë°›ì•˜ë‹¤.&lt;/p&gt;

&lt;p&gt;ì§€ê¸ˆ ì™€ì„œ ê¸ì •ì ìœ¼ë¡œ ìƒê°í•´ë³´ìë©´, ìš°ë¦¬ê°€ ê·¸ ê¸°ìˆ ì„ ì •ë§ êµ¬í˜„í–ˆì„ ìˆ˜ ìˆëŠ” ê¸°ìˆ  ìˆ˜ì¤€ì´ì—ˆëŠ”ì§€ëŠ” ëª¨ë¥´ê² ë‹¤. ê½¤ ì–´ë ¤ìš´ ë‚œì´ë„ë¥¼ ê°€ì§„ ì—°êµ¬ë¶„ì•¼/ê¸°ìˆ ì´ê¸° ë•Œë¬¸ì´ë‹¤.
ë¬¼ë¡  ì§„ì§œ ëë‹¤ë©´ ì •ë§ ì—´ì‹¬íˆ ê³µë¶€í•˜ê³  ë…¸ë ¥í•´ì„œ ì™„ì„±í•˜ë„ë¡ ë§Œë“¤ì—ˆê² ì§€ë§Œâ€¦ ê¸ì • íšŒë¡œë¥¼ ëŒë¦°ë‹¤ë©´ ã…‹ã…‹ã…‹ ì•„ì§ ë‚´ê°€ ë¶€ì¡±í•˜ë‹¤ëŠ” ì´ì•¼ê¸°ë‹ˆê¹Œâ€¦&lt;/p&gt;

&lt;p&gt;ê·¸ë˜ë„ ì´ ë•ë¶„ì—, ì—­ì‹œ ë‚´ íŒ€ì›ë“¤ì´ ì •ë§ ì¢‹ì€ ì‚¬ëŒì´ì ìš°ìˆ˜í•œ ì‚¬ëŒë“¤ì´ì—ˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë‚´ê°€ ì•„ì§ ë§ì´ ë¶€ì¡±í•´ì„œ ì´ë ‡ê²Œ ëœ ê²°ê³¼ë¼ëŠ” ê²ƒì„ ì¸ì •í•˜ê³ , ì•ìœ¼ë¡œ ë” ì—´ì‹¬íˆ ê³µë¶€í•˜ê³  ë…¸ë ¥í•´ì•¼ê² ë‹¤ëŠ” ë§ˆìŒê°€ì§ì„ ë‹¤ì‹œê¸ˆ ê°€ì§ˆ ìˆ˜ ìˆì—ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;4ï¸âƒ£-ë…¼ë¬¸-ìŠ¤í„°ë””&quot;&gt;4ï¸âƒ£ ë…¼ë¬¸ ìŠ¤í„°ë””&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/103409526-a209b200-4baa-11eb-9057-7f390de142a8.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì˜¬í•´ ê°€ì¥ í° ìˆ˜í™• ì¤‘ í•˜ë‚˜ëŠ” ì¡¸í”„ ë©”ì´íŠ¸ë¥¼ ì°¾ì€ ê²ƒì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ê³¼ì— ì•„ëŠ” ì‚¬ëŒë“¤ì´ ë§ì§€ ì•Šê¸°ë„ í•´ì„œ í˜¼ì í•´ì•¼ í•˜ë‚˜ ë§ì´ ê±±ì •í–ˆëŠ”ë° ë¨¼ì € ì œì•ˆí•´ì¤€ ê²ƒì´ êµ‰ì¥íˆ ê³ ë§ˆì› ë‹¤ ã…ã…&lt;/p&gt;

&lt;p&gt;ë¹„ìŠ·í•œ ëª©í‘œë¥¼ ê°€ì§€ê³  ìˆê³  ì„œë¡œ ì—´ì •, ì˜ì§€ë„ ê°€ë“í•˜ê¸° ë•Œë¬¸ì— ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆìœ¼ë¦¬ë¼ í™•ì‹ í•œë‹¤.&lt;/p&gt;

&lt;p&gt;ì•„ë¬´íŠ¼, ì´ ì¹œêµ¬ì™€ 7ì›”ë§ë¶€í„° &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ë…¼ë¬¸ ì½ê¸° ìŠ¤í„°ë””&lt;/code&gt;ë¥¼ ì‹œì‘í–ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì²˜ìŒ ì‹œì‘í•  ë•ŒëŠ” ë…¼ë¬¸ì´ë¼ëŠ” ê²ƒì´ êµ‰ì¥íˆ ë¶€ë‹´ìŠ¤ëŸ½ê²Œ ë‹¤ê°€ì™”ëŠ”ë°, ì´ì œëŠ” ì•„~ì£¼ ì¡°ê¸ˆì€ ìµìˆ™í•´ì ¸ì„œ í¬ê²Œ ê±°ë¶€ê°ì´ë‚˜ ë¶€ë‹´ê°ì€ ë“¤ì§€ ì•Šì•„ì¡Œë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ê²ƒë§Œ í•´ë„ êµ‰ì¥í•œ ë°œì „ ì•„ë‹Œê°€? ã…‹ã…‹ã…‹&lt;/p&gt;

&lt;p&gt;ì´ ë…¼ë¬¸ ìŠ¤í„°ë””ë¥¼ í†µí•´ ë‚´ê°€ ì–»ê²Œ ëœ ê²ƒì´ ìˆë‹¤ë©´, â€˜ë‚´ê°€ ì´ ë¶„ì•¼ë¥¼ ì˜¤ë«ë™ì•ˆ ì¢‹ì•„í•  ìˆ˜ ìˆê² êµ¬ë‚˜â€™ë¼ëŠ” ìƒê°ì´ì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ê³µë¶€ê°€ ì§€ê²¹ì§€ê°€ ì•Šê³ , ë‚´ê°€ í•´ì•¼ í•  ê²ƒì´ ì‚°ë”ë¯¸ë¼ëŠ” ê²ƒì„ ì•Œì§€ë§Œ ê·¸ê²ƒì´ ë‘ë µê±°ë‚˜ ë¶ˆì•ˆí•˜ì§€ ì•Šì•˜ë‹¤.&lt;/p&gt;

&lt;p&gt;ê·¸ë˜ë„ ë‚˜ëŠ” ì•„ì§ ë§ì´ ë¶€ì¡±í•˜ë‹¤! í•˜ë£¨ì— ë…¼ë¬¸ í•œí¸ì”© ë§¤ì¼ë§ˆë‹¤ ë¿Œì…”ë„ ì•„ì§ ëª¨ìë¼ë‹¤.&lt;/p&gt;

&lt;p&gt;ì§€ê¸ˆì€ ìš°ë¦¬ ë‘˜ ë‹¤ Domain Adaptation, GAN ìª½ ì—°êµ¬ì— ê´€ì‹¬ì„ ê°€ì§€ê²Œ ë˜ì–´ ê·¸ ìª½ ë…¼ë¬¸ë“¤ì„ ì½ì–´ë³´ê³  êµ¬í˜„í•˜ê³  ìˆë‹¤. (ì•„ì§ ë§ì´ëŠ” ëª»í–ˆì§€ë§Œ ã…‹ã…‹)&lt;/p&gt;

&lt;h3 id=&quot;5ï¸âƒ£-haicon-2020-ì…ì„ &quot;&gt;5ï¸âƒ£ HAICON 2020 ì…ì„ &lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/103409708-5c011e00-4bab-11eb-9f49-86fcfd219ff2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì•„ê¹Œ ì˜¬í•´ ì´ˆì— ë°ì´í„°ë¶„ì„ ê³µë¶€ë¥¼ í–ˆë‹¤ê³  í–ˆëŠ”ë°, ì´ ë•Œ ê°™ì´ í•œ ì¹œêµ¬ì™€ ëŒ€íšŒë¥¼ ë‚˜ê°€ê¸°ë¡œ í–ˆë‹¤.&lt;/p&gt;

&lt;p&gt;êµ­ë‚´ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì»´í”¼í‹°ì…˜ì„ ëª¨ì•„ë†“ì€ í”Œë«í¼ì¸ ë°ì´ì½˜ì—ì„œ í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ëŒ€íšŒì—ì„œ ê°€ì¥ ë§ì€ ìƒê¸ˆì„ ë¿Œë¦¬ëŠ” ëŒ€íšŒë¥¼ ê³¨ëë‹¤ ã…‹ã…‹ã…‹.&lt;/p&gt;

&lt;p&gt;ê¸ˆì•¡ë„ ê·¸ë ‡ì§€ë§Œ, êµ­ê°€ì •ë³´ì›ê³¼ êµ­ê°€ë³´ì•ˆê¸°ìˆ ì—°êµ¬ì†Œê°€ ì§„í–‰í•œë‹¤ëŠ” ì ì—ì„œë„ í° í¥ë¯¸?ë¥¼ ëŒì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì²« ì°¸ê°€í•˜ëŠ” ëŒ€íšŒì´ë‹ˆ ë§Œí¼ ì •ë§ ë§ì€ ì‹œë„ë“¤ì„ í•´ë³´ì•˜ë‹¤. ì‚¬ì‹¤ 1ë“±ê¹Œì§€ëŠ” í¬ê²Œ ë°”ë¼ì§€ëŠ” ì•Šì•˜ì§€ë§Œ ìˆ˜ìƒ ìš•ì‹¬ì€ ë¶„ëª…íˆ ìˆì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ëŒ€íšŒ ì´ˆì—ëŠ” ìˆ˜ìƒê¶Œì— ìˆì—ˆì§€ë§Œ ì¤‘í›„ë°˜ë¶€ë¡œ ë“¤ë©´ì„œ ê³ ìˆ˜ë“¤ì´ ë§ì´ ì°¸ê°€í–ˆëŠ”ì§€ ì¡°ê¸ˆì”© ì¡°ê¸ˆì”© ë°€ë ¤ë‚¬ê³ , ê²°ê³¼ì ìœ¼ë¡œ ëŒ€íšŒ ë§ˆì§€ë§‰ ì¦ˆìŒì—” ìˆ˜ìƒê¶Œì—ì„œ ê½¤ ë©€ì–´ì¡Œì—ˆë‹¤ã… ã…  (Public score ê¸°ì¤€)&lt;/p&gt;

&lt;p&gt;ê·¼ë° ì´ê²Œ ë¬´ìŠ¨ ì¼ì¸ì§€, ì „ì²´ í…ŒìŠ¤íŠ¸ ì…‹ìœ¼ë¡œ ìµœì¢… ê²°ê³¼ë¥¼ ëŒë¦° Private scoreì—ì„œ 12ìœ„ê°€ ë‚˜ì˜¨ ê²ƒì´ë‹¤! 10ìœ„ê¹Œì§€ê°€ ìˆ˜ìƒê¶Œì´ë¼ ì•„ ì´ê±° ì§„ì§œ ì•„ê¹ë‹¤, ì¡°ê¸ˆë§Œ ë” ë…¸ë ¥í•  ê±¸ ì°°ë‚˜ í›„íšŒí•˜ê¸´ í–ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ê·¸ë˜ë„ ë‚˜ë‘ ë‚´ ì¹œêµ¬ëŠ” ì •ë§ ìµœëŒ€í•œì˜ ë…¸ë ¥ì„ í–ˆê³ , ì •ë§ ë§ì€ ì‹¤í—˜ì„ ê±°ì³¤ê¸° ë•Œë¬¸ì— í° ìƒìŠ¹ì€ ì—†ì—ˆì„ì§€ë„ ëª¨ë¥¸ë‹¤. (ì²˜ìŒì´ë‹¤ ë³´ë‹ˆ ì–‘ìœ¼ë¡œ ìŠ¹ë¶€í–ˆì—ˆë‹¤)&lt;/p&gt;

&lt;p&gt;ê·¸ë˜ë„ ì¡°ê¸ˆ ê¸°ëŒ€ë¥¼ í’ˆê³ , ë³´ê³ ì„œì™€ ìµœì¢… ì œì¶œë³¸ì„ ì œì¶œí•œ ê²°ê³¼ ë‚´ ìœ„ì— 2ëª…ì´ ê·œì¹™ ìœ„ë°˜ í˜¹ì€ ë­ê°€ ë¶€ì¡±í–ˆëŠ”ì§€ ìš°ë¦¬ê°€ í„±ê±¸ì´ë¡œ ì…ì„ ì„ í•˜ê²Œ ëœ ê²ƒì´ë‹¤!! ì§„ì§œ ê¸°ë»¤ë‹¤ ì´ë•Œ ã…ã…&lt;/p&gt;

&lt;p&gt;í›„ê¸°ë¥¼ ìì„¸íˆ í’€ê³  ì‹¶ì§€ë§Œ ì™¸ë¶€ì— ë°œì„¤í•˜ì§€ ë§ë¼ëŠ” ì‚¬í•­ì´ ë§ì•˜ì–´ì„œ ì´ì— ëŒ€í•œ í›„ê¸°ëŠ” ì´ê²Œ ë!! ì •ë§ ë©‹ì§„ ì‚¬ëŒë“¤ë„ ë§ì´ ë´¤ê³ , í•™ë¶€ìƒì€ ë‚˜ë‘ í•œ íŒ€ë°–ì— ì—†ì—ˆë˜ ê²ƒ ê°™ì€ë° êµ‰ì¥íˆ ì˜ê´‘ì´ì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë‚˜ì¤‘ì—ëŠ” ë  ìˆ˜ ìˆìœ¼ë©´ ìºê¸€ ëŒ€íšŒì— ì…ìƒí•˜ê³  ì‹¶ë‹¤ ã…ã…&lt;/p&gt;

&lt;h3 id=&quot;6ï¸âƒ£-12í•™ê¸°-ëª¨ë‘-4ì ëŒ€ë¡œ-ë§ˆë¬´ë¦¬&quot;&gt;6ï¸âƒ£ 1,2í•™ê¸° ëª¨ë‘ 4ì ëŒ€ë¡œ ë§ˆë¬´ë¦¬&lt;/h3&gt;

&lt;p&gt;ì˜¬í•´ 1í•™ê¸°ëŠ” ì •ë§ í˜ë“¤ì—ˆë‹¤. ê½¤ ë¹¡ì„¼ ê³¼ëª©ë“¤ì„ ë“£ê¸°ë„ í–ˆê³ , ì½”ë¡œë‚˜ê°€ í„°ì§€ê³  ì²« ë¹„ëŒ€ë©´ ìˆ˜ì—…ì´ë¼ ì´ë¦¬ì €ë¦¬ ì •ì‹ ì—†ì—ˆë˜ ê²ƒ ê°™ë‹¤.&lt;/p&gt;

&lt;p&gt;ë•Œë¬¸ì— 1í•™ê¸°ì—ëŠ” ì •ë§ í•™ê¸° ê³µë¶€ ì™¸ì—ëŠ” ë‹¤ë¥¸ ê³µë¶€ë¥¼ í•  ìˆ˜ê°€ ì—†ì—ˆë‹¤ ã… ã… &lt;/p&gt;

&lt;p&gt;ë‹¤ë¥¸ í•™êµë‚˜ íƒ€ê³¼ë“¤ì€ í•™ì  ì˜ ë¿Œë ¤ì£¼ë˜ë° ã…ã…â€¦ ë‚´ê°€ ë“¤ì€ ìˆ˜ì—…ë“¤ì€ ë‹¤ ì½”ë¡œë‚˜ ì´ì „ì´ë‘ ë¹„ìŠ·í•˜ê±°ë‚˜ ì˜¤íˆë ¤ ë” ì§œê¸°ë„ ^^..&lt;/p&gt;

&lt;p&gt;1í•™ê¸°ëŠ” 6ì „ê³µ 1êµì–‘ 21í•™ì  ì´ìˆ˜, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4.09/4.3&lt;/code&gt;ì´ì—ˆê³ &lt;/p&gt;

&lt;p&gt;2í•™ê¸°ëŠ” 6ì „ê³µ 1êµì–‘ 19í•™ì  ì´ìˆ˜, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4.2/4.3&lt;/code&gt;ì´ì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;2í•™ê¸°ì— ë“¤ì€ êµì–‘ìˆ˜ì—…ì€ 2017ë…„ì— ë“¤ì—ˆë˜ ê³¼ëª© í•˜ë‚˜ë¥¼ ì¬ìˆ˜ê°•í•œ ê²ƒì´ë¼ 1-2ë“± ì ìˆ˜ì˜€ì„í…ë° ì¬ìˆ˜ê°• ìµœëŒ€ í•™ì ì¸ A-ë¥¼ ë°›ì•„ì„œ ê°œì¸ì ìœ¼ë¡œ ì•„ì‰¬ì› ë‹¤.
ê·¸ë¦¬ê³  2í•™ê¸°ëŠ” ê±°ì˜ ëª¨ë“  ê³¼ëª©ì„ 1ë“±, 2ë“± ì•„ë‹ˆë©´ ëª»í•´ë„ 5ë“± ì •ë„ í–ˆê¸° ë•Œë¬¸ì— ê°œì¸ì ìœ¼ë¡œ ë§˜ì— ë“œëŠ” í•™ê¸°ì˜€ë‹¤.&lt;/p&gt;

&lt;p&gt;ì‚¬ì‹¤ 2í•™ê¸°ì— ë“¤ì€ ê°•ì˜ë“¤ì´ ì‰½ë‹¤ê³  ëŠê¼ˆì„ ë•Œê°€ ì¢…ì¢… ìˆì—ˆë‹¤. ë‚´ê°€ ì˜¬í•´ ë§ì€ ì„±ì¥ì„ í–ˆë‹¤ëŠ” ê²Œ ëŠê»´ì ¸ì„œ ë¿Œë“¯í–ˆì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;í•™ê¸°ëŠ” ì•„ì§ ê½¤ ë‚¨ì•˜ì§€ë§Œ, ë‚¨ì€ ì´ìˆ˜ í•™ì ì€ ê·¸ë ‡ê²Œ ë§ì§€ ì•Šë‹¤. ë‚¨ì€ í•™ì ë“¤ë„ ëª¨ë‘ A+ì„ ë°›ì•„ì„œ ìµœìš°ë“±ì¡¸ì—…ì„ í•  ìˆ˜ ìˆê²Œ ë˜ë©´ ì¢‹ê² ë‹¤!&lt;/p&gt;

&lt;h3 id=&quot;7ï¸âƒ£-2021ë…„-ëª©í‘œ&quot;&gt;7ï¸âƒ£ 2021ë…„ ëª©í‘œ&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://images.unsplash.com/photo-1546074177-31bfa593f731?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&amp;amp;ixlib=rb-1.2.1&amp;amp;auto=format&amp;amp;fit=crop&amp;amp;w=1267&amp;amp;q=80&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì˜¬ í•œ í•´ë¥¼ ë˜ëŒì•„ë³´ë‹ˆ ì •ë§ ë§ì€ ì¼ì´ ìˆì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì½”ë¡œë‚˜ë•Œë¬¸ì— ì‚¬ëŒë“¤ì„ ë§ì´ ëª» ë§Œë‚¬ì§€ë§Œ, ì´ê²Œ ë‚˜ì—ê²Œ ê¸ì •ì ìœ¼ë¡œ ì‘ìš©ë  ìˆ˜ ìˆì—ˆë˜ ì ì€ ë¶„ëª…íˆ ìˆì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;í˜¼ì ê³µë¶€í•  ì‹œê°„ì´ ëŠ˜ì–´ë‚˜ ì „ë³´ë‹¤ ê½¤ ë§ì´ ì„±ì¥í–ˆë‹¤ëŠ” ì ì´ ë°”ë¡œ ê·¸ê²ƒì´ë‹¤.&lt;/p&gt;

&lt;p&gt;12ì›” ì´ˆì— ì¢‹ì€ ì†Œì‹ìœ¼ë¡œëŠ” &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ETRI&lt;/code&gt; (í•œêµ­ì „ìí†µì‹ ì—°êµ¬ì›) ë™ê³„ ì—°êµ¬ì—°ìˆ˜ìƒì— í•©ê²©í–ˆë‹¤ëŠ” ê²ƒì´ë‹¤!&lt;/p&gt;

&lt;p&gt;ë‹¤ìŒì£¼ 1/2ì¼ë¶€í„° 2/26ê¹Œì§€ ëŒ€ì „ì—ì„œ ì¼ì„ í•˜ê²Œ ë˜ì—ˆëŠ”ë° êµ‰ì¥íˆ ì„¤ë Œë‹¤.&lt;/p&gt;

&lt;p&gt;ì¸í„´ê¸‰ì´ê¸° ë•Œë¬¸ì— ë§ì€ ì¼ì„ ì‹œí‚¤ì§€ ì•Šì„ ê²ƒ ê°™ì§€ë§Œ, ë‚˜ ìŠ¤ìŠ¤ë¡œ ë§ì´ ë°°ìš°ë ¤ ë…¸ë ¥í•˜ê³  ë˜ í•œ ë‹¨ê³„ ì„±ì¥í•˜ëŠ” ê³„ê¸°ê°€ ë˜ì—ˆìœ¼ë©´ ì¢‹ê² ë‹¤.&lt;/p&gt;

&lt;p&gt;ì¼ë‹¨ 2021ë…„ ëª©í‘œëŠ” í¬ê²Œ 2ê°€ì§€ì´ë‹¤.&lt;/p&gt;

&lt;h4 id=&quot;ì¡¸ì—…-í”„ë¡œì íŠ¸-ì„±ê³µì --í•´ì™¸-íƒ‘-ì»¨í¼ëŸ°ìŠ¤ì—-ë…¼ë¬¸-accept-ë˜ê¸°&quot;&gt;ì¡¸ì—… í”„ë¡œì íŠ¸ ì„±ê³µì  == í•´ì™¸ íƒ‘ ì»¨í¼ëŸ°ìŠ¤ì— ë…¼ë¬¸ Accept ë˜ê¸°&lt;/h4&gt;

&lt;p&gt;ë‚˜ì—ê²Œ ìˆì–´ 2021ë…„ì€ ê°€ì¥ ì¤‘ìš”í•œ í•œ í•´ì´ë‹¤.
ì‚¬ì‹¤ìƒ ì´ë¥¼ ìœ„í•´ ì§€ê¸ˆê¹Œì§€ ì¤€ë¹„ë¥¼ í–ˆë‹¤ê³  ë´ë„ ê³¼ì–¸ì´ ì•„ë‹ˆë‹¤. ì¡¸í”„ ì‹œì‘ì„ í•œ í•™ê¸° ë¯¸ë£¬ ê²ƒë„ ì´ë¥¼ ìœ„í•´ ê³µë¶€í•˜ê¸° ìœ„í•´ì„œì˜€ë‹¤.&lt;/p&gt;

&lt;p&gt;ë§ˆì¹¨ ì¢‹ì€ ë©”ì´íŠ¸ê°€ ìˆê³ , ë‚˜ì™€ ë¹„ìŠ·í•˜ê²Œ ì•¼ë§ë„ í¬ê¸°ì— ìš°ë¦¬ ëª©í‘œëŠ” ìƒë‹¹íˆ ë†’ê²Œ ì¡ì•˜ë‹¤.&lt;/p&gt;

&lt;p&gt;ì‚¬ì‹¤ ì €ê²ƒë³´ë‹¤ í¬ë‹¤. Best Student Paper ìƒì„ ë°›ê³  ì‹¶ë‹¤!!!!!&lt;/p&gt;

&lt;p&gt;ëˆ„êµ¬ë³´ë‹¤ ì—´ì‹¬íˆ ê³µë¶€í•˜ê³ , ëˆ„êµ¬ë³´ë‹¤ ì˜ í•´ì•¼ ëœë‹¤ëŠ” ê²ƒì„ ì•ˆë‹¤. 
ë‚˜ëŠ” ë¬´ì¡°ê±´ í›„íšŒì—†ëŠ” 1ë…„ì„ ë³´ë‚¼ ê²ƒì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ê¼­ ì´ ëª©í‘œê°€ ì„±ê³µí•  ìˆ˜ ìˆë„ë¡, ìµœì„ ì„ ë‹¤í•˜ê² ë‹¤.&lt;/p&gt;

&lt;h4 id=&quot;all-a&quot;&gt;All A+&lt;/h4&gt;

&lt;p&gt;ì•ì„  ëª©í‘œì— ë¹„í•˜ë©´ ì†Œì†Œí•˜ë‹¤.
ì¡¸í”„ ë•Œë¬¸ì— í•™ì ì€ 15~16 í•™ì  ì •ë„? ë§ì§€ ì•Šê²Œ, ë¶€ë‹´ì´ ë˜ì§€ ì•ŠëŠ” ì„ ê¹Œì§€ë§Œ ë“¤ì„ ê²ƒì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ëŒ€ì‹ ì— ë¬´ì¡°ê±´ ì˜¬ì—ì´ì !&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="íšŒê³ " /><category term="Year Review" /><summary type="html">2020ë…„ ì €ì—ê²Œ ìˆì—ˆë˜ ì¼ì„ ì§§ê²Œ íšŒê³ í•˜ëŠ” ê¸€ì…ë‹ˆë‹¤.</summary></entry><entry><title type="html">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</title><link href="http://localhost:4000/paper%20review/2020/12/30/CycleGAN/" rel="alternate" type="text/html" title="Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks" /><published>2020-12-30T00:00:00+09:00</published><updated>2020-12-30T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/12/30/CycleGAN</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/12/30/CycleGAN/">&lt;p&gt;Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (CycleGAN) ì„ ì½ê³  ê°œì¸ì ìœ¼ë¡œ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;ì•ì„œ ì‚´í´ë³¸ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DCGAN&lt;/code&gt;ì—°êµ¬ëŠ” í° ë°œì „ì„ ì´ë£¨ì—ˆì§€ë§Œ, ì´ ì—­ì‹œ unstableí•˜ì—¬ mode collapseê°€ ì¼ì–´ë‚  ìˆ˜ ìˆëŠ” í•œê³„ì ì´ ëšœë ·í–ˆìŠµë‹ˆë‹¤.&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="GAN" /><summary type="html">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (CycleGAN) ì„ ì½ê³  ê°œì¸ì ìœ¼ë¡œ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.</summary></entry><entry><title type="html">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)</title><link href="http://localhost:4000/paper%20review/2020/12/25/DCGAN/" rel="alternate" type="text/html" title="Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)" /><published>2020-12-25T00:00:00+09:00</published><updated>2020-12-25T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/12/25/DCGAN</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/12/25/DCGAN/">&lt;p&gt;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)ì„ ì½ê³  ê°œì¸ì ìœ¼ë¡œ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;unsupervised-representation-learning-with-deep-convolutional-generative-adversarial-networks-dcgan&quot;&gt;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;GANì€ Representation Learningì— íš¨ê³¼ì ì´ë¼ê³  í•©ë‹ˆë‹¤. (ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ê²ƒì´ GANì˜ learning processì™€ ê´€ë ¨ì´ ìˆê³ , lack of heuristic cost function ë•ë¶„ì´ë¼ê³  ë§í•©ë‹ˆë‹¤.)&lt;/p&gt;

&lt;p&gt;ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ì²˜ìŒ ë‚˜ì˜¨ GANì€ í•™ìŠµí•˜ëŠ” ê²ƒì— ìˆì–´ ë§¤ìš° &lt;strong&gt;unstable&lt;/strong&gt;í•˜ë‹¤ëŠ” ë¬¸ì œì ì„ ê°€ì§€ê³  ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë³¸ ë…¼ë¬¸ì˜ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DCGAN&lt;/code&gt;ì€ í•´ë‹¹ ë¬¸ì œì  í•´ê²°ì„ í¬í•¨í•˜ì—¬ ì´ 4ê°€ì§€ì˜ Contributionsë¡œ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;DCGANì€ stable trainingì´ ê°€ëŠ¥í•˜ë‹¤&lt;/li&gt;
  &lt;li&gt;í•™ìŠµëœ DiscriminatorëŠ” image classification íƒœìŠ¤í¬ì— ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë‹¤. (ë‹¤ë¥¸ unsupervised ì•Œê³ ë¦¬ì¦˜ë“¤ê³¼ ë¹„êµí•  ê²ƒì„.)&lt;/li&gt;
  &lt;li&gt;GANì´ í•™ìŠµí•œ filtersë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆê³ , íŠ¹ì • ì˜¤ë¸Œì íŠ¸ì— ëŒ€í•´ íŠ¹ì • filterë¥¼ í•™ìŠµí–ˆë‹¤ëŠ” ì ì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆë‹¤.&lt;/li&gt;
  &lt;li&gt;GeneratorëŠ” vector arithmetic propertiesë¥¼ ê°€ì§€ê³  ìˆë‹¤.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;approach-and-model-architecture&quot;&gt;APPROACH AND MODEL ARCHITECTURE&lt;/h2&gt;

&lt;p&gt;&lt;img width=&quot;705&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-12-25 á„‹á…©á„’á…® 5 31 03&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103127996-f73f5280-46d6-11eb-9936-996a9ae0e84f.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ìš°ì„  DCGANì´ ê¸°ì¡´ì˜ GANê³¼ architecture ì¸¡ë©´ì—ì„œ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ì–´ë–»ê²Œ&lt;/code&gt; ë‹¬ëê¸°ì— í° ì„±ê³¼ë¥¼ ì´ë£° ìˆ˜ ìˆì—ˆëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;DCGANì´ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ì—ë„ CNNì„ ì´ìš©í•œ GANì„ ë§Œë“œëŠ” ì‹œë„ëŠ” ê³„ì† ìˆì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ì‹œë„ë“¤ì€ ëª¨ë‘ ì„±ê³µì ì´ì§€ ëª»í–ˆì£ .&lt;/p&gt;

&lt;p&gt;DCGANì€ &lt;strong&gt;CNN Architectureì—ì„œì˜ ìµœì‹  ë³€í™”(?) 3ê°€ì§€ë¥¼ ì ìš©&lt;/strong&gt;í•˜ì—¬ ì„±ê³µí•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;all-convolutional-net-ì‚¬ìš©&quot;&gt;All Convolutional Net ì‚¬ìš©&lt;/h3&gt;

&lt;p&gt;ìš°ì„  ê°€ì¥ í° ë³€í™”ì ì€ &lt;a href=&quot;https://arxiv.org/abs/1412.6806&quot;&gt;Striving for Simplicity: The All Convolutional Net&lt;/a&gt; ì„ ì°¸ê³ í•˜ì—¬ All Convoultional Netì„ ì‚¬ìš©í–ˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;All conv. netì€ &lt;strong&gt;pooling functions(ì˜ˆë¥¼ ë“¤ì–´, max pooling)ë¥¼ strided conv. ìœ¼ë¡œ ë°”ê¾¼ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°&lt;/strong&gt;ì…ë‹ˆë‹¤. Max poolingì€ ë¯¸ë¶„ë˜ì§€ ì•ŠëŠ” ì„±ì§ˆì„ ê°€ì§„ë‹¤ê³  í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë¥¼ í†µí•´ Generatorì™€ Discriminator ëª¨ë‘ ìì‹ ë“¤ì˜ spatial downsamplingì„ í•™ìŠµí•˜ê¸°ì— ì í•©í•´ì§‘ë‹ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;eliminating-fully-connected-layersfc-layers-on-top-of-convolutional-features&quot;&gt;Eliminating fully connected layers(FC layers) on top of convolutional features&lt;/h3&gt;

&lt;p&gt;ì´ ì‹œì  trendëŠ” ë§ˆì§€ë§‰ì— FC layerë¥¼ ì œê±°í•˜ê³ , &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;global average pooling&lt;/code&gt;ë¥¼ ì“°ëŠ” ë°©ì‹ì´ì—ˆë‹¤ê³  í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë‹¤ë§Œ, ì´ global average poolingì€ &lt;strong&gt;ëª¨ë¸ì˜ ì•ˆì •ì„±ì€ ì˜¬ë¦¬ëŠ” ë°˜ë©´ì— convergence speedëŠ” ë–¨ì–´ëœ¨ë¦¬ëŠ”&lt;/strong&gt; trade-off ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;batch-norm--relu-activation&quot;&gt;Batch Norm &amp;amp; ReLU activation&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Batch Normalization&lt;/strong&gt;ëŠ” í•™ìŠµì„ ì•ˆì •í™”ì‹œí‚¬ ìˆ˜ ìˆëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. (normalizing the input to each unit to have zero mean and unit variance)&lt;/p&gt;

&lt;p&gt;ì´ê²ƒì€ í•™ìŠµ ë¬¸ì œì  ì¤‘ í•˜ë‚˜ì¸ initializationê³¼ deep modelì˜ gradient flowì— í° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;í•˜ì§€ë§Œ, ëª¨ë“  layerì— BNì„ ì ìš©ì‹œí‚¤ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤.&lt;/strong&gt; Generatorì˜ output layerì™€ Discriminatorì˜ input layerì—ëŠ” BNì„ ì ìš©ì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë§ˆì§€ë§‰ìœ¼ë¡œ &lt;strong&gt;ReLUë¥¼ ì‚¬ìš©í–ˆë‹¤&lt;/strong&gt;ëŠ” ì ì´ ì–¸ê¸‰ë˜ì–´ ìˆëŠ”ë°, Generatorì™€ Discriminatorì— ì ìš©ë˜ëŠ” functionì´ ì•½ê°„ ë‹¤ë¦…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;Generatorì—ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ReLUë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, output layerì—ëŠ” ë”°ë¡œ ReLUê°€ ì•„ë‹Œ Tanh functionì„ ì ìš©ì‹œí‚µë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;Discriminatorì—ëŠ” Leaky ReLUë¥¼ ì ìš©í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;(ì—¬ëŸ¬ ì‹¤í—˜ ê²°ê³¼ ì´ functionsë“¤ì´ ì¢‹ì€ ì„±ëŠ¥ì„ ëƒˆê¸° ë•Œë¬¸ì´ê² ì£ ? í™•ì‹¤í•œ ì´ìœ , ë…¼ì¦ì€ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;details-of-adversarial-training&quot;&gt;DETAILS OF ADVERSARIAL TRAINING&lt;/h2&gt;

&lt;p&gt;DCGANì€ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LSUN, FACES, IMAGENET-1K&lt;/code&gt; ë°ì´í„°ì…‹ì— ëŒ€í•˜ì—¬ í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;training setting(parameters, â€¦)ì— ëŒ€í•´ì„œëŠ” ë…¼ë¬¸ì„ ì°¸ê³ í•´ì£¼ì‹œê³ , ìœ„ ë°ì´í„°ì…‹ ì¤‘ LSUNì— ëŒ€í•´ì„œë§Œ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤ :)&lt;/p&gt;

&lt;h3 id=&quot;lsun&quot;&gt;LSUN&lt;/h3&gt;

&lt;p&gt;LSUNì€ &lt;em&gt;Large-scale Scene Understanding&lt;/em&gt;ì˜ ì¤„ì„ë§ë¡œ, bedroom ì‚¬ì§„ë“¤ì„ ëª¨ì€ ë°ì´í„° ì…‹ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ ë°ì´í„°ì…‹ì„ ê°€ì§€ê³  í•™ìŠµì‹œí‚¨ ëª¨ë¸ë¡œ ìƒì„±ëœ ì´ë¯¸ì§€ëŠ” qualityê°€ ìƒë‹¹íˆ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;í•˜ì§€ë§Œ ì´ê²ƒì´ &lt;strong&gt;over-fittingì´ ë˜ì–´ ì´ë ‡ê²Œ ëœ ê²ƒì¸ì§€, í•™ìŠµ ë°ì´í„°ì…‹ì— ëŒ€í•˜ì—¬ ê¸°ì–µ(memorization)í•˜ì—¬ ë§Œë“¤ì–´ì§„ ê²ƒì¸ì§€&lt;/strong&gt;ì— ëŒ€í•´ íŒë³„í•´ë´ì•¼ í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;overfitting?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;700&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-12-25 á„‹á…©á„’á…® 5 45 38&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103128506-00c9ba00-46d9-11eb-8b7a-e144208bbfe8.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ë³¸ ë…¼ë¬¸ì˜ Fig.2 ì™€ Fig.3ì„ í†µí•´ ëª¨ë¸ì´ ì˜¤íˆë ¤ &lt;strong&gt;underfitting&lt;/strong&gt; ë˜ì–´ìˆìŒì„ ì–˜ê¸°í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;underfittingì´ ì´ë£¨ì–´ì¡Œë‹¤ê³  ì—¬ê¸°ëŠ” ì´ìœ ëŠ” ì•„ì§ noise textureê°€ ëˆˆì— ë³´ì´ê¸° ë•Œë¬¸ì´ë¼ê³  í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(ì—„ì²­ ê°€ê¹Œì´ ë³´ì§€ ì•ŠëŠ” ì´ìƒì€ ì˜ ëŠë¼ì§€ ëª»í•˜ê² ëŠ”ë° ë§ì´ì£ .)&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;memorization?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ì‚¬ì‹¤ ì´ê²Œ ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„ì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œ generate í•˜ëŠ” imageê°€ ì‚¬ì‹¤ í•™ìŠµ ë°ì´í„°ì—ì„œ ê¸°ì–µ(memorize)í•˜ì—¬ ë§Œë“¤ì–´ì§„ ê²ƒì´ë¼ë©´, &lt;strong&gt;ì§„ì •í•œ ì˜ë¯¸ì˜ Generateê°€ ì•„ë‹ˆë‹ˆê¹ìš”.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ì´ì— ëŒ€í•´ì„œ ë³¸ ë…¼ë¬¸ì˜ ì €ìë“¤ì€ memorizeí•˜ëŠ” ê°€ëŠ¥ì„±ì„ ì¤„ì´ê¸° ìœ„í•´ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;image de-duplication process&lt;/code&gt;(ì¤‘ë³µ ì œê±° í”„ë¡œì„¸ìŠ¤)ë¥¼ ê±°ì¹©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;de-duplicationì„ í•˜ê¸° ìœ„í•´ autoencoderë¥¼ í•˜ë‚˜ ë§Œë“­ë‹ˆë‹¤.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We fit a 3072-128-3072 de-noising dropout regularized RELU autoencoder on 32x32 downsampled center-crops of training examples.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;empirical-validation-of-dcgans-capabilities&quot;&gt;EMPIRICAL VALIDATION OF DCGANs CAPABILITIES&lt;/h2&gt;

&lt;p&gt;ë§¨ ì²˜ìŒì— ì´ ë…¼ë¬¸ì˜ contributions ì¤‘ í•˜ë‚˜ë¡œ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;í•™ìŠµëœ DiscriminatorëŠ” image classification íƒœìŠ¤í¬ì— ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë‹¤&lt;/code&gt; ë¼ê³  ì–˜ê¸°í–ˆì—ˆì£ ?&lt;/p&gt;

&lt;p&gt;ì´ ëª©ì°¨ì—ì„œëŠ” ì •ë§ DCGANì´ &lt;strong&gt;feature extractor&lt;/strong&gt;ë¡œì¨ì˜ ì—­í• ì´ ê°€ëŠ¥í•œê°€, ê·¸ë˜ì„œ CIFAR-10 ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œë„ &lt;strong&gt;classification task&lt;/strong&gt;ë¥¼ ì˜ ìˆ˜í–‰í•˜ëŠ”ê°€ë¥¼ í™•ì¸í•´ë´…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;696&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-12-25 á„‹á…©á„’á…® 6 03 36&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103129277-8484a600-46db-11eb-9e0f-de26d03694d4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ê²°ë¡ ì ìœ¼ë¡œ, ë‹¤ë¥¸ unsupervised ì•Œê³ ë¦¬ì¦˜ì€ K-means modelë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. (Exemplar CNN ëª¨ë¸ë³´ë‹¤ëŠ” ì¢€ ëª» ë¯¸ì¹˜ì§€ë§Œìš”.)&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;542&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-12-25 á„‹á…©á„’á…® 6 05 55&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103129364-d6c5c700-46db-11eb-8f7b-388b5ae3128b.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì¶”ê°€ì ìœ¼ë¡œ SVHN Digits ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œë„ ì‹¤í—˜ì„ í•´ë³´ì•˜ìŠµë‹ˆë‹¤. test error ì¸¡ë©´ì—ì„œ SOTAë¥¼ ë‹¬ì„±í•˜ëŠ” ì¾Œê±°ë¥¼ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;investigating-and-visualizing-the-internals-of-the-networks&quot;&gt;INVESTIGATING AND VISUALIZING THE INTERNALS Of THE NETWORKS&lt;/h2&gt;

&lt;p&gt;ë˜ ë‹¤ë¥¸ contributionsìœ¼ë¡œ  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GANì´ í•™ìŠµí•œ filtersë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆê³ , íŠ¹ì • ì˜¤ë¸Œì íŠ¸ì— ëŒ€í•´ íŠ¹ì • filterë¥¼ í•™ìŠµí–ˆë‹¤ëŠ” ì ì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆë‹¤&lt;/code&gt; ì™€ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GeneratorëŠ” vector arithmetic propertiesë¥¼ ê°€ì§€ê³  ìˆë‹¤&lt;/code&gt; ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ ëª©ì°¨ì—ì„œëŠ” ë‘ ë¶€ë¶„ì— ëŒ€í•´ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;walking-in-the-latent-space&quot;&gt;Walking in the Latent Space&lt;/h3&gt;

&lt;p&gt;latent spaceë¥¼ ë³€ê²½í–ˆì„ ë•Œ sharp transitions(ê¸‰ì‘ìŠ¤ëŸ¬ìš´ ë³€í™”)ê°€ ìˆìœ¼ë©´ ì´ëŠ” &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memorization&lt;/code&gt;ì´ ì¼ì–´ë‚¬ë‹¤ëŠ” ì‹ í˜¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë°˜ëŒ€ë¡œ ë¶€ë“œëŸ¬ìš´ ë³€í™”ê°€ ì¼ì–´ë‚˜ë©´ memorizationì´ ëœ ê²ƒì´ ì•„ë‹ˆë¼ ì œëŒ€ë¡œ í•™ìŠµì´ ë˜ì—ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆì£ .&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/103134579-83607280-46f5-11eb-800b-b4d5ea21bdad.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ìœ„ì˜ ì‚¬ì§„ì„ ë³´ì‹œë©´ DCGANì˜ ê²½ìš° sharp transitionì´ ì•„ë‹Œ smoothí•œ ë³€í™”ê°€ ì´ë£¨ì–´ì¡ŒìŒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;visualizing-the-discriminator-features&quot;&gt;Visualizing the Discriminator Features&lt;/h3&gt;

&lt;p&gt;ì´ ë‚´ìš©ì—ì„œëŠ” Guided backpropagationì„ í†µí•´ GANì´ í•™ìŠµí•œ filtersë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;696&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-12-25 á„‹á…©á„’á…® 9 17 38&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103134734-9f184880-46f6-11eb-9639-d0eb9878e74a.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;discriminatorê°€ featureë“¤ì„ í•™ìŠµí•´ì„œ, íŠ¹ì • íŒŒíŠ¸(bed, windows,â€¦)ë“¤ì— ëŒ€í•˜ì—¬ active í•˜ê³  ìˆìŒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;manipulating-the-generator-representation&quot;&gt;Manipulating the Generator Representation&lt;/h3&gt;

&lt;h4 id=&quot;forgetting-to-draw-certain-objects&quot;&gt;Forgetting to Draw Certain Objects&lt;/h4&gt;

&lt;p&gt;ì´ê±´ ë§¤ìš° ì¬ë¯¸ìˆëŠ” ì‹¤í—˜ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ê°„ë‹¨í•˜ê²Œ ìš”ì•½í•˜ìë©´, Generatorê°€ ë¬´ìŠ¨ representationì„ í•™ìŠµí–ˆëŠ”ì§€ ì•Œì•„ë³´ê¸° ìœ„í•˜ì—¬ íŠ¹ì • filter(ì—¬ê¸°ì„œëŠ” window filter)ë¥¼ ì‚­ì œí•´ë´…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì¦‰, Windowë¼ëŠ” objectë¥¼ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Forget&lt;/code&gt; í•˜ê²Œ ë˜ëŠ” ê²ƒì´ì£ .(ê¸°ìˆ ì ìœ¼ë¡œëŠ” window filterë¥¼ dropout ì‹œí‚¨ë‹¤ê³  ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)&lt;/p&gt;

&lt;p&gt;ê²°ê³¼ì ìœ¼ë¡œ ì´ ì‹¤í—˜ì—ì„œëŠ” ì°½ë¬¸ì´ ì•„ë‹Œ ë‹¤ë¥¸ representations, objectsê°€ ë“¤ì–´ê°€ê²Œ ë©ë‹ˆë‹¤!&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;684&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-12-25 á„‹á…©á„’á…® 9 22 03&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103134811-3da4a980-46f7-11eb-8d95-b990a0a18601.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ìœ„ì˜ ê²°ê³¼ë¥¼ ë³´ë©´ ì•„ì‹œë‹¤ì‹œí”¼, ì°½ë¬¸ì´ì—ˆë˜ ê²ƒì´ ë¬¸ìœ¼ë¡œ ë°”ë€ŒëŠ” ë“± ë‹¤ë¥¸ objectë¥¼ ìƒì„±í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;h4 id=&quot;vector-arithmetic-on-face-samples&quot;&gt;Vector Arithmetic On Face Samples&lt;/h4&gt;

&lt;p&gt;ë§ì€ ë¶„ë“¤ì´ ê°€ì¥ ì¬ë°Œì–´í•˜ì‹¤(?) ë¶€ë¶„ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;word embedding ê´€ë ¨í•´ì„œ vector(â€œKingâ€) - vector(â€œManâ€) + vector(â€œWomanâ€)ê°€ vector(â€œQueenâ€)ì˜ ê²°ê³¼ê°€ ë‚˜ì˜¤ë“¯ì´, DCGANì—ì„œë„ ì´ì™€ ë¹„ìŠ·í•œ &lt;strong&gt;arithmeticí•œ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤&lt;/strong&gt;ê³  ë°í˜”ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;Generatorì˜ inputì¸ Z vectorì— ëŒ€í•œ arithmetic operationì„ í•˜ëŠ”ë°, single sampleë¡œëŠ” ë¶ˆì•ˆì •í•˜ì—¬ 3ê°œì˜ Z vectorë¥¼ í‰ê· í•œ ê°’ì„ ì‚¬ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;580&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-12-25 á„‹á…©á„’á…® 9 36 16&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103135086-3a122200-46f9-11eb-931d-7d737e8c5ed1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;smiling woman - neutral woman + neutral man = smiling man ì´ë¯¸ì§€ê°€ ë§Œë“¤ì–´ì§€ëŠ” ë§ˆë²•ê°™ì€ ê¸°ìˆ ì„ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì•ì„œ ë§í–ˆë‹¤ì‹œí”¼ 3ê°œì˜ Z vectorë¥¼ averageí•˜ì—¬ ìƒˆë¡œìš´ Yë²¡í„°ë¥¼ ë§Œë“  ê²ƒë„ í™•ì¸í•´ë³¼ ìˆ˜ ìˆì£ .&lt;/p&gt;

&lt;p&gt;ì´ê²Œ ë‹¤ê°€ ì•„ë‹™ë‹ˆë‹¤. &lt;strong&gt;face pose&lt;/strong&gt; ë˜í•œ Z spaceì— ì„ í˜•ì ìœ¼ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;557&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-12-25 á„‹á…©á„’á…® 9 38 01&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103135120-780f4600-46f9-11eb-8d7e-8d723c10e439.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ë°”ë¡œ ì´ë ‡ê²Œ ë§ì´ì£ . 
ì´ë¯¸ ì´ì „ë¶€í„° scale, rotation, positionì— ëŒ€í•˜ì—¬ conditional generative modelì€ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ê³  ì—°êµ¬ë˜ì–´ì™”ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ì—°êµ¬ëŠ” &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;purely unsupervised model&lt;/code&gt; ì´ë¼ëŠ” ì ì—ì„œ í° ë³€í™˜ì ì´ ëœ ê²ƒì´ì£ .&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;FUTURE WORK&lt;/h2&gt;

&lt;p&gt;ì‚¬ì‹¤ stablityë¥¼ ì™„ì „íˆ í•´ê²°í•œ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;DCGANì„ ì˜¤ë«ë™ì•ˆ í•™ìŠµí•˜ê²Œ ë˜ë©´ collapse mode, oscillating modeê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ì§ë„ &lt;strong&gt;ë¶ˆì•ˆì •ì„±&lt;/strong&gt;ì´ ë‚¨ì€ ê²ƒì´ì£ .&lt;/p&gt;

&lt;p&gt;ê·¸ë˜ì„œ ì´ ë…¼ë¬¸ì—ì„œëŠ” í•´ë‹¹ ë¬¸ì œì ì„ Future workë¡œ ë‚¨ê¸°ê³  ë§ˆë¬´ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://jaejunyoo.blogspot.com/2017/02/deep-convolutional-gan-dcgan-1.html_&quot;&gt;Jaejun Yooâ€™s Playground&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;DCGANë„ ì½ì—ˆìœ¼ë‹ˆ, êµ¬í˜„ë„ í•´ë³´ê³  í›„ì† ë…¼ë¬¸ë“¤ë„ ì°¬ì°¬íˆ ì½ì–´ë³´ë ¤ê³  í•©ë‹ˆë‹¤.
ê¸´ ê¸€ ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤ :)&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="GAN" /><summary type="html">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)ì„ ì½ê³  ê°œì¸ì ìœ¼ë¡œ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.</summary></entry><entry><title type="html">Multimodal Unsupervised Image-to-Image Translation (MUNIT)</title><link href="http://localhost:4000/paper%20review/2020/12/22/MUNIT/" rel="alternate" type="text/html" title="Multimodal Unsupervised Image-to-Image Translation (MUNIT)" /><published>2020-12-22T00:00:00+09:00</published><updated>2020-12-22T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/12/22/MUNIT</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/12/22/MUNIT/">&lt;p&gt;Multimodal Unsupervised Image-to-Image Translation (MUNIT) ì„ ì½ê³  ê°„ëµí•˜ê²Œ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;multimodal-unsupervised-image-to-image-translation-munit&quot;&gt;Multimodal Unsupervised Image-to-Image Translation (MUNIT)&lt;/h1&gt;

&lt;h2 id=&quot;keywords--gan-image-to-image-translation-style-transfer&quot;&gt;Keywords : GAN, Image-to-Image translation, style transfer&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;image representation = content code + style code
    &lt;ul&gt;
      &lt;li&gt;content code : domain invariant&lt;/li&gt;
      &lt;li&gt;style code : domain specific&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;657&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-12-22 á„‹á…©á„Œá…¥á†« 12 14 37&quot; src=&quot;https://user-images.githubusercontent.com/48315997/102791653-adaaeb00-43ea-11eb-9dd6-fc7d4e6c6e6e.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Image Translation == Recombine its content code with a random style code
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;random style code&lt;/code&gt; : style space of the target domain&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;651&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-12-22 á„‹á…©á„Œá…¥á†« 12 14 23&quot; src=&quot;https://user-images.githubusercontent.com/48315997/102791642-a5eb4680-43ea-11eb-8ea5-8af9ebc97f17.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;auto-encoder-architecture&quot;&gt;Auto-encoder Architecture&lt;/h3&gt;

&lt;p&gt;&lt;img width=&quot;624&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-12-22 á„‹á…©á„Œá…¥á†« 12 19 33&quot; src=&quot;https://user-images.githubusercontent.com/48315997/102792116-61ac7600-43eb-11eb-8a7b-3f4cc735c0f1.png&quot; /&gt;&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="GAN" /><category term="Domain Adaptation" /><category term="Style Transfer" /><category term="Image-to-Image Translation" /><summary type="html">Multimodal Unsupervised Image-to-Image Translation (MUNIT) ì„ ì½ê³  ê°„ëµí•˜ê²Œ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.</summary></entry><entry><title type="html">Unsupervised Intra-domain Adaptation for Semantic Segmentation</title><link href="http://localhost:4000/paper%20review/2020/12/22/intraDA/" rel="alternate" type="text/html" title="Unsupervised Intra-domain Adaptation for Semantic Segmentation" /><published>2020-12-22T00:00:00+09:00</published><updated>2020-12-22T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/12/22/intraDA</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/12/22/intraDA/">&lt;p&gt;Unsupervised Intra-domain Adaptation for Semantic Segmentation ì„ ì½ê³  ê°„ëµí•˜ê²Œ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;unsupervised-intra-domain-adaptation-for-semantic-segmentation&quot;&gt;Unsupervised Intra-domain Adaptation for Semantic Segmentation&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;automatically annotated data&lt;/code&gt; has a problem.
    &lt;ul&gt;
      &lt;li&gt;synthetic data -&amp;gt; real data&lt;/li&gt;
      &lt;li&gt;directly adapting models from the source data to the unlabeled target data (to reduce the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inter-domain gap&lt;/code&gt;)&lt;/li&gt;
      &lt;li&gt;But result? ==&amp;gt; bad :(
        &lt;ul&gt;
          &lt;li&gt;there is the large distribution gap among the target data itself(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intra-domain gap&lt;/code&gt;)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;p&gt;&lt;img width=&quot;300&quot; src=&quot;https://user-images.githubusercontent.com/48315997/102793576-7984f980-43ed-11eb-8cbe-ca0d2986d14e.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;595&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-12-22 á„‹á…©á„Œá…¥á†« 12 44 17&quot; src=&quot;https://user-images.githubusercontent.com/48315997/102794504-d3d28a00-43ee-11eb-9b61-4eb50fde5d1d.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;two-step self-supervised domain adaptation approach to minimize the inter-domain and intra-domain gap together.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;dl&gt;
      &lt;dt&gt;inter-domain gap&lt;/dt&gt;
      &lt;dd&gt;separate the target domain into an easy &amp;amp; hard split (using entropy-based ranking function)&lt;/dd&gt;
    &lt;/dl&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;dl&gt;
      &lt;dt&gt;intra-domain gap&lt;/dt&gt;
      &lt;dd&gt;self-supervised adaption from the easy to hard split&lt;/dd&gt;
    &lt;/dl&gt;
    &lt;ul&gt;
      &lt;li&gt;segmentation predictions of easy split data (from G_inter) =&amp;gt; pseudo labels ë¡œ ì‚¬ìš©&lt;/li&gt;
      &lt;li&gt;Given easy split data &amp;amp; pseudo labels, hard split data =&amp;gt; D_intraëŠ” easy? hard? íŒë³„&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;&lt;img width=&quot;698&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-12-22 á„‹á…©á„Œá…¥á†« 12 46 37&quot; src=&quot;https://user-images.githubusercontent.com/48315997/102794749-26ac4180-43ef-11eb-853c-45c2e9a5cfd4.png&quot; /&gt;&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Domain Adaptation" /><category term="GAN" /><category term="Semantic Segmentation" /><summary type="html">Unsupervised Intra-domain Adaptation for Semantic Segmentation ì„ ì½ê³  ê°„ëµí•˜ê²Œ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.</summary></entry><entry><title type="html">DETR:End-to-End Object Detection with Transformers</title><link href="http://localhost:4000/paper%20review/2020/11/17/DETR/" rel="alternate" type="text/html" title="DETR:End-to-End Object Detection with Transformers" /><published>2020-11-17T00:00:00+09:00</published><updated>2020-11-17T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/11/17/DETR</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/11/17/DETR/">&lt;p&gt;DETR : End-to-End Object Detection with Transformers ë¥¼ ì½ê³  ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;ì˜¬í•´ ê°€ì¥ í° ì¸ê¸°ë¥¼ ëª°ê³  ìˆëŠ” ë…¼ë¬¸ ì¤‘ í•˜ë‚˜ì¸ DETRì„ ì½ì–´ë³´ê² ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;Yann Lecun êµìˆ˜ë‹˜ê»˜ì„œ ì‚¼ì„± AI í¬ëŸ¼ì—ì„œ DETR ë…¼ë¬¸ì„ ì–¸ê¸‰í•˜ë©´ì„œ Transformer + Vision ì˜ ì—°êµ¬ê°€ ì´ì œ ì£¼ ê³¼ì œì¼ ê²ƒì´ë¼ê³  ë§ì”€í•˜ì…¨ìŠµë‹ˆë‹¤. 
DETRì€ ë‚˜ì˜¤ìë§ˆì í° í™”ì œê°€ ë˜ì—ˆê¸°ì— ì‚´ì§ ì½ì–´ë³´ê³  ë¦¬ë·°ëŠ” ì•ˆí•˜ë ¤ í–ˆìœ¼ë‚˜, ViT ë…¼ë¬¸ì„ ì½ê³  ì €ë„ ì´ ë¶„ì•¼ì˜ ì¥ë˜ê°€ ìœ ë§í•˜ë‹¤ê³  ì¸ì‹í•˜ê²Œ ë˜ì–´ ì´ë ‡ê²Œ í•˜ë‚˜í•˜ë‚˜ ëœ¯ì–´ë´¤ìŠµë‹ˆë‹¤!&lt;/p&gt;

&lt;p&gt;í”¼ë“œë°±ì€ ì–¸ì œë‚˜ í™˜ì˜í•©ë‹ˆë‹¤! ëŒ“ê¸€ì°½ì„ ì•„ì§ ì—´ì§€ ëª»í•´ì„œ ë©”ì¼ë¡œ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ê¾¸ë²…(__)&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ì´ ë…¼ë¬¸ì—ì„œëŠ” object detectionì„ í•˜ë‚˜ì˜ &lt;strong&gt;direct set prediction&lt;/strong&gt; ë¬¸ì œë¡œ ë³¸ë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ ì ‘ê·¼ë²•ì€ &lt;strong&gt;Non-maximum suppression, anchor generation&lt;/strong&gt;ê³¼ ê°™ì€ ì‘ì—…ë“¤ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë°©ë²•ìœ¼ë¡œ, detection pipelineì„ ë§¤ìš° ê°„ì†Œí™”ì‹œí‚¬ ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” ëª¨ë¸ì€ DEtection Transformer(ì´í•˜ DETR)ì´ë‹¤. ì´ ëª¨ë¸ì˜ ì£¼ìš” íŠ¹ì§•ìœ¼ë¡œëŠ” object detectionì„ set prediction problemìœ¼ë¡œ ë³¸ë‹¤ í•˜ì˜€ìœ¼ë¯€ë¡œ, &lt;strong&gt;set ê¸°ë°˜ì˜ global lossë¥¼ ì‚¬ìš©&lt;/strong&gt;í•œë‹¤ëŠ” ì ì´ë‹¤. Transformer encoder-decoder êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ bipartite matchingì„ í†µí•´ unique predictionì„ í•œë‹¤.&lt;/p&gt;

&lt;p&gt;í•™ìŠµëœ object ì¿¼ë¦¬ë“¤ì˜ small setì´ ì£¼ì–´ì§€ë©´, DETRì€ ê°ì²´ë“¤ê³¼ ì „ì²´ ì´ë¯¸ì§€ context ê°„ì˜ ê´€ê³„ì— ëŒ€í•´ &lt;strong&gt;ë³‘ë ¬ì (parallel)&lt;/strong&gt; ìœ¼ë¡œ final setì— ëŒ€í•œ ì˜ˆì¸¡ì„ directly outputìœ¼ë¡œ ë‚´ë³´ë‚¸ë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ ëª¨ë¸ì€ libraryë„ í•„ìš”ì—†ê³ , ê°œë…ì ìœ¼ë¡œ ë§¤ìš° ì‹¬í”Œí•˜ë‹¤.
COCO datasetì— ëŒ€í•´ì„œ Faster R-CNN baseline ê¸‰ì˜ ì •í™•ë„ì™€ ëŸ°íƒ€ì„ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ë‹¤ê³  í•œë‹¤.&lt;/p&gt;

&lt;p&gt;ê²Œë‹¤ê°€, DETRì€ &lt;strong&gt;panoptic segmentation&lt;/strong&gt;ì„ ì‰½ê²Œ ì¼ë°˜í™”í•  ìˆ˜ ìˆë‹¤!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Object detectionì˜ ëª©í‘œëŠ” ë°”ìš´ë”© ë°•ìŠ¤ setê³¼ labelì„ ë§ì¶”ëŠ” ê²ƒì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ìµœê·¼ ëŒ€ë¶€ë¶„ì˜ detectorë“¤ì€ ì´ë¥¼ set of proposals, anchors, or window centersì— ëŒ€í•´ regression/classificationì„ ì‚¬ìš©í•˜ëŠ” â€˜ê°„ì ‘ì ì¸â€™ë°©ì‹ìœ¼ë¡œ í•´ê²°í•˜ë ¤ í•œë‹¤.&lt;/p&gt;

&lt;p&gt;í•˜ì§€ë§Œ ì´ëŸ° ë°©ì‹ë“¤ì€ postprocessing ë‹¨ê³„ì—ì„œ ì¤‘ë³µëœ ì˜ˆì¸¡ê°’ë“¤ì„ collapse ì‹œí‚¤ëŠ” ê³¼ì •ì´ë‚˜(NMSë¥¼ ì–˜ê¸°í•˜ëŠ” ë“¯í•˜ë‹¤), ì•µì»¤ ì…‹ì— ëŒ€í•´ ë¯¸ë¦¬ assigní•˜ëŠ” íœ´ë¦¬ìŠ¤í‹±í•œ ë°©ì‹ ë“±ì— ì˜í•´ performanceê°€ ì˜í–¥ ë°›ì„ ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;ê·¸ë˜ì„œ ì´ ë…¼ë¬¸ì—ì„œëŠ” ìœ„ì™€ ê°™ì€ &lt;em&gt;Indirect&lt;/em&gt; ë°©ì‹ì´ ì•„ë‹Œ &lt;strong&gt;&lt;em&gt;Direct&lt;/em&gt;&lt;/strong&gt; ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We propose a direct set prediction approach to bypass the surrogate tasks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ì´ì™€ ê°™ì€ end-to-end ë°©ì‹ì€ ê¸°ì¡´ì˜ object detection ì—°êµ¬ì—ì„œëŠ” ìì£¼ ë‹¤ë¤„ì§€ì§€ ì•Šì•˜ë˜ ê²ƒì´ë‹¤.&lt;/p&gt;

&lt;p&gt;Obejct detection ë¬¸ì œë¥¼ direct set prediction problemìœ¼ë¡œ ë§Œë“¤ì–´ training pipelineì„ ë§Œë“ ë‹¤.
ì´ ë•Œ, transformer ê¸°ë°˜ì˜ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ì˜ self-attention ê¸°ë²•ì€ elementsë“¤ ê°„ì˜ ëª¨ë“  pairwise interactionsë¥¼ í†µí•´ ì¤‘ë³µëœ predictionì„ ì œê±°í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.&lt;/p&gt;

&lt;p&gt;DETRì€ ëª¨ë“  objectë“¤ì„ &lt;strong&gt;í•œë²ˆì—&lt;/strong&gt; predictí•œë‹¤. ì´ëŠ” end-to-end íŠ¹ì„±ì¸ë°, ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ ì‚¬ì´ì—ì„œ set lossë¥¼ í†µí•´ bipartite matching(ì´ë¶„ ë§¤ì¹­)ì„ í›ˆë ¨í•œë‹¤. ì´ë¡œ ì¸í•´ DETRì€ hand-designed ì‘ì—… ì—†ì´ ê°„ë‹¨í•œ detection íŒŒì´í”„ë¼ì¸ì„ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ë‹¤ë¥¸ detection modelë“¤ê³¼ ë‹¤ë¥´ê²Œ DETRì€ customized layerë„ í•„ìš”í•˜ì§€ ì•Šì•„, ëª¨ë¸ ìì²´ë„ êµ‰ì¥íˆ ê°„ë‹¨í•˜ë‹¤. CNNê³¼ transformerë§Œ ê°€ì§€ê³  ìˆë‹¤ë©´ ì‰½ê²Œ reproduce í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ì „ì˜ direct set prediction ì—°êµ¬ë“¤ê³¼ ë¹„êµí•  ë•Œ, DETRì˜ ì£¼ìš”ì ì€ bipartite matching(ì´ë¶„ ë§¤ì¹­)ê³¼ parallel decodingì„ ê²°í•©ì‹œí‚¨ ê²ƒì´ë‹¤. 
DETRì˜ Matching loss functionì€ ì˜ˆì¸¡ê°’ì„ ground truthì— uniquely í• ë‹¹í•  ìˆ˜ ìˆê³ , predicted objectsì˜ ìˆœì—´ì´ ë³€í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë³‘ë ¬ì ìœ¼ë¡œ predictí•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;DETRì€ COCO datasetì— evaluate í•´ë´¤ì„ ë•Œ, Faster R-CNN baselineì— ëŒ€í•˜ì—¬ ê²½ìŸë ¥ ìˆëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤. íŠ¹íˆ í° objectë“¤ì— ëŒ€í•˜ì—¬ ìƒë‹¹íˆ ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤ê³  í•œë‹¤. &lt;em&gt;í•˜ì§€ë§Œ, small objectë“¤ì— ëŒ€í•´ì„œëŠ” low performanceë¥¼ ë³´ì˜€ë‹¤.&lt;/em&gt; ì´ëŠ” ì•„ë§ˆ Faster R-CNNì— FPNì„ ì ìš©í•˜ì—¬ ì¢‹ì€ ì„±ê³¼ë¥¼ ì´ë¤˜ë“¯ì´ future workì—ì„œë„ í•´ê²°ë²•ì´ ìƒê¸°ì§€ ì•Šì„ê¹Œ ê¸°ëŒ€í•œë‹¤.&lt;/p&gt;

&lt;p&gt;DETRì€ ë³µì¡í•œ ë¬¸ì œë¥¼ ì‰½ê²Œ í™•ì¥ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ì‹¤í—˜ì—ì„œëŠ” op pretrained DETRì„ ì´ìš©í•˜ì—¬ segmentation headë¥¼ í•™ìŠµì‹œì¼°ë”ë‹ˆ Panoptic Segmentationì— ê²½ìŸë ¥ì„ ê°€ì¡Œë‹¤ê³  í•œë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Our work build on prior work in several domains: bipartite matching losses for
set prediction, encoder-decoder architectures based on the transformer, parallel
decoding, and object detection methods.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;the-detr-model&quot;&gt;The DETR model&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;set predictionì„ ìœ„í•´ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†ŒëŠ” 2ê°œì´ë‹¤.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;a set prediction loss&lt;/strong&gt; that forces &lt;strong&gt;unique matching&lt;/strong&gt; between predicted and ground truth boxes&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;an architecture&lt;/strong&gt; that &lt;strong&gt;predicts&lt;/strong&gt; (in a single pass) a set of objects and models their relation&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;object-detection-set-prediction-loss&quot;&gt;Object detection set prediction loss&lt;/h2&gt;

&lt;p&gt;DETRì€ fixed-size setì—ì„œ Nê°œì˜ predictionì„ í•œë‹¤. &lt;strong&gt;ì´ ë•Œ Nì€ ì´ë¯¸ì§€ì˜ object ê°œìˆ˜ë³´ë‹¤ ìƒë‹¹íˆ í¬ê²Œ ì„¤ì •í•œë‹¤.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ground truthì— ëŒ€í•˜ì—¬ predicted objects(class, position, size)ë¥¼ scoringí•˜ëŠ” ê²ƒì´ í•™ìŠµê³¼ì •ì—ì„œ ì–´ë ¤ì› ë‹¤ê³  í•œë‹¤.
ì—¬ê¸°ì„œ lossëŠ” ì˜ˆì¸¡ê°’ê³¼ ground truth ì‚¬ì´ì˜ ìµœì ì˜ ì´ë¶„ ë§¤ì¹­ì„ ë§Œë“¤ì–´ ë‚´ê³ , ì´ë¥¼ í†µí•´ bounding box lossë¥¼ ìµœì í™”í•œë‹¤.&lt;/p&gt;

&lt;p&gt;$y$ë¥¼ ground truth, $\hat{y} = {\hat{y_i}}_{i=1}^N$ ì€ Nê°œì˜ prediction setì´ë¼ í•˜ì.&lt;/p&gt;

&lt;p&gt;Nì€ ì´ë¯¸ì§€ë‚´ì˜ object ê°œìˆ˜ë³´ë‹¤ í¬ë‹¤ê³  ê°€ì •í•˜ë©´, $y$ë„ a set of size Nì— $\varnothing$ (no object)ê°€ padded ëë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ëŸ¬í•œ y, y_hat ì‚¬ì´ì˜ bipartite matching(ì´ëŠ” ground truthì™€ prediction ì‚¬ì´ì˜ ì¼ëŒ€ì¼ ë§¤ì¹­ì´ë¼ ìƒê°í•˜ë©´ í¸í•  ë“¯) ì°¾ê¸° ìœ„í•´ì„œ, matching costê°€ minimumì´ ë˜ëŠ” ë°”ìš´ë”© ë°•ìŠ¤ì˜ ìˆœì„œë“¤, ì¦‰ ìˆœì—´ì„ ì°¾ì•„ì•¼ í•œë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ëŸ¬í•œ optimal assignentëŠ” í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜ì— ì˜í•´ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°ë  ìˆ˜ ìˆìŒ.&lt;/p&gt;

&lt;p&gt;ì´ì œ ë‹¤ì‹œ matching costì— ëŒ€í•´ ì•Œì•„ë³´ì. each element of &lt;em&gt;i&lt;/em&gt; of the ground truth setì€ $y_i = (c_i, b_i)$ ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŒ. ì´ ë•Œ c_iëŠ” target class label, b_iëŠ” box center coordinates(xyì˜ center, height, width) ì„.&lt;/p&gt;

\[L_{Hungarian}(y,\hat{y} = \mathbb{-1}_{c_i\neq\varnothing} + \mathbb{1}L_{box}(b_i, \hat{b_{\sigma(i)}})\]

&lt;p&gt;ì•ì˜ í•­ì— -1ì„ ê³±í•˜ëŠ” ì´ìœ ëŠ” í•´ë‹¹ í´ë˜ìŠ¤ probabilityê°€ ë†’ì„ìˆ˜ë¡ lossë¥¼ ë‚®ê²Œ ì‚°ì •í•˜ê¸° ìœ„í•¨ì´ê³ , ë’· í•­ì€ bounding boxì˜ ë¡œìŠ¤ë¥¼ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ì™€ ê°™ì€ ê³¼ì •ì—†ì´ ê·¸ëŒ€ë¡œ ì ìš©í•œë‹¤ëŠ” ê²ƒì´ ì¡°ê¸ˆ íŠ¹ì´í•œ ì§€ì ì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ì—¬ê¸°ì„œ ì´ ë…¼ë¬¸ì˜ ì°¨ë³„ì  í˜¹ì€ ì¤‘ìš”í•œ ì§€ì ì€ ì¤‘ë³µ(duplicates)ì—†ì´ direct set predictionì„ ìœ„í•œ ì¼ëŒ€ì¼ ë§¤ì¹­ì´ ëœë‹¤ëŠ” ì ì´ë‹¤. ì¦‰, ì¤‘ë³µë˜ëŠ” predictionì´ ì¡´ì¬í•˜ì§€ ì•Šë‹¤ëŠ” ëœ». &lt;strong&gt;(ì¼ëŒ€ì¼ ë§¤ì¹­ì„ í•˜ê¸° ë•Œë¬¸ì— ì¤‘ë³µë˜ëŠ” ì˜ˆì¸¡ì´ ìƒê¸°ì§€ ì•ŠìŒ -&amp;gt; NMSê°€ í•„ìš”ì—†ë‹¤!!)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;ë‹¤ë¥¸ ëª¨ë¸ì—ì„œëŠ” ì•µì»¤ë°•ìŠ¤ë¥¼ ë§ì´ ë½‘ì•„ì„œ í•˜ë‚˜ ë¬¼ì²´ì— ì—¬ëŸ¬ ê°œì˜ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ê·¸ë ¤ì„œ NMS(Non-maximum suppression)ì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ì—ˆìŒ!!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ì´ë ‡ê²Œ ì¼ëŒ€ì¼ ë§¤ì¹­ëœ predictionê³¼ ground truth ì‚¬ì´ì—ì„œëŠ” ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ í—ê°€ë¦¬ì•ˆ ê¸°ë°˜ì˜ lossë¥¼ ì‚¬ìš©í•´ì„œ í•™ìŠµí•¨. (ì¦‰ ë°©ê¸ˆí–ˆë˜ ê²ƒì€ matching costë¥¼ ê³„ì‚°í•˜ëŠ” lossì˜€ê³ , ì´ë²ˆì´ ì§„ì§œ ì˜ˆì¸¡ê°’ê³¼ Ground Truthì‚¬ì´ì˜ lossì¸ë“¯?)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99262847-c8c19280-2861-11eb-8c4a-ea37362777ae.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;L_matchì™€ ì•½ê°„ ë¹„ìŠ·í•´ë³´ì´ì§€ë§Œ, class í™•ë¥  ê°’ì„ no-object ì¼ ë•Œë„ ê³„ì‚°í•œë‹¤ëŠ” ì , logë¥¼ ì‚¬ìš©í•´ cross entropyê°™ì€ ì„±ì§ˆì´ ì¶”ê°€ë˜ì—ˆë‹¤ëŠ” ì ì´ ë‹¤ë¥´ë‹¤.
í´ë˜ìŠ¤ê°€ no_objectì¼ ë•Œ ì•ì˜ í•­ì˜ lossë¥¼ 1/10ìœ¼ë¡œ ì¤„ì—¬ class imbalanceë¥¼ ì¡°ì ˆí–ˆë‹¤ê³  í•œë‹¤.&lt;/p&gt;

&lt;p&gt;ë§ˆì§€ë§‰ìœ¼ë¡œ bounding box lossë¥¼ ì‚´í´ë³´ì. ë‹¤ë¥¸ detectorë“¤ì€ ë¯¸ë¦¬ ì •í•´ë†“ì€ ì•µì»¤ ë°•ìŠ¤ candidateë“¤ì„ ì–¼ë§ˆë‚˜ ì›€ì§ì¼ì§€ì— ëŒ€í•œ &lt;strong&gt;ë¸íƒ€&lt;/strong&gt;ë¥¼ í•™ìŠµí–ˆë‹¤. ê·¸ëŸ¬ë‚˜ DETRì—ì„œëŠ” ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ directly ì˜ˆì¸¡í•œë‹¤. L1 Lossë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, L1 lossì˜ ë‹¨ì ì€ ë°”ìš´ë”© ë°•ìŠ¤ê°€ í¬ë©´ ì‘ì€ ê±°ì— ë¹„í•´ lossê°€ ë” í¼. (ë‹¨ìˆœíˆ ì¢Œí‘œì˜ì°¨ì´ë§Œì„ ê³ ë ¤í•˜ë‹ˆê¹Œ)&lt;/p&gt;

&lt;p&gt;ì´ ë‹¨ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ Generalized IoU lossë¥¼ L1 Lossì— linear combination í–ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99262815-c0695780-2861-11eb-8170-8b6271d35ddd.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;detr-architecture&quot;&gt;DETR Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99263078-11794b80-2862-11eb-9452-ca8b498d812c.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;[ìš”ì•½]&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CNNìœ¼ë¡œ feature map ìƒì„±&lt;/li&gt;
  &lt;li&gt;íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ì— ë“¤ì–´ê°€ê¸° ìœ„í•´ì„œ positional encoding ë”í•¨&lt;/li&gt;
  &lt;li&gt;íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ì˜ ì¶œë ¥ì€ ë””ì½”ë”ì— ë“¤ì–´ê°
    &lt;ul&gt;
      &lt;li&gt;ì´ ê°’ë“¤ì´ &lt;strong&gt;key, value&lt;/strong&gt; ì—­í• ì„ í•¨.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ë””ì½”ë”ì˜ inputìœ¼ë¡œëŠ” Nê°œì˜ object &lt;strong&gt;query&lt;/strong&gt;ê°€ ë“¤ì–´ê°„ë‹¤.
    &lt;ul&gt;
      &lt;li&gt;ì´ ì¿¼ë¦¬ë„ í•™ìŠµí•˜ê²Œ ë¨. ì²˜ìŒì—ëŠ” ëœë¤ê°’ìœ¼ë¡œ ì±„ìš°ì§€ë§Œ, ì ì  í•™ìŠµí•˜ëŠ” ê²ƒ&lt;/li&gt;
      &lt;li&gt;ì¼ì¢…ì˜ positional encoding ì—­í• ì„ í•¨ (ì–´ëŠ ë¶€ë¶„ì„ ê´€ì‹¬ìˆê²Œ ë³¼ ê²ƒì¸ì§€)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ê°ê° FFNì„ í†µí•´ì„œ Nê°œë¥¼ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;DETRì˜ ì „ì²´ì ì¸ êµ¬ì¡°ëŠ” ìœ„ Figureê³¼ ê°™ë‹¤. í¬ê²Œ 3ê°€ì§€ ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ëœë‹¤.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;CNN backbone : CNNì„ ì´ìš©í•´ compact feature representationì„ ì¶”ì¶œí•œë‹¤.(feature map)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Encoder-Decoder Transformer&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A simple Feed Forward Network(FFN) : ê²°ê³¼ê°’ ì¶œë ¥&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;DETRì˜ êµ¬ì¡°ëŠ” ë§¤ìš° ê°„ë‹¨í•´ì„œ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì„ íŒŒì´í† ì¹˜ë¡œ êµ¬í˜„í•˜ë©´ 50ì¤„ì •ë„ì´ë‹¤! (ê¸€ í•˜ë‹¨ë¶€ì— ì²¨ë¶€í•˜ê² ìŒ.)&lt;/p&gt;

&lt;h3 id=&quot;backbone&quot;&gt;Backbone&lt;/h3&gt;

&lt;p&gt;ì‹¤í—˜ì—ì„œëŠ” ResNetì„ ì‚¬ìš©í•¨. H, WëŠ” 5ë²ˆ ì •ë„ì˜ down-scaleì„ í–ˆë‹¤.(H0/32, W0/32)
ìµœì¢… C = 2048ë¡œ ì‚¬ìš©(ResNet ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ê°€ 2048ì´ê¸° ë•Œë¬¸)&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;609&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-11-17 á„‹á…©á„Œá…¥á†« 12 10 58&quot; src=&quot;https://user-images.githubusercontent.com/48315997/99269155-61a7dc00-2869-11eb-9ee9-9d13ef280c23.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;transformer-encoder&quot;&gt;Transformer encoder&lt;/h3&gt;

&lt;p&gt;ìš°ì„  1x1 conv.ë¥¼ í†µí•´ ì±„ë„ì„ $d$ dimensionìœ¼ë¡œ ì¤„ì¸ë‹¤.&lt;/p&gt;

&lt;p&gt;ë˜ ì¸ì½”ë”ì— ë“¤ì–´ê°€ê¸° ìœ„í•´ì„  ë²¡í„°ë¡œ ë°”ë€Œì–´ì•¼ í•˜ë‹ˆê¹Œ 3ì°¨ì›ì„ 2ì°¨ì›ìœ¼ë¡œ ë°”ê¿”ì•¼ í•œë‹¤. 
H,Wë¥¼ ë‹¤ í•©ì³ì„œ spatial dimensions of d x HWë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.
ì´ ë•Œ HWëŠ” ì‹œí€€ìŠ¤ì˜ ê°œìˆ˜ê°€ ë˜ê³ , ê° ì‹œí€€ìŠ¤ ë²¡í„°ì˜ ì‚¬ì´ì¦ˆê°€ dê°€ ë˜ëŠ” ê²ƒì´ë‹¤.&lt;/p&gt;

&lt;p&gt;íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” attentionê¸°ë°˜ì´ê¸° ë•Œë¬¸ì— permutation-invariantí•˜ë‹¤. ì¦‰, ìˆœì„œë¥¼ ë¬´ì‹œí•œë‹¤ëŠ” ê²ƒì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ë•Œë¬¸ì—, ìˆœì„œë¥¼ ë¶€ì—¬í•˜ê¸° ìœ„í•´ fixed positional encodingì„ input of each attention layerì— ì¶”ê°€í•œë‹¤. 
&lt;em&gt;(ì°¸ê³ ) x, yì¶• ë”°ë¡œ d/2ì”© í•´ì„œ ì¸ì½”ë”©í•´ì„œ concatí•˜ëŠ” ì‹ìœ¼ë¡œ positional encodingì„ í•´ì¤€ë‹¤.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;transformer-decoder&quot;&gt;Transformer decoder&lt;/h3&gt;

&lt;p&gt;transformerì˜ ìŠ¤íƒ ë‹¤ë“œ ì•„í‚¤í…ì³ë¥¼ ë”°ëë‹¤ê³  í•œë‹¤.&lt;/p&gt;

&lt;p&gt;ë‹¤ë§Œ originalê³¼ì˜ ì°¨ì´ì ì€ ê° ë””ì½”ë” ë ˆì´ì–´ì—ì„œ Nê°œì˜ ê°ì²´ë¥¼ &lt;strong&gt;parallelí•˜ê²Œ ë””ì½”ë”©&lt;/strong&gt;í•œë‹¤ëŠ” ê²ƒì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ë””ì½”ë”ë¥¼ í†µê³¼í•˜ê²Œ ë˜ë©´ Nê°œì˜ ì˜¤ë¸Œì íŠ¸ê°€ parallelí•˜ê²Œ ë‚˜ì˜¤ëŠ”ë°, ì´ ë•Œ ë””ì½”ë”ì˜ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” ê²ƒì€ object queriesì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ë””ì½”ë” ì—­ì‹œ &lt;strong&gt;permutation-invariant&lt;/strong&gt; í•˜ê¸° ë•Œë¬¸ì— ì¼ì¢…ì˜ positional encodingì´ í•„ìš”í•˜ë‹¤. (ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” ê²ƒë“¤ì´ ë‹¤ ë‹¬ë¼ì•¼ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ë½‘ì•„ì¤„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤)&lt;/p&gt;

&lt;p&gt;ì—¬ê¸°ì„œ object queryê°€ positional encoding ì—­í• ì„ í•œë‹¤.
ì´ ì¿¼ë¦¬ëŠ” í•™ìŠµì˜ ëŒ€ìƒì´ ë˜ë©° each attention layerì˜ ì¸í’‹ìœ¼ë¡œ ì¶”ê°€ëœë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ê²ƒë“¤ì€ &lt;strong&gt;independently FFNì„ ì§€ë‚˜&lt;/strong&gt; ë°•ìŠ¤ ì¢Œí‘œ, í´ë˜ìŠ¤ ë¼ë²¨ë¡œ ë””ì½”ë“œëœë‹¤. (ê·¸ë ‡ê²Œ Nê°œì˜ final predictionì„ ì–»ê²Œ ëœë‹¤.)&lt;/p&gt;

&lt;p&gt;ê°œì¸ì ìœ¼ë¡œ ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„ì´ë¼ê³  ìƒê°í•˜ëŠ”ê²Œ, ì´ëŸ¬í•œ self- and encoder-decoder attention ëª¨ë¸ì€ imageë¥¼ &lt;strong&gt;global ì¶”ë¡ í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.&lt;/strong&gt; ì¦‰ use the whole image as contextë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤!!&lt;/p&gt;

&lt;h3 id=&quot;prediction-feed-forward-networks-ffns&quot;&gt;Prediction feed-forward networks (FFNs)&lt;/h3&gt;

&lt;p&gt;3-layer í¼ì…‰íŠ¸ë¡  with ReLU êµ¬ì¡°ì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ FFNì€ bounding boxì˜ normalized center coordinates, height, widthë¥¼ ì˜ˆì¸¡í•˜ë©°, softmax functionì„ í†µí•´ class labelì„ ì˜ˆì¸¡í•œë‹¤.&lt;/p&gt;

&lt;p&gt;no object í´ë˜ìŠ¤ëŠ” background roleì„ í•˜ê¸°ë„ í•œë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ë‹¤ëŠ” ì‚´í´ë³´ì§€ ì•Šê³ , ëª‡ ê°œë§Œ ì‚´í´ë³¼ ê²ƒì„.&lt;/p&gt;

&lt;h2 id=&quot;comparison-with-faster-r-cnn&quot;&gt;Comparison with Faster R-CNN&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99273109-eb58a900-286b-11eb-9c47-c2bbeae303b1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;AP_S : Small scale images&lt;/p&gt;

&lt;p&gt;AP_L : Large scale images&lt;/p&gt;

&lt;p&gt;CNNì€ local íŠ¹ì„±ì„ ì˜ ë°˜ì˜í•˜ë¯€ë¡œ small img.ë“¤ì— DETRë³´ë‹¤ ë” íš¨ê³¼ì ì´ë‹¤.&lt;/p&gt;

&lt;p&gt;í•˜ì§€ë§Œ DETRì€ Attention ê¸°ë°˜ì´ê¸° ë•Œë¬¸ì— ëª¨ë“  positionì„ ë³¼ ìˆ˜ ìˆê³ , ë•Œë¬¸ì— Large img.ì— ë§¤ìš° íš¨ê³¼ì ì´ë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;ê·¸ëŸ¬ë‚˜ ìµœê·¼ ë‚˜ì˜¨ ë…¼ë¬¸ Deformable DETRì—ì„œ AP_Së„ Faster R-CNNë³´ë‹¤ ë†’ì€ ì„±ê³¼ë¥¼ ê±°ë‘ì—ˆë‹¤! (ë‹¤ìŒ ë…¼ë¬¸ì€ ì´ê±¸ ì½ì–´ë³´ë ¤ê³  í•œë‹¤.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Attentionì´ globalí•œ reasonì´ ëœë‹¤ëŠ” ì¥ì ì´ ìˆê¸° ë•Œë¬¸ì— panoptic segmentationì— ì˜ ë§ì§€ ì•Šì„ê¹Œí•˜ëŠ” ìƒê°ì´ ë“ ë‹¤â€¦)&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;number-of-encoder-layers&quot;&gt;Number of encoder layers&lt;/h2&gt;

&lt;p&gt;&lt;img width=&quot;644&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-11-17 á„‹á…©á„Œá…¥á†« 12 33 56&quot; src=&quot;https://user-images.githubusercontent.com/48315997/99273597-95383580-286c-11eb-9501-87a86806a63f.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì¸ì½”ë”ì˜ ë¸”ëŸ­ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ APëŠ” ì˜¬ë¼ê°„ë‹¤.&lt;/p&gt;

&lt;p&gt;ë˜í•œ &lt;strong&gt;EncoderëŠ” disentangling&lt;/strong&gt;(ì´ë¯¸ì§€ ë¶„í•´, í•´ì²´) ì—­í• ì— ì•„ì£¼ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤ê³  í•œë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99273849-e3e5cf80-286c-11eb-8f92-c330bddc9e51.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;image disentanglement ë€ ìœ„ì˜ ì‚¬ì§„ê³¼ ê°™ì´ ê°ì²´ë³„ë¡œ(ì¸ìŠ¤íƒ„ìŠ¤ë³„ë¡œ) ì˜ ë¶„ë¦¬ê°€ ëìŒì„ í‘œí˜„í•˜ëŠ” ë“¯í•˜ë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ë ‡ê²Œ ê°ì²´ë³„ë¡œ attentionì´ ì˜ ë‚˜ëˆ„ì–´ì§„ë‹¤ë©´ ë””ì½”ë”ì—ì„œ ê°ì²´ì˜ bounding box, classë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì‰½ë‹¤ê³  í•œë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;number-of-decoder-layers&quot;&gt;Number of decoder layers&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99274139-4dfe7480-286d-11eb-9d6d-e06ab83853f3.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì´ê²ƒì€ decoder layerì˜ ê°œìˆ˜ê°€ ì–´ëŠì •ë„ ë˜ë©´, NMSë¥¼ ì“¸ ë•Œì™€ ì„±ëŠ¥ìƒ ë³„ ì°¨ì´ê°€ ì—†ë‹¤ëŠ” ê²°ê³¼ì´ë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99274340-9322a680-286d-11eb-811d-ae3aeb96712f.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ê°œì¸ì ìœ¼ë¡œ ì •ë§ ë†€ëë˜ ê²°ê³¼ì´ë‹¤. ê°ì²´ì˜ head, edgeë¥¼ ì •ë§ ì˜ attentioní•˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. 
&lt;strong&gt;ì¸ì½”ë”ëŠ” global attentionì„ í†µí•´ ì¸ìŠ¤í„´ìŠ¤ë“¤ì„ ë¶„ë¦¬í•œë‹¤ë©´, ë””ì½”ë”ëŠ” í´ë˜ìŠ¤ì™€ ê°ì²´ì˜ ë°”ìš´ë”ë¦¬(ê°€ì¥ìë¦¬)ë¥¼ ì¶”ì¶œí•œë‹¤ê³  í•œë‹¤&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;panoptic-segmentation&quot;&gt;Panoptic Segmentation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99275057-60c57900-286e-11eb-87fa-4c29646eb60c.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;DETRì€ object detection ë¬¸ì œë¥¼ direct set predictionìœ¼ë¡œ ë³´ì•˜ê³ , ì´ë¥¼ Transformerì™€ bipartite matchingì„ í†µí•œ end-to-end ë°©ì‹ìœ¼ë¡œ í’€ì–´ëƒˆë‹¤. (ë§ì€ ì—°êµ¬ìë“¤ì´ ëª¨ë“  ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ìµœì¢… ì§€í–¥ì ì€ end-to-endë¼ê³  í•œë‹¤.)&lt;/p&gt;

&lt;p&gt;transformer íŠ¹ì„±ìƒ, DETR ë˜í•œ architectureë¥¼ flexible í•˜ê²Œ í™•ì¥ì‹œí‚¬ ìˆ˜ ìˆë‹¤.(Panoptic segmentationì²˜ëŸ¼)&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;code&quot;&gt;Code&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99277842-ab94c000-2871-11eb-803f-d34e1bc3406b.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;additional-studies&quot;&gt;Additional studies&lt;/h1&gt;
&lt;p&gt;(If you have some parts that cannot understand, you have to do additional studies for them. Itâ€™s optional.)&lt;/p&gt;

&lt;p&gt;Deformable DETR&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;(References for your additional studies)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://keyog.tistory.com/32&quot;&gt;ì¸ê°„ì§€ëŠ¥ì´ ì¸ê³µì§€ëŠ¥ì„ ê³µë¶€í•˜ëŠ” ì¥ì†Œ&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kp1994.tistory.com/15&quot;&gt;KPâ€™s blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=lXpBcW_I54U&amp;amp;feature=youtu.be&amp;amp;fbclid=IwAR25dYNnKxCsN5apneKrmPus-umovk7fziMedDQMqBfhg28eaBj6u-tRxzI&amp;amp;ab_channel=JinWonLee&quot;&gt;TF ë…¼ë¬¸ ì½ê¸° ëª¨ì„ PR-284&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Computer Vision" /><category term="Object Detection" /><category term="Encoder-Decoder" /><category term="Transformer" /><category term="Panoptic Segmentation" /><category term="DETR" /><category term="ViT" /><summary type="html">DETR : End-to-End Object Detection with Transformers ë¥¼ ì½ê³  ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.</summary></entry><entry><title type="html">MMDetection ì‚¬ìš©í•˜ê¸°</title><link href="http://localhost:4000/computer%20vision/2020/11/03/mmdetection-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/" rel="alternate" type="text/html" title="MMDetection ì‚¬ìš©í•˜ê¸°" /><published>2020-11-03T00:00:00+09:00</published><updated>2020-11-03T00:00:00+09:00</updated><id>http://localhost:4000/computer%20vision/2020/11/03/mmdetection%20%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0</id><content type="html" xml:base="http://localhost:4000/computer%20vision/2020/11/03/mmdetection-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/">&lt;p&gt;Daconì˜ K-Fashion AI ê²½ì§„ëŒ€íšŒì˜ baseline ì„¤ëª…ì— ë”°ë¼ &lt;strong&gt;MMdetection&lt;/strong&gt; toolkitì„ ì„¤ì¹˜í•´ë³´ê³  í•™ìŠµê¹Œì§€ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;ì €ëŠ” ì˜¤ëŠ˜ë¶€í„° ì‹œì‘ëœ &lt;a href=&quot;https://dacon.io/competitions/official/235672/overview/&quot;&gt;K-Fashion AI ê²½ì§„ëŒ€íšŒ&lt;/a&gt;ì— ì°¸ê°€í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ ëŒ€íšŒëŠ” ì œê°€ ê´€ì‹¬ìˆëŠ” ë¶„ì•¼ì¸ &lt;strong&gt;&lt;em&gt;ì»´í“¨í„° ë¹„ì „&lt;/em&gt;&lt;/strong&gt; ëŒ€íšŒë¼ ê´€ì‹¬ì„ ê°€ì§€ê²Œ ë˜ì—ˆê³ , ì‹¤ì œë¡œ CV ê´€ë ¨ ëŒ€íšŒëŠ” ì²˜ìŒì´ë‹ˆ ë°°ìš´ë‹¤ëŠ” ë§ˆìŒê°€ì§ìœ¼ë¡œ ì—´ì‹¬íˆ ê³µë¶€í•´ë³´ë ¤ í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;h1 id=&quot;mmdectection&quot;&gt;MMdectection?&lt;/h1&gt;

&lt;p&gt;ì´ ëŒ€íšŒì˜ ë² ì´ìŠ¤ë¼ì¸ì„ ì°¸ê³ í•˜ë©´ì„œ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mmdetection&lt;/code&gt; ì´ë¼ëŠ” ê²ƒì„ ì²˜ìŒ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. 
ë…¼ë¬¸ë„ ìˆìœ¼ë‹ˆ ì‹œê°„ ë‚¨ì„ ë•Œ ë´ì•¼ê² ë„¤ìš”&lt;/p&gt;

&lt;p&gt;ê¹ƒí—ˆë¸ŒëŠ” &lt;a href=&quot;https://github.com/open-mmlab/mmdetection&quot;&gt;ì—¬ê¸°&lt;/a&gt;ì— ë“¤ì–´ê°€ë©´ ë˜ê³ , ë¦¬ë“œë¯¸ íŒŒì¼ì— mmdetectionì´ ì–´ë–¤ ê±´ì§€ ë‚˜ì™€ìˆìŠµë‹ˆë‹¤.
ì§§ê²Œ ìš”ì•½í•˜ìë©´,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;MMDetection&lt;/strong&gt; is an open source object detection toolbox based on PyTorch.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ì´ë¼ í•˜ë„¤ìš”.&lt;/p&gt;

&lt;p&gt;object detectionì—ì„œ ë‹¤ë£¨ëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ë“¤ì„ í•œ ê³³ì— ëª¨ì•„ë’€ë‹¤ê³  í•˜ë‹ˆ ì •ë§ ê°„í¸í•´ë³´ì…ë‹ˆë‹¤. (ì´ëŸ° ê±¸ ì´ì œ ì•Œê²Œëœ ê²Œ ì•„ì‰¬ìš¸ ì •ë„ë¡œâ€¦)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmdetection#benchmark-and-model-zoo&quot;&gt;ì—¬ê¸°&lt;/a&gt; ì— support í•˜ëŠ” ëª¨ë¸ë“¤ì´ ë‚˜ì—´ë˜ì–´ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë˜í•œ ë²„ì „ ì—…ë°ì´íŠ¸ ë° ìœ ì§€ë³´ìˆ˜ë„ êµ‰ì¥íˆ ì˜ ë˜ê³  ìˆë‹¤ê³  í•©ë‹ˆë‹¤!&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;installation&quot;&gt;Installation&lt;/h1&gt;

&lt;p&gt;ë°ì´ì½˜ ë² ì´ìŠ¤ ë¼ì¸ì„ ë³´ê³  ë”°ë¼í•˜ëŠ” ê²ƒì´ë¯€ë¡œ ë²„ì „ë„ ë§ì¶°ì„œ í•´ë´…ë‹ˆë‹¤.
ìš°ì„  mmdetection ê¹ƒí—ˆë¸Œ ë§í¬ì— ê°€ì„œ ë¸Œëœì¹˜ë¥¼ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v2.3.0&lt;/code&gt; ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmdetection/tree/v2.3.0&quot;&gt;ë°”ë¡œ ê°€ê¸°&lt;/a&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/97955665-0f3ada00-1dea-11eb-93d6-a233996a32f8.png&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-11-03 á„‹á…©á„’á…® 3 34 25&quot; /&gt;&lt;/p&gt;

&lt;p&gt;README.md ë¥¼ ì¡°ê¸ˆ ë‚´ë¦¬ë‹¤ ë³´ë©´ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;install.md&lt;/code&gt; ë¡œ ë„˜ì–´ê°ˆ ìˆ˜ ìˆëŠ” í•˜ì´í¼ë§í¬ê°€ ìˆìŠµë‹ˆë‹¤.
ê·¸ê±¸ ëˆŒëŸ¬ install.mdë¡œ ì´ë™í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/97955738-3db8b500-1dea-11eb-9988-2a6afd19ec79.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/97955982-dbac7f80-1dea-11eb-9aa4-e7ff1a77c779.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt; ì˜ í™•ì¸í•˜ì‹œê³ , install mmdetection ìˆœì„œëŒ€ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;1-ê°€ìƒí™˜ê²½-ìƒì„±&quot;&gt;1. ê°€ìƒí™˜ê²½ ìƒì„±&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda create -n ê°€ìƒí™˜ê²½ì´ë¦„ python=3.7 -y
conda activate ê°€ìƒí™˜ê²½ì´ë¦„ 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ì´ë ‡ê²Œ ê°€ìƒ í™˜ê²½ ìƒì„±í•˜ê³  activate ì‹œí‚µë‹ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;2-pytorch-torchvision-ì„¤ì¹˜-ë²„ì „-ì£¼ì˜&quot;&gt;2. pytorch, torchvision ì„¤ì¹˜ (ë²„ì „ ì£¼ì˜)&lt;/h3&gt;

&lt;p&gt;ë² ì´ìŠ¤ë¼ì¸ ë”°ë¼ê°€ë ¤ë©´ (ì¦‰, mmdetection version==2.3.0) torch version == &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1.5.0&lt;/code&gt; ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
pytorchë¥¼ ì›í•˜ëŠ” ì´ì „ ë²„ì „ìœ¼ë¡œ ì„¤ì¹˜í•´ì•¼ í•  ë•Œ ê³µì‹í™ˆí˜ì´ì§€ì— ì»¤ë§¨ë“œê°€ ë‹¤ ë‚˜ì™€ìˆìŠµë‹ˆë‹¤. &lt;a href=&quot;https://pytorch.org/get-started/previous-versions/&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda install pytorch==1.5.0 torchvision==0.6.0 -c pytorch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3-mmdetection-repo-clone&quot;&gt;3. MMdetection repo. clone&lt;/h3&gt;

&lt;p&gt;mmdetection ë ˆí¬ì§€í„°ë¦¬ë¥¼ clone í•´ì•¼ í•©ë‹ˆë‹¤.
ì´ ë•Œë„ ì´ ë„íë¨¼íŠ¸ì™€ ë‹¤ë¥´ê²Œ ì£¼ì˜í•´ì•¼ í•  ì ì´ ìˆìŠµë‹ˆë‹¤. ì•ì„œ ë§í–ˆë“¯ì´ ì €ëŠ” ë² ì´ìŠ¤ë¼ì¸ ë”°ë¼ branch == v2.3.0ìœ¼ë¡œ ì´ë™í–ˆìŠµë‹ˆë‹¤. ì´ ë¸Œëœì¹˜ì— í•´ë‹¹ë˜ëŠ” ë²„ì „ìœ¼ë¡œ git clone í•´ì•¼ í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone --branch v2.3.0 https://github.com/open-mmlab/mmdetection.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;4-requirements-ì„¤ì¹˜&quot;&gt;4. Requirements ì„¤ì¹˜&lt;/h3&gt;

&lt;p&gt;ìš°ì„  í´ë¡ í•œ ë””ë ‰í† ë¦¬ë¡œ ê°‘ë‹ˆë‹¤.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd ./mmdetection
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ì €ì™€ ê°™ì´ ë‹¤ë¥¸ ì‚¬ëŒë“¤ê³¼ ê°™ì´ ì“°ëŠ” ì„œë²„ë¥¼ ì“°ì‹œëŠ” í™˜ê²½ì´ë¼ë©´ ë§ˆìŒëŒ€ë¡œ íŒ¨í‚¤ì§€/ë¼ì´ë²„ë¦¬ë¥¼ ë‹¤ìš´ ë°›ì•˜ì„ ë•Œ ì¶©ëŒë  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ê·¸ë˜ì„œ ì´ requirements ë“¤ì„ ê°€ìƒ í™˜ê²½ ë‚´ì— ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.&lt;/strong&gt;
í˜„ì¬ ê°€ìƒí™˜ê²½ì˜ ì ˆëŒ€ ì£¼ì†Œë¥¼ ëª¨ë¥´ì‹œë©´, í„°ë¯¸ë„ì—&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda env list
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ë¥¼ ì¹˜ì‹œë©´ ê°€ìƒí™˜ê²½ ì´ë¦„ ë’¤ì— ê²½ë¡œê°€ ë‚˜ì˜µë‹ˆë‹¤.
ê·¸ pathë¥¼ copy í•´ë‘ì„¸ìš”.
ê°€ìƒí™˜ê²½ì˜ ê²½ë¡œë¥¼ {PATH}ë¼ê³  ê°€ì •í•˜ê³  ì ì–´ë³´ê² ìŠµë‹ˆë‹¤.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{PATH}/bin/pip install -r requirements/build.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ì´ë ‡ê²Œ í•˜ë©´ requirementsê°€ ì˜ ì„¤ì¹˜ë  ê²ƒì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;5-set-up&quot;&gt;5. Set up&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install -v -e .  # or &quot;python setup.py develop&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ì•ì— ëª…ë ¹ì–´ í•´ë³´ê³  ì•ˆë˜ë©´ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python setup.py develop&lt;/code&gt; í•˜ì‹œë©´ ë©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;(ì € ê°™ì€ ê²½ìš°ëŠ” &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install -v -e .&lt;/code&gt; í–ˆì„ ë•Œ ì•ˆëì–´ì„œ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python setup.py develop&lt;/code&gt; ëª…ë ¹ì–´ í•´ì„œ ì •ìƒ ì„¤ì¹˜ ëìŠµë‹ˆë‹¤.)&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;training-í•˜ê¸°-ì „-ì¤€ë¹„-í•´ì•¼-í• -ê²ƒë“¤&quot;&gt;Training í•˜ê¸° ì „ ì¤€ë¹„ í•´ì•¼ í•  ê²ƒë“¤&lt;/h1&gt;

&lt;h3 id=&quot;1-dataset-ê´€ë ¨&quot;&gt;1. Dataset ê´€ë ¨&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mmdetection/mmdet/datasets&lt;/code&gt; ì—ì„œ ë°ì´í„°ì…‹ì— ëŒ€í•œ ê¸°ë³¸ ì„¤ì •ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;custom.py&lt;/code&gt; ë¡œ ì§ì ‘ personal í•œ ì„¤ì •ì„ í•  ìˆ˜ ìˆê² ì§€ë§Œ ì´ ëŒ€íšŒëŠ” &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;coco.py&lt;/code&gt; ì—ì„œ ì„¤ì •ëœ ê²ƒì—ì„œ í¬ê²Œ ë²—ì–´ë‚˜ì§€ ì•Šì•„ ì´ëŒ€ë¡œ ì‚¬ìš©í•´ë„ ëœë‹¤ê³  í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ê·¸ë˜ì„œ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;coco.py&lt;/code&gt; ì—ì„œ ë¯¸ë¦¬ ì„¤ì •ëœ CLASSES ë“¤ì„ ì´ ëŒ€íšŒì—ì„œ ì“°ì¼ í´ë˜ìŠ¤ë¡œ ë°”ê¾¸ê² ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/97957150-c71db680-1ded-11eb-88e2-3b503374e352.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ë˜í•œ, mmdetectionì˜ ê²½ìš° train/test í´ë”ê°€ í•œ ê³³ì— flatten í•˜ê²Œ ì¡´ì¬í•´ì•¼ í•œë‹¤ê³  í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/train/a/*.jpg&lt;/code&gt; ì´ëŸ° í˜•íƒœê°€ ì•„ë‹Œ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/train/a_*.jpg&lt;/code&gt; ì´ëŸ° ì‹ìœ¼ë¡œìš”.&lt;/p&gt;

&lt;p&gt;ì´ê±´ íŒŒì´ì¬ ì½”ë“œë¡œ ì‰½ê²Œ ìˆ˜ì • ê°€ëŠ¥í•˜ë‹ˆ ìŠì§€ ì•Šê³  í•˜ì‹œê¸¸ ë°”ë¦½ë‹ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;2-training-options&quot;&gt;2. training options&lt;/h3&gt;

&lt;p&gt;íŠ¸ë ˆì´ë‹í•  ë•Œ í•„ìš”í•œ ê¸°ë³¸ ì˜µì…˜ë“¤ì„ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mmdetection/config/_base_/default_runtime.py&lt;/code&gt; ì—ì„œ ê°€ëŠ¥í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ configëŠ” ìš°ì„  ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œì—ì„œ ì œê³µí•œ ê²ƒì„ ë¶™ì—¬ ì¨ì„œ ì§€ê¸ˆ ì œ í™˜ê²½ì— ë§ê²Œ ì¡°ê¸ˆì”© ê³ ì³¤ìŠµë‹ˆë‹¤. (data root, gpu ê°œìˆ˜, epoch ë“±)&lt;/p&gt;

&lt;p&gt;ìì„¸í•œ ê±´ &lt;a href=&quot;https://github.com/dacon-ai/K-fashion-baseline/blob/master/configs/_base_/default_runtime.py&quot;&gt;ì—¬ê¸°&lt;/a&gt;ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.&lt;/p&gt;

&lt;p&gt;train/test set ê²½ë¡œ ë“±ë“± ì˜ ì„¤ì •í•˜ê³  ë‚˜ë©´ ë°”ë¡œ í•™ìŠµ ì‹œì‘ ê°€ëŠ¥í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;training-&quot;&gt;Training !&lt;/h1&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python tools/train.py configs/_base_/default_runtime.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ì´ê±¸ ì¹˜ì‹œë©´ configë¡œ í•´ë†¨ë˜ ì„¤ì •ë“¤ì´ ì£¼ë£¨ë£© ëœ¨ë©´ì„œ í•™ìŠµì´ ì‹œì‘ë©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/97957533-bf124680-1dee-11eb-93b9-a3b3921adbe6.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;troubleshooting&quot;&gt;Troubleshooting&lt;/h1&gt;

&lt;h3 id=&quot;attributeerror-coco-object-has-no-attribute-get_cat_ids&quot;&gt;AttributeError: â€˜COCOâ€™ object has no attribute â€˜get_cat_idsâ€™&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pycocotools&lt;/code&gt; ê´€ë ¨ ì˜¤ë¥˜ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì €ê°™ì€ ê²½ìš°ì—ëŠ”&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{PATH}/bin/pip install &quot;git+https://github.com/open-mmlab/

cocoapi.git#subdirectory=pycocotools&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ë¡œ í•´ê²°í–ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;modulenotfounderror-no-module-named-mmcv_ext&quot;&gt;ModuleNotFoundError: No module named â€˜mmcv._extâ€™&lt;/h3&gt;

&lt;p&gt;mmcvê°€ ì œëŒ€ë¡œ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{PATH}/bin pip uninstall mmcv
{PATH}/bin pip install mmcv-full==1.0.5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ì €ëŠ” ì´ë ‡ê²Œ í•´ê²°í–ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=UEu73ew7mSY&amp;amp;feature=youtu.be&amp;amp;ab_channel=%EB%8D%B0%EC%9D%B4%EC%BD%98&quot;&gt;ë°ì´ì½˜ ë² ì´ìŠ¤ë¼ì¸&lt;/a&gt;&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Computer Vision" /><category term="MMdetection" /><category term="Mask R-CNN" /><summary type="html">Daconì˜ K-Fashion AI ê²½ì§„ëŒ€íšŒì˜ baseline ì„¤ëª…ì— ë”°ë¼ MMdetection toolkitì„ ì„¤ì¹˜í•´ë³´ê³  í•™ìŠµê¹Œì§€ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.</summary></entry><entry><title type="html">An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale(ViT)</title><link href="http://localhost:4000/paper%20review/2020/10/31/ViT/" rel="alternate" type="text/html" title="An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale(ViT)" /><published>2020-10-31T00:00:00+09:00</published><updated>2020-10-31T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/10/31/ViT</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/10/31/ViT/">&lt;p&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT) ë¥¼ ì½ê³  ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale&quot;&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/h1&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;image classification taskì— ê¸°ì¡´ì˜ transformer ëª¨ë¸ì„ ì´ìš©í•œë‹¤.
    &lt;ul&gt;
      &lt;li&gt;transformerì˜ ì¥ì ë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ.&lt;/li&gt;
      &lt;li&gt;simple, computational efficiency, scalability&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Vision Transformer(ViT)
    &lt;ul&gt;
      &lt;li&gt;ì›ë³¸ ì´ë¯¸ì§€ë¥¼ &lt;strong&gt;patches&lt;/strong&gt;ë“¤ë¡œ splití•œë‹¤.
        &lt;ul&gt;
          &lt;li&gt;ì´ ë•Œ ì´ë¯¸ì§€ íŒ¨ì¹˜ë“¤ì„ NLPì—ì„œì˜ &lt;strong&gt;token(word)&lt;/strong&gt;ì™€ ê°™ì€ ì—­í• ì„.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;ì´ íŒ¨ì¹˜ë“¤ì˜ sequence of linear embeddingì„ Transformerì˜ inputìœ¼ë¡œ feedí•œë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;í¬ì§€ì•Šì€ ë°ì´í„° ì…‹ì—ì„œëŠ” ResNetë³´ë‹¤ ì•½ê°„ ë‚®ì€ ì •í™•ë„
    &lt;ul&gt;
      &lt;li&gt;TransformerëŠ” CNNê³¼ ë‹¤ë¥´ê²Œ &lt;em&gt;translation equivariance, locality&lt;/em&gt; ê°™ì€ &lt;strong&gt;inductive biases&lt;/strong&gt;(=weight sharing)ì´ ì—†ê¸° ë•Œë¬¸ì— ë°ì´í„°ì…‹ì´ ì ìœ¼ë©´ generalizeë˜ê¸° ì–´ë ¤ì›€.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;í•˜ì§€ë§Œ large scale ë°ì´í„°ì—ì„œëŠ” CNNì˜ inductive biasë¥¼ ëŠ¥ê°€í•¨.
    &lt;ul&gt;
      &lt;li&gt;image recognition benchmarksì—ì„œ ì—¬ëŸ¬ SOTA ë‹¬ì„±í•¨.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-work-basic-concepts&quot;&gt;Related work (Basic concepts)&lt;/h2&gt;

&lt;p&gt;ì œì•ˆí•œ ëª¨ë¸ì´ ìµœê·¼ ì—°êµ¬ ì¤‘ì—ì„œëŠ” iGPT ë…¼ë¬¸ê³¼ ìœ ì‚¬í•˜ë‹¤ê³  í•¨.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;reducing image resolution and color spaceí•œ í›„ transformerë¥¼ ì ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;
&lt;p&gt;(Explain one of the methods that the thesis used.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Model overview
  &lt;img width=&quot;777&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-11-01 á„‹á…©á„Œá…¥á†« 11 51 34&quot; src=&quot;https://user-images.githubusercontent.com/48315997/97794098-97797d80-1c38-11eb-8221-30ea0e5d6ea7.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Vision Transformer(ViT)&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;imageë¥¼ patch ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ flattenì‹œí‚¨ í›„, ê·¸ê²ƒì„ linear projectioní•˜ì—¬ encoderì— feedí•œë‹¤.
  &lt;img src=&quot;https://user-images.githubusercontent.com/48315997/97794316-8da54980-1c3b-11eb-963b-2c07da280861.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Equation
  &lt;img width=&quot;737&quot; alt=&quot;á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2020-11-01 á„‹á…©á„’á…® 12 32 37&quot; src=&quot;https://user-images.githubusercontent.com/48315997/97794501-55ebd100-1c3e-11eb-97cd-cc00bde85b4d.png&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;(Eq.1) trainable linear projectionì€ flattenëœ patchë“¤ì„ &lt;em&gt;D&lt;/em&gt; dimensionì— &lt;strong&gt;mappingì‹œí‚¨ë‹¤.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;(Eq.4) BERTì˜ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[class]&lt;/code&gt; í† í°ì²˜ëŸ¼, embedded patchì˜ sequence(image representation &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y&lt;/code&gt;) ì „ì— &lt;strong&gt;learnable embedding&lt;/strong&gt;ì„ ì¶”ê°€í•œë‹¤. (prepend)
        &lt;ul&gt;
          &lt;li&gt;both during pre-training and fine-tuning, a classification head is attached to $\left({z}_{L}^{0} \right)$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Position embeddings
        &lt;ul&gt;
          &lt;li&gt;positional informationì„ ìœ ì§€í•˜ê¸° ìœ„í•´ patch embeddingì— ë¶™ì—¬ì§. (ìì„¸í•œê±´ Appendix.D.3)&lt;/li&gt;
          &lt;li&gt;standard learnable 1D position embedding ì‚¬ìš©&lt;/li&gt;
          &lt;li&gt;ì´ë ‡ê²Œ position embeddingê¹Œì§€ ë”í•´ì§„ sequence of embedding vectorë“¤ì€ encoderì˜ inputì´ ë¨.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Encoder
        &lt;ul&gt;
          &lt;li&gt;alternating layers of multiheaded self-attention(MSA)&lt;/li&gt;
          &lt;li&gt;MLP blocks
            &lt;ul&gt;
              &lt;li&gt;contains 2 layer with a GELU non-linearity&lt;/li&gt;
              &lt;li&gt;MLP ë¶€ë¶„ code&lt;/li&gt;
            &lt;/ul&gt;

            &lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MlpBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Transformer MLP / feed-forward block.&quot;&quot;&quot;&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;mlp_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jnp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;out_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;deterministic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;kernel_init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initializers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xavier_uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;bias_init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initializers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stddev&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Applies Transformer MlpBlock module.&quot;&quot;&quot;&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;actual_out_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_dim&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_dim&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;mlp_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;kernel_init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;bias_init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gelu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deterministic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deterministic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;actual_out_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;kernel_init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;bias_init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deterministic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deterministic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;            &lt;/div&gt;
          &lt;/li&gt;
          &lt;li&gt;Layernorm(LN) is applied before every block&lt;/li&gt;
          &lt;li&gt;residual connections after every block&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Hybrid Architecture
        &lt;ul&gt;
          &lt;li&gt;patch embedding projection E (Eq.1)ì´ CNN feature mapìœ¼ë¡œ ëŒ€ì²´ë  ìˆ˜ ìˆë‹¤.
            &lt;ul&gt;
              &lt;li&gt;ì¦‰, ResNetê³¼ ê°™ì€ CNNêµ¬ì¡°ì˜ ëª¨ë¸ì„ ê°€ì§€ê³ , 2D feature map ì¤‘ í•˜ë‚˜ë¥¼ 1Dë¡œ flattenì‹œí‚¨ í›„ transformer dimensionì— projecting ì‹œí‚´.&lt;/li&gt;
              &lt;li&gt;ìœ„ì—ì„œ ë§Œë“¤ì–´ì§„ sequenceì— classification input embedding, position embeddingë¥¼ ì¶”ê°€ì‹œì¼œ encoderì— inputìœ¼ë¡œì¨ feed ì‹œí‚¬ ìˆ˜ ìˆìŒ.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fine-tuning-and-higher-resolution&quot;&gt;Fine-Tuning and Higher Resolution&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;large datasetìœ¼ë¡œ pre-trainingí•˜ê³ , smaller downstream taskì— ëŒ€í•´ fine-tune í•˜ë ¤ê³  í•¨.&lt;/li&gt;
  &lt;li&gt;ì´ë¥¼ ìœ„í•´ì„œ pre-trained prediction headë¥¼ ì§€ìš°ê³ , 0ìœ¼ë¡œ initializedgks D x K feedforward layerë¥¼ ì¶”ê°€í•¨.
    &lt;ul&gt;
      &lt;li&gt;K : # of downstream classes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;pre-training ë³´ë‹¤ ë†’ì€ resolutionìœ¼ë¡œ fine-tuningí•˜ëŠ” ê²ƒì€ beneficialí•  ë•Œë„ ìˆìŒ.&lt;/li&gt;
  &lt;li&gt;higher resolutionì„ feedí•˜ê²Œ ë˜ë©´, patch sizeëŠ” ë™ì¼í•˜ë¯€ë¡œ sequence lengthê°€ ê¸¸ì–´ì§&lt;/li&gt;
  &lt;li&gt;ViTëŠ” ì„ì˜ì ì¸ sequence lengthë¥¼ ë‹¤ë£° ìˆ˜ ìˆìŒ(ë©”ëª¨ë¦¬ ì œì•½ì— ë”°ë¼ì„œ)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;í•˜ì§€ë§Œ pre-trained position embeddingì´ ì˜ë¯¸ ì—†ì–´ì§ˆ ìˆ˜ ìˆìŒ&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;ì›ë³¸ ì´ë¯¸ì§€ì˜ locationì— ë”°ë¼ pre-trained position embeddingì˜ 2D interpolationì„ ìˆ˜í–‰í•¨.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ìœ„ì™€ ê°™ì€ resolution adjustmentì™€ patch extractionì€ ì´ë¯¸ì§€ì˜ 2D êµ¬ì¡°ì— ëŒ€í•´ inductive biasë¥¼ &lt;strong&gt;manually&lt;/strong&gt; ViTì— ì£¼ì…ì‹œí‚¤ëŠ” ìœ ì¼í•œ í¬ì¸íŠ¸ì„.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model.py&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/google-research/vision_transformer/blob/master/vit_jax/models.py&quot;&gt;Github&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;(References for your additional studies)&lt;/p&gt;

&lt;p&gt;https://jeonsworld.github.io/vision/vit/&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Computer Vision" /><category term="Transformer" /><summary type="html">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT) ë¥¼ ì½ê³  ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.</summary></entry></feed>