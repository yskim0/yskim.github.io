<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-05-12T00:01:06+09:00</updated><id>http://localhost:4000/atom.xml</id><title type="html">Yeonsoo Kim’s blog</title><author><name>Yeonsoo Kim</name></author><entry><title type="html">VizML - A Machine Learning Approach to Visualization Recommendation</title><link href="http://localhost:4000/paper%20review/2021/05/06/VizML/" rel="alternate" type="text/html" title="VizML - A Machine Learning Approach to Visualization Recommendation" /><published>2021-05-06T00:00:00+09:00</published><updated>2021-05-06T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2021/05/06/VizML</id><content type="html" xml:base="http://localhost:4000/paper%20review/2021/05/06/VizML/">&lt;p&gt;VizML : A Machine Learning Approach to Visualization Recommendation 을 읽고 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;vizml--a-machine-learning-approach-to-visualization-recommendation&quot;&gt;VizML : A Machine Learning Approach to Visualization Recommendation&lt;/h1&gt;

&lt;p&gt;&lt;img width=&quot;600&quot; alt=&quot;스크린샷 2021-05-06 오후 11 37 45&quot; src=&quot;https://user-images.githubusercontent.com/48315997/117316985-1095ae00-aec4-11eb-9934-cc9110acc841.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;ML approach to visualization recommendation&lt;/li&gt;
  &lt;li&gt;learns visualization design choices from a large corpus of datasets and associated visualization
    &lt;ul&gt;
      &lt;li&gt;identify five key design choices (viz. type, encoding type, …)&lt;/li&gt;
      &lt;li&gt;train models to predict these design choices using 1M dataset-viz. pairs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;NN predicts well&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problem-formulation&quot;&gt;Problem Formulation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;representation&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;are specified using &lt;strong&gt;encodings&lt;/strong&gt; that map from data to the retinal properties(position, length, color) of graphical marks(points, lines, rectangles)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;433&quot; alt=&quot;스크린샷 2021-05-06 오후 11 41 19&quot; src=&quot;https://user-images.githubusercontent.com/48315997/117317548-90237d00-aec4-11eb-8a08-fac0d1997ced.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;That is, to create basic visualizations in many grammars or tools, &lt;strong&gt;an analyst specifes higher-level design choices&lt;/strong&gt;, which we defne as statements that compactly and uniquely specify a bundle of lower-level encodings. Equivalently, each gram- mar or tool affords a design space of visualizations, which a user constrains by making choices.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Trained with a corpus of datasets ${d}$ and corresponding design choices ${C}$, ML-based recommender systems treat recommendation as an optimization problem,such that predicted $ C_{rec} ∼ C_{max}. $&lt;/p&gt;

&lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&quot;rule-based&quot;&gt;Rule-based&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;encode visualization guidelines as collection of “if-then” statements or &lt;strong&gt;rules.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;sometimes effective, but high cost&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ml-based&quot;&gt;ML-based&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;learn the relationship between data and visualizations by training models on analyst interaction&lt;/li&gt;
  &lt;li&gt;DeepEye, Data2Vis, Draco-Learn
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;do not learn to make visualization design choices&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;trained with annotations on rule-generated visualizations in controlled settings -&amp;gt; limit&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DeepEye
    &lt;ul&gt;
      &lt;li&gt;combines rule-based visualization generation with models trained to 1) classify Good/Bad 2) rank lists of viz.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;learning to rank&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Data2Vis
    &lt;ul&gt;
      &lt;li&gt;Seq2Seq Model that maps JSON-encoded datasets to Vega-lite visualization specifications
        &lt;blockquote&gt;
          &lt;p&gt;Vega and Vega-Lite are visualization tools implementing a grammar of graphics, similar to ggplot2.&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
      &lt;li&gt;4300 automatically generated Vega-Lite ex.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Draco-Learn
    &lt;ul&gt;
      &lt;li&gt;represents 1) visualizations as logical facts 2)design guidelines as hard and soft constraints, SVM&lt;/li&gt;
      &lt;li&gt;recommends visualizations that satisfy these constraints&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VizML&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;In terms of &lt;strong&gt;LEARNING TASK&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;DeepEye learns to classify and rank visualizations&lt;/li&gt;
          &lt;li&gt;Data2Vis learns an &lt;strong&gt;end-to-end&lt;/strong&gt; generation model&lt;/li&gt;
          &lt;li&gt;Draco-Learn learns soft constraints weights&lt;/li&gt;
          &lt;li&gt;By learning to &lt;strong&gt;predict design choices&lt;/strong&gt;, &lt;strong&gt;VizML models are easier to quantitatively validate&lt;/strong&gt;, provide interpretable measures of feature importance, and can be more easily integrated into visualization systems.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;In terms of &lt;strong&gt;DATA QUANTITY&lt;/strong&gt; …&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;BUT 3 ML-Based systems recommend &lt;strong&gt;both data queries and visual encodings&lt;/strong&gt;, while VizML only recommends &lt;strong&gt;the latter.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt;
&lt;h3 id=&quot;feature-extracting&quot;&gt;Feature Extracting&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/mitmedialab/vizml/blob/b36310106791927eaef3831a0cda7abcec598999/feature_extraction/extract.py&quot;&gt;code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;453&quot; alt=&quot;스크린샷 2021-05-06 오후 11 57 01&quot; src=&quot;https://user-images.githubusercontent.com/48315997/117319963-c235de80-aec6-11eb-9a3f-f3e8bfff428f.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We map each dataset to 841 features, mapped from 81 single- column features and 30 pairwise-column features using 16 aggregation functions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Each Col. -&amp;gt; 81 single-column features across four categories&lt;/li&gt;
  &lt;li&gt;Dimension(D) feature = # of rows in col.&lt;/li&gt;
  &lt;li&gt;Types(T) feature = categorical/temporal/quantitative&lt;/li&gt;
  &lt;li&gt;Values(V) feature = the statistical &amp;amp; structural properties of the values within a col.&lt;/li&gt;
  &lt;li&gt;Names(N) feature = column name&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;We distinguish between these feature categories for three reasons.&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;First, these categories let us &lt;strong&gt;organize how we create and interpret features.&lt;/strong&gt;&lt;/li&gt;
    &lt;li&gt;Second, we can observe the contribution of diferent types of features.&lt;/li&gt;
    &lt;li&gt;Third, some categories of features may be less generalizable than others.&lt;/li&gt;
    &lt;li&gt;We order these categories &lt;strong&gt;(D → T → V → N)&lt;/strong&gt; by how biased we expect those features to be towards the Plotly corpus.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;We create 841 dataset-level features by aggregating these single- and pairwise-column features using the 16 ag- gregation functions&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;design-choice-extraction&quot;&gt;Design Choice Extraction&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;Examples of encoding-level design choices include mark type, such as scatter, line, bar; and X or Y column encoding, which specifes which column is represented on which axis; and whether or not an X or Y column is the single column represented along that axis.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;By aggregating these &lt;strong&gt;encoding-level design choices&lt;/strong&gt;, we can characterize &lt;strong&gt;visualization-level design choices&lt;/strong&gt; of a chart&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;
&lt;h3 id=&quot;feature-preprocessing&quot;&gt;Feature Preprocessing&lt;/h3&gt;
&lt;h3 id=&quot;prediction-tasks&quot;&gt;Prediction Tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Two visualization-level prediction tasks&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Dataset-level features to predict visualization-level design&lt;/li&gt;
      &lt;li&gt;1) Visualization Type[VT]&lt;/li&gt;
      &lt;li&gt;2) Has Shared Axis [HSA]&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Three encoding-level prediction tasks&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;use features about individual columns to predict how theay are visually encoded&lt;/li&gt;
      &lt;li&gt;consider col. indep.&lt;/li&gt;
      &lt;li&gt;1) Mark Type[MT]&lt;/li&gt;
      &lt;li&gt;2) Is Shared X-axis or Y-axis [ISA]&lt;/li&gt;
      &lt;li&gt;3) Is on X-axis or Y-axis [XY]&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;For the &lt;strong&gt;VT, MT&lt;/strong&gt; tasks, the 2- class task predicts line vs. bar, and the 3-class predicts scatter vs. line vs. bar.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;neural-network-and-baseline-models&quot;&gt;Neural Network and Baseline Models&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;In terms of features, we constructed four diferent feature sets by incrementally adding the Dimensions (D), Types (T), Values (V), and Names (N) categories of features, in that order. We refer to these feature sets as &lt;strong&gt;D, D+T, D+T+V, and D+T+V+N=All&lt;/strong&gt;. The neural network was trained and tested using all four feature sets independently. The four base- line models only used the full feature set (D+T+V+N=All).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;additional-studies&quot;&gt;Additional Studies&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;DeepEye: Towards Automatic Data Visualization.&lt;/li&gt;
  &lt;li&gt;Data2Vis: Automatic Generation of Data Vi-
sualizations Using Sequence to Sequence Recurrent Neural Networks.&lt;/li&gt;
  &lt;li&gt;Draco-Learn : Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco.&lt;/li&gt;
  &lt;li&gt;Vega- Lite: A Grammar of Interactive Graphics.&lt;/li&gt;
  &lt;li&gt;Polaris: a system for query, analysis, and visualization of multidimensional databases&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Machine Learning" /><category term="Visualization" /><summary type="html">VizML : A Machine Learning Approach to Visualization Recommendation 을 읽고 정리한 글입니다.</summary></entry><entry><title type="html">Data2Vis - Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural Networks</title><link href="http://localhost:4000/paper%20review/2021/05/06/Data2Vis/" rel="alternate" type="text/html" title="Data2Vis - Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural Networks" /><published>2021-05-06T00:00:00+09:00</published><updated>2021-05-06T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2021/05/06/Data2Vis</id><content type="html" xml:base="http://localhost:4000/paper%20review/2021/05/06/Data2Vis/">&lt;p&gt;Data2Vis: Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural Networks 을 읽고 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;data2vis-automatic-generation-of-data-visualizations-using-sequence-to-sequence-recurrent-neural-networks&quot;&gt;Data2Vis: Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural Networks&lt;/h1&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;end-to-end trainable neural translation model&lt;/li&gt;
  &lt;li&gt;formulate visualization generation as &lt;strong&gt;a language translation problem&lt;/strong&gt;, where data specifications are mapped to visualization specifications in a declarative language &lt;strong&gt;(Vega-Lite)&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;Vege-Lite -&amp;gt; JSON format&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;multilayered ateention-based encoder-decoder network with LSTM&lt;/li&gt;
  &lt;li&gt;introduce 2 metrics - language syntax validity, visualization grammar syntax validity&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-work&quot;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&quot;declarative-visualization-specification&quot;&gt;Declarative Visualization Specification&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;One of our aims with Data2Vis is to bridge this gap between the speed and expressivity in specifying visualizations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;automated-visulaization&quot;&gt;Automated Visulaization&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;We pose visualization specifica- tion as a machine translation problem and intro- duce Data2Vis, a deep neural translation model trained to automatically translate data specifica- tions to visualization specifications. Data2Vis emphasizes the creation of visualizations using rules learned from examples, without resorting to a predefined enumeration or extraction of con- straints, rules, heuristics, and features.&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Machine Translation Problem&lt;/strong&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;dnns-for-machine-translation&quot;&gt;DNNs for Machine Translation&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;Data2Vis is also a sequence- to-sequence model using the textual source and target specifications directly for translation, with- out relying on explicit syntax representations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;

&lt;p&gt;&lt;img width=&quot;754&quot; alt=&quot;스크린샷 2021-05-07 오전 1 43 49&quot; src=&quot;https://user-images.githubusercontent.com/48315997/117335052-adad1280-aed5-11eb-9188-0b30c1cb9533.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the data visualization problem as a &lt;strong&gt;Seq2Seq translation problem&lt;/strong&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input : dataset (fields, values in JSON format)
output : valid Vega-Lite visualization specification
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;encoder-decoder archi.&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;where the encoder reads and encodes a source sequence into a fixed length vector, and a decoder outputs a translation based on this vec- tor.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Attention&lt;/strong&gt; Mechanism
    &lt;blockquote&gt;
      &lt;p&gt;Atten- tion mechanisms allow a model to focus on aspects of an input sequence while generating out- put tokens.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Beam Search algorithm&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;The beam search algorithm used in sequence-to-sequence neural translation models keeps track of k most probable output tokens at each step of decoding, where k is known as the beamwidth. This enables the generation of k most likely output sequences for a given input sequence.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;THREE techniques : &lt;strong&gt;bidirectional encoding, differential weighing of context via an attention mechanism, and beam search&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;character&lt;/strong&gt;-based sequence model&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data-and-preprocessing&quot;&gt;Data and Preprocessing&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;the model must select a subset of fields to focus on when creating visual- izations (most datasets have multiple fields that cannot all be simultaneously visualized)&lt;/li&gt;
  &lt;li&gt;the model must learn differences in data types across the data fields (numeric, string, temporal, ordinal, categorical, etc.), which in turn guides how each field is specified in the generation of a visualiza- tion specification.&lt;/li&gt;
  &lt;li&gt;the model must learn the appropriate transformations to apply to a field given its data type (e.g., aggregate transform does not apply to string fields).&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;view-level transforms : aggregate, bin, calculate, filter, timeUnit&lt;/li&gt;
  &lt;li&gt;field-level transforms : aggregate, bin, sort, timeUnit&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluation-metrics&quot;&gt;Evaluation Metrics&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;language syntax validity(lsv)
    &lt;ul&gt;
      &lt;li&gt;measure of how well a model learns the syntax of the underlying language used to specify the visualization.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;grammar syntax validity(gsv)
    &lt;ul&gt;
      &lt;li&gt;a measure of how well a model learns the syntax of the grammar for visualization specification.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;
&lt;p&gt;&lt;img width=&quot;887&quot; alt=&quot;스크린샷 2021-05-07 오전 1 49 00&quot; src=&quot;https://user-images.githubusercontent.com/48315997/117335706-66735180-aed6-11eb-90cc-95b38ea1224a.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;765&quot; alt=&quot;스크린샷 2021-05-07 오전 1 49 10&quot; src=&quot;https://user-images.githubusercontent.com/48315997/117335723-6bd09c00-aed6-11eb-8c0e-9e9d0e1348a2.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Field Selection and Transformation&lt;/li&gt;
  &lt;li&gt;Training Data&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Training Data and Training Strategy&lt;/li&gt;
  &lt;li&gt;Extending Data2Vis to Generate Multiple Plausible Visualizations&lt;/li&gt;
  &lt;li&gt;Targeting Additional Grammars&lt;/li&gt;
  &lt;li&gt;Natural Language and Visualization Specification&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Visualization" /><summary type="html">Data2Vis: Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural Networks 을 읽고 정리한 글입니다.</summary></entry><entry><title type="html">Stanford CS231n Lec 02. Image Classification</title><link href="http://localhost:4000/cs231n/2021/03/21/cs231n_lec02/" rel="alternate" type="text/html" title="Stanford CS231n Lec 02. Image Classification" /><published>2021-03-21T00:00:00+09:00</published><updated>2021-03-21T00:00:00+09:00</updated><id>http://localhost:4000/cs231n/2021/03/21/cs231n_lec02</id><content type="html" xml:base="http://localhost:4000/cs231n/2021/03/21/cs231n_lec02/">&lt;p&gt;Stanford CS231n 2017 강의를 듣고 개인적으로 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;lecture-2--image-classification-pipeline&quot;&gt;Lecture 2 : Image Classification pipeline&lt;/h1&gt;
&lt;h2 id=&quot;image-classification&quot;&gt;Image Classification&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Computer Vision Task&lt;/li&gt;
  &lt;li&gt;Problem is …
    &lt;ul&gt;
      &lt;li&gt;Semantic Gap : between image and pixels (what the computer sees)
        &lt;ul&gt;
          &lt;li&gt;Computer understands the image as a big grid of numbers (800,600,3)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Challenges (algorithm should be robust to these challenges)
    &lt;ul&gt;
      &lt;li&gt;Viewpoint variation
        &lt;ul&gt;
          &lt;li&gt;all pixels change when the viewpoint is changed&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Illumination
        &lt;ul&gt;
          &lt;li&gt;different light condition&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Deformation
        &lt;ul&gt;
          &lt;li&gt;Example : cat…&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Occlusion
        &lt;ul&gt;
          &lt;li&gt;The image shows just “part” of a cat&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Background Cluttuer&lt;/li&gt;
      &lt;li&gt;I track as Variation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;image-classifier&quot;&gt;Image Classifier&lt;/h2&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;classify_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;# some magic!
&lt;/span&gt;	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_label&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;Attempts have been made
    &lt;ul&gt;
      &lt;li&gt;find edges and corners&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Data-Driven Approach&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Collect a dataset of images and labels&lt;/li&gt;
      &lt;li&gt;Use ML to train a classifier&lt;/li&gt;
      &lt;li&gt;Evaluate the classifier on new images
        &lt;h2 id=&quot;first-classifier--nearest-neighbor&quot;&gt;First Classifier : &lt;strong&gt;Nearest Neighbor&lt;/strong&gt;&lt;/h2&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;train : &lt;strong&gt;memorize all training data&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;O(1)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Predict : predict &lt;strong&gt;the label of most similar training image&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;O(N)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;But we want classifier that are fast at prediction; slow for training is OK.&lt;/strong&gt;**&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;K-Nearest Nighbors&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Take &lt;strong&gt;majority vote&lt;/strong&gt; from K closest points
  &lt;img src=&quot;https://user-images.githubusercontent.com/48315997/112023051-36fec480-8b76-11eb-819a-447974d99b0c.png&quot; alt=&quot;image&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Distance Metric (to compare images)
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/112022917-12a2e800-8b76-11eb-93f5-c1a1dd4cc39d.png&quot; alt=&quot;image&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;L1 distance(Manhattan distance)
        &lt;ul&gt;
          &lt;li&gt;Calculate the difference of image&lt;/li&gt;
          &lt;li&gt;Depends on &lt;strong&gt;choice of coordinate system&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;Use when individual vector is meaningful&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;L2 distance(Euclidean distance)
        &lt;ul&gt;
          &lt;li&gt;Use when &lt;strong&gt;generic vector&lt;/strong&gt; in some space&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;kNN on images never used&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Very slow at test time&lt;/li&gt;
      &lt;li&gt;Distance metrics on pixels are not informative
        &lt;ul&gt;
          &lt;li&gt;couldn’t reflected “perceptional distance”&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Curse of dimensionality
  &lt;img src=&quot;https://user-images.githubusercontent.com/48315997/112023009-2f3f2000-8b76-11eb-85b2-5ad0eba72979.png&quot; alt=&quot;image&quot; /&gt;
        &lt;ul&gt;
          &lt;li&gt;If dimensions are increased in image, data points must densely cover to these dimensions&lt;/li&gt;
          &lt;li&gt;Training examples are exponentially needed.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hyperparameters--pipeline&quot;&gt;Hyperparameters &amp;amp; Pipeline&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Problem-dependent
    &lt;ul&gt;
      &lt;li&gt;try them all out and see what works best …&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Split data into &lt;strong&gt;train, val, and test&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;underlying : the same probability distribution&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cross-Validation
    &lt;ul&gt;
      &lt;li&gt;Split data into &lt;strong&gt;folds&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;We can know which hyperparameters are going to perform more &lt;strong&gt;robustly&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Useful to small datasets -&amp;gt; not used too frequently in deep learning&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;linear-classification&quot;&gt;Linear Classification&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Parametric Approach : summarize knowledge of training examples &amp;amp; stick all that knowledge into W.&lt;/li&gt;
  &lt;li&gt;Image -&amp;gt; f(x,W) -&amp;gt; N numbers giving class scores
    &lt;ul&gt;
      &lt;li&gt;W : parameters or weights&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;f(x,W) = Wx + b (# of classes = 10, input dimension = 3072)
    &lt;ul&gt;
      &lt;li&gt;f(x,W) : 10x1&lt;/li&gt;
      &lt;li&gt;W should be 10 x 3072&lt;/li&gt;
      &lt;li&gt;x : 3072x1&lt;/li&gt;
      &lt;li&gt;b : 10x1&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Wx&lt;/code&gt; gives classes’ scores&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Bias
    &lt;ul&gt;
      &lt;li&gt;constant vector&lt;/li&gt;
      &lt;li&gt;Not interact with training set&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;data independent&lt;/strong&gt;, preferences for some classes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Overview
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/112023440-98bf2e80-8b76-11eb-837b-fb2bc8758a44.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Interpretation of linear classifiers as &lt;strong&gt;template matching&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;1 class : 1 template (driven from training data)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hard cases for a linear classifier&lt;/li&gt;
  &lt;li&gt;Question
    &lt;ul&gt;
      &lt;li&gt;how can we tell &lt;strong&gt;whether this W is good or bad?&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yeonsoo Kim</name></author><category term="cs231n" /><category term="Deep Learning" /><category term="lecture note" /><category term="Computer Vision" /><summary type="html">Stanford CS231n 2017 강의를 듣고 개인적으로 정리한 글입니다.</summary></entry><entry><title type="html">지난 한 달을 돌아보며, 2021년 2월 회고</title><link href="http://localhost:4000/logs/2021/03/14/2021_February/" rel="alternate" type="text/html" title="지난 한 달을 돌아보며, 2021년 2월 회고" /><published>2021-03-14T00:00:00+09:00</published><updated>2021-03-14T00:00:00+09:00</updated><id>http://localhost:4000/logs/2021/03/14/2021_February</id><content type="html" xml:base="http://localhost:4000/logs/2021/03/14/2021_February/">&lt;p&gt;2021년 2월 회고 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;목차&quot;&gt;목차&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;ETRI 인턴 종료&lt;/li&gt;
  &lt;li&gt;이화보이스 인터뷰&lt;/li&gt;
  &lt;li&gt;AI/데이터분석 동아리 개설&lt;/li&gt;
  &lt;li&gt;3월의 각오&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;etri-인턴-종료&quot;&gt;ETRI 인턴 종료&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/111064641-7edb7700-84f8-11eb-9dfb-1e5add8fa87d.jpeg&quot; alt=&quot;DD64C628-9482-43AE-B258-D6D83EF86E7A_1_105_c&quot; /&gt;&lt;/p&gt;

&lt;p&gt;벌써 두 달이 지나 연구연수생 기간이 끝났다. 시간은 내 생각보다도 더 쏜살같이 흘러간다는 걸 몸소 느낄 수 있었다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;결론적으로 말하면 ETRI 인턴을 하길 참 잘했다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;좋은 동기들, 멋있는 박사님들과 선배님들, 이  모두와 함께 할 수 있어 영광이었다.&lt;/p&gt;

&lt;p&gt;또 그동안 잘 몰랐기에 관심 없던 분야에 대해 새롭게 알게 되는 쾌감을 느낄 수 있었다.&lt;/p&gt;

&lt;p&gt;특허는 작성했고 논문은 작성할지 안할지 미지수인 상태로 떠났다. 논문을 당연히 써야된다고 생각했지만 이것저것 우려되는 부분들이 있어서 지금도 고민 중이다.&lt;/p&gt;

&lt;p&gt;동기들한테 특히나 고마웠다. 미숙하고 툴툴거리는 내 성격을 재밌게 잘 받아주고 또 서로 열정적이라 누구 하나 낙오되지 않고 정말 열심히 일할 수 있었다. 내가 아는 ETRI 인턴들, 다른 랩 연구연수생들 중에 우리가 가장 바쁘게 일하지 않았을까 싶다. 야근과 주말 출근을 자주 했으니… (우리끼리 다음 기수 인턴들이 불쌍하다고 계속 말했음ㅋㅋ)&lt;/p&gt;

&lt;p&gt;특허나 논문을 내기에 두 달이라는 시간은 매우 짧은 시간이라고 여겨졌다. 그래서 사실 ‘아, 못해도 그만이지! 배운 게 많으니까’ 이런 류의 생각을 했었는데 &lt;strong&gt;생각보다 할 만하다.&lt;/strong&gt; 박사님들께서 정말 많이 도와주시고 인프라도 빵빵하니 본인의 의지만 있다면 못 할 기간이 절대 아니었다.&lt;/p&gt;

&lt;p&gt;ETRI 인턴에 대한 후기는 아예 다른 글로 파서 쓰려고 한다. 어찌됐든 나는 ETRI 인턴을 적극 추천하는 입장이다. 이 정도의 환경과 사람을 충족시키는 회사가 많지 않을 거라 생각되기 때문이다. 사람도 좋고 일도 좋다면 마다할 이유가 있을까?&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;이화보이스-인터뷰&quot;&gt;이화보이스 인터뷰&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/111064661-9e729f80-84f8-11eb-8f2a-c0f4458f93b5.png&quot; alt=&quot;스크린샷 2021-03-14 오후 7 08 09&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우연찮은 기회로 이화보이스에 인터뷰를 하게 되었다.&lt;/p&gt;

&lt;p&gt;현재 나는 아는 동생이랑 이화여대 AI/데이터분석 분야 오카방과 디스코드를 같이 운영하고 있는데 여기에 계신 이화보이스 기자님께서 코로나시대 네트워킹에 대한 주제로 인터뷰를 요청해주셨다.&lt;/p&gt;

&lt;p&gt;굉장히 영광이었고 사실 나는 딱히 한 일이 없는데 어떤 답변을 드려야 할까 많은 고민을 했다.&lt;/p&gt;

&lt;p&gt;또 이화보이스는 학교 공식 영자 신문인지라 꽤 부담이 된 것도 사실이었다.&lt;/p&gt;

&lt;p&gt;하지만 기자벗께서 굉장히 분위기를 잘 풀어주셨고, 우리가 답변할 수 있을 정도의 좋은 질문들을 주셔서 재밌는 인터뷰가 가능했다.&lt;/p&gt;

&lt;p&gt;우리 학교 네트워킹의 장을 조금이라도 넓히고 싶었던 나의 작은 바람이 점차 많은 이화인에게 닿는 것 같아 뿌듯했다. 또 더 열심히 (공부)해야 겠다는 동기부여도 확실히 됐다.&lt;/p&gt;

&lt;p&gt;이 네트워킹의 장이 별 탈없이, 부디 오래 갈 수 있으면 좋겠다.&lt;/p&gt;

&lt;p&gt;기사의 전문은 &lt;a href=&quot;https://evoice.ewha.ac.kr/news/articleView.html?idxno=10596&quot;&gt;여기&lt;/a&gt;서 볼 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;ai데이터분석-동아리-개설&quot;&gt;AI/데이터분석 동아리 개설&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/111064679-b813e700-84f8-11eb-929f-9024599f1417.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;작년부터 데이터 사이언스 분야 학회를 하나 만들까 고민하긴 했으나, 졸프 등 사정 상 부담이 많이 되는 일은 사실인지라 가능하면 피하려고 했다. 하지만 결국 만들게 되었다… ㅋㅋㅋㅋ 심지어 졸프 메이트까지 꼬셔서 같이~&lt;/p&gt;

&lt;p&gt;우리 학교 사람들이 이 분야에 더 많이 진출하고, 더 많이 꿈을 꿨으면 좋겠다는 생각은 항상 갖고 있었기 때문에 이 동아리를 반드시 잘 키우고 싶다. 사실 어려울 것이 없는 게 나한테는 든든한 임원진들도 있고, 또 열정 있는 부원들까지 있다. 내가 방향성만 잃지 않으면 충분히 된다.&lt;/p&gt;

&lt;p&gt;내가 어엿한 사회인이 되어 후배님들을 도와줄 수 있는 위치에 갈 때까지 이 동아리가 쭉 이어져나갔으면 좋겠다.&lt;/p&gt;

&lt;p&gt;굉장히 빡센 커리큘럼으로 진행하는데, 탈주자 없이 모두가 성장할 수 있는 한 해가 되길!&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;3월의-각오&quot;&gt;3월의 각오&lt;/h3&gt;

&lt;p&gt;사실 이 글을 쓰는 시점이 3/14일이긴 한데… 3월의 할 일은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;졸업 프로젝트 주제 확정&lt;/li&gt;
  &lt;li&gt;강의 열심히 듣기, 과제 잘 하기&lt;/li&gt;
  &lt;li&gt;동아리 공부 잘 하기&lt;/li&gt;
  &lt;li&gt;이교수님 강의, 논문 따라 가기&lt;/li&gt;
  &lt;li&gt;논문 스터디&lt;/li&gt;
  &lt;li&gt;건강 챙기기&lt;/li&gt;
  &lt;li&gt;(ETRI 논문 작성?) -&amp;gt; 완전 미정.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yeonsoo Kim</name></author><category term="월간 회고" /><summary type="html">2021년 2월 회고 글입니다.</summary></entry><entry><title type="html">지난 한 달을 돌아보며, 2021년 1월 회고</title><link href="http://localhost:4000/logs/2021/02/14/2021_January/" rel="alternate" type="text/html" title="지난 한 달을 돌아보며, 2021년 1월 회고" /><published>2021-02-14T00:00:00+09:00</published><updated>2021-02-14T00:00:00+09:00</updated><id>http://localhost:4000/logs/2021/02/14/2021_January</id><content type="html" xml:base="http://localhost:4000/logs/2021/02/14/2021_January/">&lt;p&gt;2021년 1월 회고 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;목차&quot;&gt;목차&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;ETRI 1개월 차&lt;/li&gt;
  &lt;li&gt;데이터 분석 스터디 종료&lt;/li&gt;
  &lt;li&gt;기타&lt;/li&gt;
  &lt;li&gt;한 달 동안 한 공부&lt;/li&gt;
  &lt;li&gt;남은 방학 동안 할 일&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;1️⃣-etri-1개월-차&quot;&gt;1️⃣ ETRI 1개월 차&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107854608-48123280-6e60-11eb-9221-aeba67c94e38.jpeg&quot; alt=&quot;E11D4A80-51E2-4D70-8452-DDB5C3B6B70D_1_105_c&quot; /&gt;&lt;/p&gt;

&lt;p&gt;어느새 ETRI 인턴 한 달이 지났다. (지금은 약 2주정도밖에 안남았지만…)
한 달간 한 일을 정리해보자면 다음과 같다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Active Learning 개념 공부 및 논문 리딩&lt;/li&gt;
  &lt;li&gt;Active Learning - Uncertainty Sampling Implementation&lt;/li&gt;
  &lt;li&gt;실험에 필요한 데이터들 라벨링&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이전까지 나는 딥러닝 모델 자체에 관심이 있었던지라 Active Learning이랑 Human in the Loop 개념을 이번에 새로 알았다.
Active Learning은 요약하자면 적은 데이터 수로 높은 성능을 낼 수 있는 것이 목적인데, 이 때 &lt;strong&gt;그 적은 데이터들을 무슨 알고리즘으로 샘플링하는가&lt;/strong&gt;에 관한 이야기이다.&lt;/p&gt;

&lt;p&gt;책임님께서 레퍼런스할 만한 논문, 아티클을 주셔서 실험의 갈피를 빠르게 잡을 수 있었다.
또 데이터를 받기까지 시간이 조금 걸려서 그 동안 Uncertainty Sampling을 구현하고 오픈 데이터셋으로 실험해보았다.&lt;/p&gt;

&lt;p&gt;1월이 가장 힘들었던 이유는 라벨링^_^… 인데…
나는 이쪽의 모든 일을 참여하기 전에 라벨링 일을 하는지 안하는지 확인하고, 라벨링 업무가 낀다면 그냥 안한다^^…
그래서 ETRI 지원할 때도 라벨링해야할 것 같은 부서는 다 제외하고 생각했는데 ㅠ_ㅠ 내가 지원한 곳에서 갑자기 라벨링을 시키실 줄은 몰랐다..!(얘기 없었잖아요!!)&lt;/p&gt;

&lt;p&gt;아무튼 2주간 라벨링을 하고,, 또 112 데이터라 그런지 정신 피폐해질 만한 내용이 많았다ㅋㅋㅋ 내가 둔하고 멘탈이 괜찮은 편이라 다행이었지, 감정이입 잘하는 사람이 봤으면 힘들었을 것 같다.&lt;/p&gt;

&lt;p&gt;2월부터는 실험을 시작했는데, 그건 2월 회고에 기록할 것.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;2️⃣데이터-분석-스터디-종료&quot;&gt;2️⃣데이터 분석 스터디 종료&lt;/h3&gt;

&lt;p&gt;10월에 모집하여 진행했던 데이터 분석 스터디가 드디어 마무리 됐다!
후반부에는 내가 정신이 없어서 공부를 많이 못 했지만ㅠㅠ 그래도 좋은 벗들을 만나서 너무 기분이 좋았다 :)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107866777-7a02b380-6eb7-11eb-9dc2-135c04404f49.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;‘파이썬 머신러닝 완벽 가이드’ 라는 책은 내가 이전에 한 번 읽은 책이긴 하지만, 이렇게 벗들이랑 다시 세미나 형태로 공부하니까 또 새롭게 와닿았다. 공부는 역시 할 때마다 새로운 건가?
이전에는 앙상블, 부스팅 기법을 깊게 생각하지 않고 공부했는데 실제 캐글이나 데이콘과 같은 데이터 사이언스 컴피티션에서는 매우 잘 쓰이는 기법들이다. 아무튼 이러한 내용들을 세미나를 준비하면서 공부할 수 있어서 좋았다.&lt;/p&gt;

&lt;p&gt;우리 학교 벗들이 데이터 분석과 인공지능 분야에 많이 많이 진입했으면 좋겠다 :)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;3️⃣-기타&quot;&gt;3️⃣ 기타&lt;/h3&gt;

&lt;p&gt;첫 자취 생활로 인해 몸과 마음이 조금 힘들었었다 ㅠㅠ&lt;/p&gt;

&lt;p&gt;그래서 1월에만 호캉스를 두 번 했다!&lt;/p&gt;

&lt;p&gt;첫 번째 호캉스는 워커힐이다. 아빠가 워커힐 쪽에서 바우처랑 카드들을 받아서 워커힐 스위트룸에서 묵을 수 있었다! 엄마아빠 // 나 이렇게 해서 각각 방 1개씩을 썼다. 나 혼자 스위트룸 쓴 적은 처음이다 ㅎㅎㅎ&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107870217-cd85f900-6ed9-11eb-9bc6-1374187ccc5b.jpeg&quot; alt=&quot;90B06C1A-7396-41AC-8EF6-A23F6D707C44_1_105_c&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107870215-c6f78180-6ed9-11eb-9975-a4bc47ed7582.jpeg&quot; alt=&quot;FE0403FB-98D0-4BD6-8DF9-B343FF2D1FCD_1_105_c&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107870223-dd054200-6ed9-11eb-9666-b3682e317ec1.jpeg&quot; alt=&quot;E001E8C5-7F44-4A72-9A27-56E05590520B_1_105_c&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107870251-e8586d80-6ed9-11eb-897a-82fcea7590cb.jpeg&quot; alt=&quot;30019A7C-B51E-467C-945C-2EC7F62D30C1_1_105_c&quot; /&gt;&lt;/p&gt;

&lt;p&gt;한강이 바로 보이는 것도 마음에 들었고, 4층에 스카이야드도 굉장히 좋았다. 오픈 시간에 바로 간 거라 족욕도 했다!&lt;/p&gt;

&lt;p&gt;두 번째 호캉스는 엄마가 대전에 왔을 때 휴가내고 간 ‘대전 롯데시티호텔’이다. 이 때 정말 서울 공화국이라는 것을 느꼈는데, 대전은 제일 좋은 호텔이 4성급이다ㅜㅜ 아무튼 이 때쯤에 힘들었던 일들이 많았어서 휴가를 내고 호캉스를 갔다. 결과적으로는 밤에 일을 하긴 했지만 …^^&lt;/p&gt;

&lt;p&gt;여기는 한강뷰는 아니고 갑천?뷰 ㅋㅋㅋㅋ 되게 깔끔했고, 가성비 있는 호텔이었다!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107870334-982ddb00-6eda-11eb-9aa1-bd2c23f6b77b.jpeg&quot; alt=&quot;B635E2B2-B96E-479A-8CD4-6C709BBA2C70_1_105_c&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107870332-96fcae00-6eda-11eb-9b28-514afb771b8b.jpeg&quot; alt=&quot;B9009AFF-7402-49E0-B58E-E85AABF755D1_1_105_c&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 바로 맞은 편에 성심당이 있어서 바로 달려갔다. 나랑 우리 엄마는 빵을 매우 좋아하기 때문에… 사고 싶은 빵들이 무척이나 많았지만 나름 절제하며 골랐다 ㅋㅋ 초코 튀김 소보로를 먹었는데 생각보다 팥맛이 많이 나서 아쉬웠다ㅠㅠ 명물이라는 명란 바게트는 저녁이랑 아침 모두 sold out…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107870326-8cdaaf80-6eda-11eb-8e15-6d41c70f971d.jpeg&quot; alt=&quot;576C4EBC-F28D-4ECE-9432-A16B8C44D9A8_1_105_c&quot; /&gt;&lt;/p&gt;

&lt;p&gt;휴가 날에는 대전 현대프리미엄아울렛도 갔다! 생긴지 얼마 안된 곳이라 굉장히 깔끔했다. 또 평일 낮이라 그런가 사람들이 별로 없어서 매우 좋았다!
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/107870367-24400280-6edb-11eb-9495-41ffbae82b11.jpeg&quot; alt=&quot;45D7BCF6-C915-4251-A660-7882422F783E_1_105_c&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아무튼 1월 중순부터는 가족들이 내 기분을 풀어주려고 정말 많이 도와줬다. 역시 가족이 최고야 &amp;lt;3
(친구들도 최고야)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;4️⃣한-달-동안-한-공부&quot;&gt;4️⃣한 달 동안 한 공부&lt;/h3&gt;

&lt;p&gt;1월 동안 한 공부를 정리해보자면 다음과 같다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Active Learning (인턴 업무)&lt;/li&gt;
  &lt;li&gt;논문 스터디(Transformer, GAN, BERT, StarGAN, GPT-1)&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;딱히 공부를 한 건 별로 없구나. 반성…
직장을 다니면서 공부하는 사람들이 정말 대단하게 보인다. 나는 인턴일 뿐인데도 퇴근하고 오면 뻗기 일수이다 ㅠㅠ 체력을 길러야 할 것 같다&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;5️⃣남은-방학-동안-할-일&quot;&gt;5️⃣남은 방학 동안 할 일&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;AI/데이터분석 동아리 모집 및 선발&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;원래 만들까 말까 고민했는데 어찌저찌 만들게 됐다(현재 2월 상황)
이미 하게 된 거, 책임감 있고 이화의 역사에 한 줄을 긋는 동아리가 되도록 열심히 노력할 것이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;논문 스터디&lt;/li&gt;
  &lt;li&gt;특허 작성&lt;/li&gt;
  &lt;li&gt;논문 작성&lt;/li&gt;
  &lt;li&gt;여유가 된다면, Pix2Pix 구현&lt;/li&gt;
  &lt;li&gt;건강한 음식 먹기(식단 관리) &amp;amp; 자주 걷기&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;특허 작성과 논문 작성이 가장 주가 될 듯 하다. 과연 2월안에 다 할 수 있을까? 해야만 한다ㅠ&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="월간 회고" /><summary type="html">2021년 1월 회고 글입니다.</summary></entry><entry><title type="html">Ubuntu 18.04 clean, ***/*** files, ***/*** blocks 문제해결</title><link href="http://localhost:4000/troubleshooting/2021/01/26/Ubuntu-clean-files-blocks-trouble/" rel="alternate" type="text/html" title="Ubuntu 18.04 clean, ***/*** files, ***/*** blocks 문제해결" /><published>2021-01-26T00:00:00+09:00</published><updated>2021-01-26T00:00:00+09:00</updated><id>http://localhost:4000/troubleshooting/2021/01/26/Ubuntu-clean-files-blocks-trouble</id><content type="html" xml:base="http://localhost:4000/troubleshooting/2021/01/26/Ubuntu-clean-files-blocks-trouble/">&lt;p&gt;Ubuntu 18.04에서 clean, ***/*** files, ***/*** blocks 에러가 떴을 때 고치는 방법을 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;잡소리&quot;&gt;잡소리&lt;/h2&gt;

&lt;p&gt;오늘 연구실에서 정말 당황한 에러를 마주쳤습니다.&lt;/p&gt;

&lt;p&gt;사건의 역시나 CUDA에서 시작… 저번주까지만 해도 잘 됐던 gpu사용이 오늘 하려니 갑자기 찾을 수 없다(?)는 에러가 떴습니다.&lt;/p&gt;

&lt;p&gt;침착하게 그 에러 메시지를 구글링해서 스택오버플로우에서 하라는 대로 하고 재부팅했는데 아래와 같은 에러가 뜨고 무한 블랙스크린이었습니다 ㅜㅜ.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/fBYUJ.jpg&quot; alt=&quot;&quot; /&gt; &lt;a href=&quot;https://askubuntu.com/questions/1222496/system-wont-boot-stuck-at-dev-sda5-clean-xxxx-xxxx-files-yyyy-yyyy-blocks&quot;&gt;이미지 출처&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;진심 저거 봤을 때 눈물이 찔끔 나올 뻔 했는데요 ..ㅋㅋㅋ 스택오버플로우보니까 아예 우분투를 재설치 해야 할 수도 있다는 글을 보고… 진짜 온몸이 오싹해졌습니다.&lt;/p&gt;

&lt;h2 id=&quot;solution&quot;&gt;Solution&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;우선 다시 부팅하고 recovery mode (safe mode) 로 들어가야 합니다. 부팅될 때 무한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Shift&lt;/code&gt; 누르십시오. (우분투 18.04 기준)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Advanced options for Ubuntu&lt;/code&gt; 를 클릭하고, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;recovery mode&lt;/code&gt;로 들어갑니다.
제가 캡쳐할 상황은 아니었던지라 다 인터넷에서 들고 오는 점 이해 부탁드립니다^^; &lt;a href=&quot;https://askubuntu.com/questions/92556/how-do-i-boot-into-a-root-shell&quot;&gt;출처&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/01e8n.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://i.stack.imgur.com/UP5j7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;이제 리커버리 모드에 들어왔는데요. 여기서 방향키를 사용해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;root&lt;/code&gt;로 이동하고 Enter합니다. 이제 터미널을 쓸 수 있습니다! 
&lt;img src=&quot;https://i.stack.imgur.com/tHkmh.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이제 터미널에서&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt-get purge nvidia*&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;를 치면 됩니다만, 여기서 저는&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dpkg was interrupted, you must manually run 'sudo dpkg --configure -a' to correct the problem
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;라는 에러가 또 뜹니다.&lt;/p&gt;

&lt;p&gt;여기서 제 부팅 실패의 원인을 유추해볼 수 있었습니다. 재부팅 전에 했던 동작들에서 뭔가를 건드렸고 그 결과 dpkg 패키지가 제대로 구성되지 않았던 겁니다.(maybe….)&lt;/p&gt;

&lt;p&gt;아무튼 이 에러는 상대적으로 쉽게 해결할 수 있습니다.
에러 메시지대로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo dpkg --configure -a&lt;/code&gt; 를 해주면 됩니다.&lt;/p&gt;

&lt;p&gt;이제 다시&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt-get purge nvidia*&lt;/code&gt;
를 하게 되면 잘 설치 됩니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reboot&lt;/code&gt; 를 써서 재부팅합니다 -&amp;gt; 성공!&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;이 일은 저에게 있어 정말 끔찍했던 순간이었습니다. 퇴사해야하나 생각도 했어요 ㅋㅋㅋ 아무튼 빠르게 고치고 퇴근했습니다^^.. 혹시 구글링하다 저와 같은 에러를 발견하시게 된다면, 위 과정을 통해 고치길 바라겠습니다.&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Ubuntu" /><category term="Troubleshooting" /><summary type="html">Ubuntu 18.04에서 clean, ***/*** files, ***/*** blocks 에러가 떴을 때 고치는 방법을 정리한 글입니다.</summary></entry><entry><title type="html">Pre-training of Deep Bidirectional Transformers for Language Understanding(BERT)</title><link href="http://localhost:4000/paper%20review/2021/01/25/BERT/" rel="alternate" type="text/html" title="Pre-training of Deep Bidirectional Transformers for Language Understanding(BERT)" /><published>2021-01-25T00:00:00+09:00</published><updated>2021-01-25T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2021/01/25/BERT</id><content type="html" xml:base="http://localhost:4000/paper%20review/2021/01/25/BERT/">&lt;p&gt;Pre-training of Deep Bidirectional Transformers for Language Understanding(BERT) 을 읽고 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;bidirectional&lt;/strong&gt;이라는 점이 이전 연구들과의 가장 큰 차이점이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create SOTA ~...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;fine-tuning이 매우 용이한 모델이다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;여러 가지 task에서 SOTA를 기록했음.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Applying pre-trained language representations to downstream tasks&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feature-based&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;ELMo&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fine-tuning&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;GPT&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;limitation-of-unidirectional-approach&quot;&gt;Limitation of Unidirectional approach&lt;/h3&gt;

&lt;p&gt;두 접근법 모두 &lt;strong&gt;unidirectional&lt;/strong&gt; 방법을 사용함.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;But, unidirectional 방법은 한계점이 뚜렷함.&lt;/li&gt;
  &lt;li&gt;GPT(1)처럼 left-to-right architecture를 사용하게 되면, 모든 token들은 이전의 token들에만 self-attention할 수 있게 된다.
    &lt;ul&gt;
      &lt;li&gt;이는 token-level task (예를 들어 QA)들에 부적합하다. 이러한 태스크들은 양 방향을 봐서 &lt;strong&gt;context&lt;/strong&gt;를 읽어야하기 때문.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;masked-langauge-modelmlm&quot;&gt;Masked Langauge Model(MLM)&lt;/h3&gt;

&lt;p&gt;BERT에서는 pre-training 할 때, 위에서 언급한 unidirectionality constraint를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Masked Language Model(MLM)&lt;/code&gt; 으로 완화(?)했음.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MLM은 input 토큰들로부터 &lt;strong&gt;randomly mask&lt;/strong&gt;함.
    &lt;ul&gt;
      &lt;li&gt;이것의 목적은, &lt;strong&gt;오직 context만 가지고&lt;/strong&gt; mask된 original vocab.을 맞추는 것.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Left-to-right LM pre-training과 달리, MLM의 목적 함수는 양 방향의 representations를 이해할 수 있다.
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  which allows us to pre-train a deep bidirectional Transformer
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;next-sentence-prediction&quot;&gt;Next Sentence Prediction&lt;/h3&gt;

&lt;p&gt;jointly pre-train text-pair representation&lt;/p&gt;

&lt;h2 id=&quot;bert&quot;&gt;BERT&lt;/h2&gt;

&lt;p&gt;&lt;img width=&quot;766&quot; alt=&quot;스크린샷 2021-01-25 오후 9 09 09&quot; src=&quot;https://user-images.githubusercontent.com/48315997/105704076-91eab580-5f51-11eb-8b52-16c48146bd50.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;pre-training&quot;&gt;Pre-training&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;During pre-training, the model is trained on unlabeled data over different pre-training tasks.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Pre-training 단계에서 모델은 &lt;strong&gt;unlabeled data&lt;/strong&gt;를 가지고 학습된다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TWO Unsupervised task를 사용함.&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;task-1-masked-lm&quot;&gt;Task #1. Masked LM&lt;/h4&gt;

&lt;p&gt;deep bidirectional representation을 학습시키기 위해, input token에서 랜덤으로 몇 %정도를 masking하고 이 masked token들을 predict하도록 학습시킴.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;[MASK]&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Random&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Unchanged&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;80%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;task-2-next-sentence-prediction-nsp&quot;&gt;Task #2. Next Sentence Prediction (NSP)&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;두 문장 사이의 관계(relationship)를 이해&lt;/strong&gt;하기 위해 학습하는 task이다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IsNext&lt;/code&gt;인지 아닌지(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NotNext&lt;/code&gt;) 분류하는 binary classification 문제임.&lt;/p&gt;

&lt;h3 id=&quot;fine-tuning-bert&quot;&gt;Fine-tuning BERT&lt;/h3&gt;

&lt;p&gt;적절한 Input과 Output을 넣으면, single text나 text pair 모두 가능.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="BERT" /><category term="Attention" /><category term="Transformer" /><summary type="html">Pre-training of Deep Bidirectional Transformers for Language Understanding(BERT) 을 읽고 정리한 글입니다.</summary></entry><entry><title type="html">2020년을 돌아보며, 2020 YEAR in REVIEW</title><link href="http://localhost:4000/logs/2020/12/31/2020_YEAR_in_REVIEW/" rel="alternate" type="text/html" title="2020년을 돌아보며, 2020 YEAR in REVIEW" /><published>2020-12-31T00:00:00+09:00</published><updated>2020-12-31T00:00:00+09:00</updated><id>http://localhost:4000/logs/2020/12/31/2020_YEAR_in_REVIEW</id><content type="html" xml:base="http://localhost:4000/logs/2020/12/31/2020_YEAR_in_REVIEW/">&lt;p&gt;2020년 저에게 있었던 일을 짧게 회고하는 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;목차&quot;&gt;목차&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;수화인식 웹 어플리케이션 프로젝트 완성&lt;/li&gt;
  &lt;li&gt;데이터 분석 공부&lt;/li&gt;
  &lt;li&gt;SKT AI Fellowship 2기 지원과 탈락&lt;/li&gt;
  &lt;li&gt;논문 스터디&lt;/li&gt;
  &lt;li&gt;HAICON 2020 입선&lt;/li&gt;
  &lt;li&gt;1,2학기 모두 4점대로 마무리&lt;/li&gt;
  &lt;li&gt;2021년 목표&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;1️⃣-수화인식-웹-어플리케이션-프로젝트-완성&quot;&gt;1️⃣ 수화인식 웹 어플리케이션 프로젝트 완성&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/103408719-15112980-4ba7-11eb-8161-5ec2c9fbcbb4.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;DSC Ewha에서 우리팀이 진행하던 프로젝트는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;수화 인식&lt;/code&gt;과 관련된 프로젝트였다.&lt;/p&gt;

&lt;p&gt;정확히는 “딥러닝 모델을 이용한 수화 교육 웹 어플리케이션”.&lt;/p&gt;

&lt;p&gt;쉽지 않은 프로젝트였기에 굉장히 다사다난했었다.&lt;/p&gt;

&lt;p&gt;딥러닝을 아직 자주 다루지 않은 팀원들이 많았기에(나 포함) 처음부터 이론 공부도 해보고, 수화 인식 관련 논문도 읽어보고, 관련 프로젝트들도 많이 조사했었다.&lt;/p&gt;

&lt;p&gt;프로젝트 초기에 이렇게 많이 고생했기 때문에 상대적으로 우리 팀은 중하반부터는 할 일이 없었다 :)&lt;/p&gt;

&lt;p&gt;팀원들이 열정, 끈기, 의지, 실력까지 있어서 덕분에 이 프로젝트를 수월하게(?) 완수할 수 있었던 것 같다.&lt;/p&gt;

&lt;p&gt;굉장히 다사다난한 일이 많았지만…^^ 그 이야기들은 서로의 가슴 속에 묻어두기로 하고 ㅋㅋㅋ&lt;/p&gt;

&lt;p&gt;우리만의 수화 인식 모델을 만드느라 가장 많은 시간을 쏟아 부었었다. 하지만 이덕분에 나는 Object Detection에 대한 공부와 관련 경험을 쌓을 수 있었다.&lt;/p&gt;

&lt;p&gt;YOLO 등 여러 모델들을 직접 돌려보기도 하고, 직접 만들어보기도 하였다. 결과적으로는 우리 팀의 ㅈㅇ언니가 만든 모델을 썼지만 😆&lt;/p&gt;

&lt;p&gt;아무튼 이 프로젝트로 인해 나는 &lt;strong&gt;좋은 사람들도 얻었고, 딥러닝에 대한 호감과 흥미, 자신감 등을 모두 얻을 수 있었다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;확실히 프로젝트를 한번 하고 나면 많은 성장을 이루는 것 같다. 내년의 나는 졸업 프로젝트를 통해 훨씬 더 성장할 수 있기를.&lt;/p&gt;

&lt;h3 id=&quot;2️⃣-데이터-분석-공부&quot;&gt;2️⃣ 데이터 분석 공부&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/103408999-23137a00-4ba8-11eb-888e-4532bc570e1e.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;데이터 분석 공부를 하게 된 계기는 좀 속물적이다.&lt;/p&gt;

&lt;p&gt;AI 리서쳐/엔지니어 직군을 모집할 때 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;우대사항&lt;/code&gt;에 캐글과 같은 데이터 사이언스 컴피티션과 관련된 수상 경험을 적어놨기 때문 ^^;;&lt;/p&gt;

&lt;p&gt;캐글같은 데이터 사이언스 대회 상금도 어마어마하고 말이다ㅋㅋㅋ.&lt;/p&gt;

&lt;p&gt;캐글러들의 추천을 받아 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;파이썬 머신러닝 완벽 가이드&lt;/code&gt; (부제: 공룡책)을 공부하기 시작했다.&lt;/p&gt;

&lt;p&gt;인프런에서 강의 들으면서 허겁지겁 진도를 뺐던 기억이 난다.&lt;/p&gt;

&lt;p&gt;이 때 코로나가 막 유행하기 시작인 때라 친구랑 공부나 하자는 생각으로 시작했었다.&lt;/p&gt;

&lt;p&gt;속물적으로 시작한 공부이지만, 나는 딥러닝을 포함한 인공지능에 관심이 많았기 때문에 실생활 데이터에 머신러닝 알고리즘을 적용시키는 것이 나에게 큰 흥미로 다가왔다.&lt;/p&gt;

&lt;p&gt;지금은 물론 데이터 분석이 이는 전부는 아니라는 것은 알고 있다.&lt;/p&gt;

&lt;p&gt;어쨌든 이 느낌? 이 생각 덕분에 공부를 수월하게 할 수 있었고, 캐글 필사 커리큘럼을 따라 가면서(완전 조금 따라갔다 ㅋㅋㅋ 그때가 개강할 때 쯤 되어서..) 나중에 꼭 캐글에 참가해서 상을 받고 싶다는 생각이 들었다.&lt;/p&gt;

&lt;p&gt;이는 좀 이따 서술할 HAICON에 입상할 수 있게 된 계기가 된다ㅎㅎ&lt;/p&gt;

&lt;h3 id=&quot;3️⃣-skt-ai-fellowship-2기-지원과-탈락&quot;&gt;3️⃣ SKT AI Fellowship 2기 지원과 탈락&lt;/h3&gt;

&lt;!-- ![image](https://user-images.githubusercontent.com/48315997/103409204-00359580-4ba9-11eb-8ed6-6fa981f29d59.png) --&gt;
&lt;p&gt;&lt;img width=&quot;550&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103409204-00359580-4ba9-11eb-8ed6-6fa981f29d59.png&quot; /&gt;
서류 합격 😆
&lt;!-- ![image](https://user-images.githubusercontent.com/48315997/103409209-088dd080-4ba9-11eb-8f5a-787e4b3e0068.png) --&gt;
&lt;img width=&quot;550&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103409209-088dd080-4ba9-11eb-8f5a-787e4b3e0068.png&quot; /&gt;
면접 탈락 😂&lt;/p&gt;

&lt;p&gt;======&lt;/p&gt;

&lt;p&gt;어쩌다 알게 된 SKT AI Fellowship.&lt;/p&gt;

&lt;p&gt;SKT에서 공대 대학(원)생에게 인공지능과 관련된 몇몇 프로젝트를 제시하고, 큰 규모로 지원해주는 프로그램이었다.&lt;/p&gt;

&lt;p&gt;수화인식 프로젝트를 같이한 팀원들에게 물어본 후, 같이 준비하기로 했다.&lt;/p&gt;

&lt;p&gt;주제는 내가 좋아하는(ㅋㅋㅋ) 리그오브레전드의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;하이라이트 자동 추출&lt;/code&gt; 연구 프로젝트였다.&lt;/p&gt;

&lt;p&gt;리그오브레전드라 딱 지칭하지는 않고, 스포츠라고 말했지만 내가 워낙 롤을 즐겨보기 때문에 이 주제로 어필을 강하게 했다.&lt;/p&gt;

&lt;p&gt;관련 연구 논문들, 다른 스포츠들(ex.야구)에 적용된 하이라이트AI 기술 등을 조사하여 우리도 아이디어를 냈다.&lt;/p&gt;

&lt;p&gt;이때는 정말 별 기대 안했는데, 면접까지 올라갔다! 면접은 각 주제당 최종 3팀(우리 주제는 그랬는데 확실하진 않다)이 올라가며, 이 중 1팀이 최종 팀으로 선정된다.&lt;/p&gt;

&lt;p&gt;생각지도 못한 결과에 우리는 빠르게 면접 대비를 했다. 스프레드시트에 예상 질문과 그에 대한 답변들을 모두 준비했다.&lt;/p&gt;

&lt;p&gt;또 프로젝트를 어떻게 진행할 것인지 발표해야 했는데 이는 내가 맡았었다.&lt;/p&gt;

&lt;p&gt;코로나로 인해 실제 면접은 Zoom으로 진행되었다. 면접관은 3명이었고 친절하고 편하게 해주셨다. 물론 기술적인 얘기에 대해서는 날카롭고 예리한 질문, 비판들을 해주셨다.&lt;/p&gt;

&lt;p&gt;사실 면접 분위기가 굉장히 좋았고, 우리가 질문에 대한 답변을 잘 했기에 기대를 하고 있던 상태였다ㅠㅠ&lt;/p&gt;

&lt;p&gt;하지만 아쉽게도 탈락 통보를 받았다.&lt;/p&gt;

&lt;p&gt;지금 와서 긍정적으로 생각해보자면, 우리가 그 기술을 정말 구현했을 수 있는 기술 수준이었는지는 모르겠다. 꽤 어려운 난이도를 가진 연구분야/기술이기 때문이다.
물론 진짜 됐다면 정말 열심히 공부하고 노력해서 완성하도록 만들었겠지만… 긍정 회로를 돌린다면 ㅋㅋㅋ 아직 내가 부족하다는 이야기니까…&lt;/p&gt;

&lt;p&gt;그래도 이 덕분에, 역시 내 팀원들이 정말 좋은 사람이자 우수한 사람들이었다는 것을 알 수 있었다.&lt;/p&gt;

&lt;p&gt;내가 아직 많이 부족해서 이렇게 된 결과라는 것을 인정하고, 앞으로 더 열심히 공부하고 노력해야겠다는 마음가짐을 다시금 가질 수 있었다.&lt;/p&gt;

&lt;h3 id=&quot;4️⃣-논문-스터디&quot;&gt;4️⃣ 논문 스터디&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/103409526-a209b200-4baa-11eb-9057-7f390de142a8.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;올해 가장 큰 수확 중 하나는 졸프 메이트를 찾은 것이다.&lt;/p&gt;

&lt;p&gt;과에 아는 사람들이 많지 않기도 해서 혼자 해야 하나 많이 걱정했는데 먼저 제안해준 것이 굉장히 고마웠다 ㅎㅎ&lt;/p&gt;

&lt;p&gt;비슷한 목표를 가지고 있고 서로 열정, 의지도 가득하기 때문에 좋은 결과를 낼 수 있으리라 확신한다.&lt;/p&gt;

&lt;p&gt;아무튼, 이 친구와 7월말부터 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;논문 읽기 스터디&lt;/code&gt;를 시작했다.&lt;/p&gt;

&lt;p&gt;처음 시작할 때는 논문이라는 것이 굉장히 부담스럽게 다가왔는데, 이제는 아~주 조금은 익숙해져서 크게 거부감이나 부담감은 들지 않아졌다.&lt;/p&gt;

&lt;p&gt;이것만 해도 굉장한 발전 아닌가? ㅋㅋㅋ&lt;/p&gt;

&lt;p&gt;이 논문 스터디를 통해 내가 얻게 된 것이 있다면, ‘내가 이 분야를 오랫동안 좋아할 수 있겠구나’라는 생각이었다.&lt;/p&gt;

&lt;p&gt;공부가 지겹지가 않고, 내가 해야 할 것이 산더미라는 것을 알지만 그것이 두렵거나 불안하지 않았다.&lt;/p&gt;

&lt;p&gt;그래도 나는 아직 많이 부족하다! 하루에 논문 한편씩 매일마다 뿌셔도 아직 모자라다.&lt;/p&gt;

&lt;p&gt;지금은 우리 둘 다 Domain Adaptation, GAN 쪽 연구에 관심을 가지게 되어 그 쪽 논문들을 읽어보고 구현하고 있다. (아직 많이는 못했지만 ㅋㅋ)&lt;/p&gt;

&lt;h3 id=&quot;5️⃣-haicon-2020-입선&quot;&gt;5️⃣ HAICON 2020 입선&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/103409708-5c011e00-4bab-11eb-9f49-86fcfd219ff2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아까 올해 초에 데이터분석 공부를 했다고 했는데, 이 때 같이 한 친구와 대회를 나가기로 했다.&lt;/p&gt;

&lt;p&gt;국내 데이터 사이언스 컴피티션을 모아놓은 플랫폼인 데이콘에서 현재 진행 중인 대회에서 가장 많은 상금을 뿌리는 대회를 골랐다 ㅋㅋㅋ.&lt;/p&gt;

&lt;p&gt;금액도 그렇지만, 국가정보원과 국가보안기술연구소가 진행한다는 점에서도 큰 흥미?를 끌었다.&lt;/p&gt;

&lt;p&gt;첫 참가하는 대회이니 만큼 정말 많은 시도들을 해보았다. 사실 1등까지는 크게 바라지는 않았지만 수상 욕심은 분명히 있었다.&lt;/p&gt;

&lt;p&gt;대회 초에는 수상권에 있었지만 중후반부로 들면서 고수들이 많이 참가했는지 조금씩 조금씩 밀려났고, 결과적으로 대회 마지막 즈음엔 수상권에서 꽤 멀어졌었다ㅠㅠ (Public score 기준)&lt;/p&gt;

&lt;p&gt;근데 이게 무슨 일인지, 전체 테스트 셋으로 최종 결과를 돌린 Private score에서 12위가 나온 것이다! 10위까지가 수상권이라 아 이거 진짜 아깝다, 조금만 더 노력할 걸 찰나 후회하긴 했다.&lt;/p&gt;

&lt;p&gt;그래도 나랑 내 친구는 정말 최대한의 노력을 했고, 정말 많은 실험을 거쳤기 때문에 큰 상승은 없었을지도 모른다. (처음이다 보니 양으로 승부했었다)&lt;/p&gt;

&lt;p&gt;그래도 조금 기대를 품고, 보고서와 최종 제출본을 제출한 결과 내 위에 2명이 규칙 위반 혹은 뭐가 부족했는지 우리가 턱걸이로 입선을 하게 된 것이다!! 진짜 기뻤다 이때 ㅎㅎ&lt;/p&gt;

&lt;p&gt;후기를 자세히 풀고 싶지만 외부에 발설하지 말라는 사항이 많았어서 이에 대한 후기는 이게 끝!! 정말 멋진 사람들도 많이 봤고, 학부생은 나랑 한 팀밖에 없었던 것 같은데 굉장히 영광이었다.&lt;/p&gt;

&lt;p&gt;나중에는 될 수 있으면 캐글 대회에 입상하고 싶다 ㅎㅎ&lt;/p&gt;

&lt;h3 id=&quot;6️⃣-12학기-모두-4점대로-마무리&quot;&gt;6️⃣ 1,2학기 모두 4점대로 마무리&lt;/h3&gt;

&lt;p&gt;올해 1학기는 정말 힘들었다. 꽤 빡센 과목들을 듣기도 했고, 코로나가 터지고 첫 비대면 수업이라 이리저리 정신없었던 것 같다.&lt;/p&gt;

&lt;p&gt;때문에 1학기에는 정말 학기 공부 외에는 다른 공부를 할 수가 없었다 ㅠㅠ&lt;/p&gt;

&lt;p&gt;다른 학교나 타과들은 학점 잘 뿌려주던데 ㅎㅎ… 내가 들은 수업들은 다 코로나 이전이랑 비슷하거나 오히려 더 짜기도 ^^..&lt;/p&gt;

&lt;p&gt;1학기는 6전공 1교양 21학점 이수, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4.09/4.3&lt;/code&gt;이었고&lt;/p&gt;

&lt;p&gt;2학기는 6전공 1교양 19학점 이수, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4.2/4.3&lt;/code&gt;이었다.&lt;/p&gt;

&lt;p&gt;2학기에 들은 교양수업은 2017년에 들었던 과목 하나를 재수강한 것이라 1-2등 점수였을텐데 재수강 최대 학점인 A-를 받아서 개인적으로 아쉬웠다.
그리고 2학기는 거의 모든 과목을 1등, 2등 아니면 못해도 5등 정도 했기 때문에 개인적으로 맘에 드는 학기였다.&lt;/p&gt;

&lt;p&gt;사실 2학기에 들은 강의들이 쉽다고 느꼈을 때가 종종 있었다. 내가 올해 많은 성장을 했다는 게 느껴져서 뿌듯했었다.&lt;/p&gt;

&lt;p&gt;학기는 아직 꽤 남았지만, 남은 이수 학점은 그렇게 많지 않다. 남은 학점들도 모두 A+을 받아서 최우등졸업을 할 수 있게 되면 좋겠다!&lt;/p&gt;

&lt;h3 id=&quot;7️⃣-2021년-목표&quot;&gt;7️⃣ 2021년 목표&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://images.unsplash.com/photo-1546074177-31bfa593f731?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&amp;amp;ixlib=rb-1.2.1&amp;amp;auto=format&amp;amp;fit=crop&amp;amp;w=1267&amp;amp;q=80&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;올 한 해를 되돌아보니 정말 많은 일이 있었다.&lt;/p&gt;

&lt;p&gt;코로나때문에 사람들을 많이 못 만났지만, 이게 나에게 긍정적으로 작용될 수 있었던 점은 분명히 있었다.&lt;/p&gt;

&lt;p&gt;혼자 공부할 시간이 늘어나 전보다 꽤 많이 성장했다는 점이 바로 그것이다.&lt;/p&gt;

&lt;p&gt;12월 초에 좋은 소식으로는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ETRI&lt;/code&gt; (한국전자통신연구원) 동계 연구연수생에 합격했다는 것이다!&lt;/p&gt;

&lt;p&gt;다음주 1/2일부터 2/26까지 대전에서 일을 하게 되었는데 굉장히 설렌다.&lt;/p&gt;

&lt;p&gt;인턴급이기 때문에 많은 일을 시키지 않을 것 같지만, 나 스스로 많이 배우려 노력하고 또 한 단계 성장하는 계기가 되었으면 좋겠다.&lt;/p&gt;

&lt;p&gt;일단 2021년 목표는 크게 2가지이다.&lt;/p&gt;

&lt;h4 id=&quot;졸업-프로젝트-성공적--해외-탑-컨퍼런스에-논문-accept-되기&quot;&gt;졸업 프로젝트 성공적 == 해외 탑 컨퍼런스에 논문 Accept 되기&lt;/h4&gt;

&lt;p&gt;나에게 있어 2021년은 가장 중요한 한 해이다.
사실상 이를 위해 지금까지 준비를 했다고 봐도 과언이 아니다. 졸프 시작을 한 학기 미룬 것도 이를 위해 공부하기 위해서였다.&lt;/p&gt;

&lt;p&gt;마침 좋은 메이트가 있고, 나와 비슷하게 야망도 크기에 우리 목표는 상당히 높게 잡았다.&lt;/p&gt;

&lt;p&gt;사실 저것보다 크다. Best Student Paper 상을 받고 싶다!!!!!&lt;/p&gt;

&lt;p&gt;누구보다 열심히 공부하고, 누구보다 잘 해야 된다는 것을 안다. 
나는 무조건 후회없는 1년을 보낼 것이다.&lt;/p&gt;

&lt;p&gt;꼭 이 목표가 성공할 수 있도록, 최선을 다하겠다.&lt;/p&gt;

&lt;h4 id=&quot;all-a&quot;&gt;All A+&lt;/h4&gt;

&lt;p&gt;앞선 목표에 비하면 소소하다.
졸프 때문에 학점은 15~16 학점 정도? 많지 않게, 부담이 되지 않는 선까지만 들을 것이다.&lt;/p&gt;

&lt;p&gt;대신에 무조건 올에이쁠!&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="회고" /><category term="Year Review" /><summary type="html">2020년 저에게 있었던 일을 짧게 회고하는 글입니다.</summary></entry><entry><title type="html">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</title><link href="http://localhost:4000/paper%20review/2020/12/30/CycleGAN/" rel="alternate" type="text/html" title="Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks" /><published>2020-12-30T00:00:00+09:00</published><updated>2020-12-30T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/12/30/CycleGAN</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/12/30/CycleGAN/">&lt;p&gt;Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (CycleGAN) 을 읽고 개인적으로 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;앞서 살펴본 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DCGAN&lt;/code&gt;연구는 큰 발전을 이루었지만, 이 역시 unstable하여 mode collapse가 일어날 수 있는 한계점이 뚜렷했습니다.&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="GAN" /><summary type="html">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (CycleGAN) 을 읽고 개인적으로 정리한 글입니다.</summary></entry><entry><title type="html">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)</title><link href="http://localhost:4000/paper%20review/2020/12/25/DCGAN/" rel="alternate" type="text/html" title="Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)" /><published>2020-12-25T00:00:00+09:00</published><updated>2020-12-25T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/12/25/DCGAN</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/12/25/DCGAN/">&lt;p&gt;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)을 읽고 개인적으로 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;unsupervised-representation-learning-with-deep-convolutional-generative-adversarial-networks-dcgan&quot;&gt;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;GAN은 Representation Learning에 효과적이라고 합니다. (본 논문에서는 이것이 GAN의 learning process와 관련이 있고, lack of heuristic cost function 덕분이라고 말합니다.)&lt;/p&gt;

&lt;p&gt;그럼에도 불구하고 처음 나온 GAN은 학습하는 것에 있어 매우 &lt;strong&gt;unstable&lt;/strong&gt;하다는 문제점을 가지고 있었습니다.&lt;/p&gt;

&lt;p&gt;본 논문의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DCGAN&lt;/code&gt;은 해당 문제점 해결을 포함하여 총 4가지의 Contributions로 정리할 수 있습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;DCGAN은 stable training이 가능하다&lt;/li&gt;
  &lt;li&gt;학습된 Discriminator는 image classification 태스크에 사용이 가능하다. (다른 unsupervised 알고리즘들과 비교할 것임.)&lt;/li&gt;
  &lt;li&gt;GAN이 학습한 filters를 시각화할 수 있고, 특정 오브젝트에 대해 특정 filter를 학습했다는 점을 보여줄 수 있다.&lt;/li&gt;
  &lt;li&gt;Generator는 vector arithmetic properties를 가지고 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;approach-and-model-architecture&quot;&gt;APPROACH AND MODEL ARCHITECTURE&lt;/h2&gt;

&lt;p&gt;&lt;img width=&quot;705&quot; alt=&quot;스크린샷 2020-12-25 오후 5 31 03&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103127996-f73f5280-46d6-11eb-9936-996a9ae0e84f.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우선 DCGAN이 기존의 GAN과 architecture 측면에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;어떻게&lt;/code&gt; 달랐기에 큰 성과를 이룰 수 있었는지 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;DCGAN이 나오기 전까지에도 CNN을 이용한 GAN을 만드는 시도는 계속 있었습니다. 하지만 이 시도들은 모두 성공적이지 못했죠.&lt;/p&gt;

&lt;p&gt;DCGAN은 &lt;strong&gt;CNN Architecture에서의 최신 변화(?) 3가지를 적용&lt;/strong&gt;하여 성공하였습니다.&lt;/p&gt;

&lt;h3 id=&quot;all-convolutional-net-사용&quot;&gt;All Convolutional Net 사용&lt;/h3&gt;

&lt;p&gt;우선 가장 큰 변화점은 &lt;a href=&quot;https://arxiv.org/abs/1412.6806&quot;&gt;Striving for Simplicity: The All Convolutional Net&lt;/a&gt; 을 참고하여 All Convoultional Net을 사용했다는 점입니다.&lt;/p&gt;

&lt;p&gt;All conv. net은 &lt;strong&gt;pooling functions(예를 들어, max pooling)를 strided conv. 으로 바꾼 네트워크 구조&lt;/strong&gt;입니다. Max pooling은 미분되지 않는 성질을 가진다고 합니다.&lt;/p&gt;

&lt;p&gt;이 네트워크 구조를 통해 Generator와 Discriminator 모두 자신들의 spatial downsampling을 학습하기에 적합해집니다.&lt;/p&gt;

&lt;h3 id=&quot;eliminating-fully-connected-layersfc-layers-on-top-of-convolutional-features&quot;&gt;Eliminating fully connected layers(FC layers) on top of convolutional features&lt;/h3&gt;

&lt;p&gt;이 시점 trend는 마지막에 FC layer를 제거하고, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;global average pooling&lt;/code&gt;를 쓰는 방식이었다고 합니다.&lt;/p&gt;

&lt;p&gt;다만, 이 global average pooling은 &lt;strong&gt;모델의 안정성은 올리는 반면에 convergence speed는 떨어뜨리는&lt;/strong&gt; trade-off 관계를 가지고 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;batch-norm--relu-activation&quot;&gt;Batch Norm &amp;amp; ReLU activation&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Batch Normalization&lt;/strong&gt;는 학습을 안정화시킬 수 있는 방법 중 하나입니다. (normalizing the input to each unit to have zero mean and unit variance)&lt;/p&gt;

&lt;p&gt;이것은 학습 문제점 중 하나인 initialization과 deep model의 gradient flow에 큰 도움을 줄 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;하지만, 모든 layer에 BN을 적용시키는 것은 아닙니다.&lt;/strong&gt; Generator의 output layer와 Discriminator의 input layer에는 BN을 적용시키지 않습니다.&lt;/p&gt;

&lt;p&gt;마지막으로 &lt;strong&gt;ReLU를 사용했다&lt;/strong&gt;는 점이 언급되어 있는데, Generator와 Discriminator에 적용되는 function이 약간 다릅니다.&lt;/p&gt;

&lt;p&gt;Generator에는 기본적으로 ReLU를 사용하지만, output layer에는 따로 ReLU가 아닌 Tanh function을 적용시킵니다.&lt;/p&gt;

&lt;p&gt;Discriminator에는 Leaky ReLU를 적용합니다.&lt;/p&gt;

&lt;p&gt;(여러 실험 결과 이 functions들이 좋은 성능을 냈기 때문이겠죠? 확실한 이유, 논증은 모르겠습니다.)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;details-of-adversarial-training&quot;&gt;DETAILS OF ADVERSARIAL TRAINING&lt;/h2&gt;

&lt;p&gt;DCGAN은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LSUN, FACES, IMAGENET-1K&lt;/code&gt; 데이터셋에 대하여 학습하였습니다.&lt;/p&gt;

&lt;p&gt;training setting(parameters, …)에 대해서는 논문을 참고해주시고, 위 데이터셋 중 LSUN에 대해서만 알아보겠습니다 :)&lt;/p&gt;

&lt;h3 id=&quot;lsun&quot;&gt;LSUN&lt;/h3&gt;

&lt;p&gt;LSUN은 &lt;em&gt;Large-scale Scene Understanding&lt;/em&gt;의 줄임말로, bedroom 사진들을 모은 데이터 셋입니다.&lt;/p&gt;

&lt;p&gt;이 데이터셋을 가지고 학습시킨 모델로 생성된 이미지는 quality가 상당히 향상되었습니다.&lt;/p&gt;

&lt;p&gt;하지만 이것이 &lt;strong&gt;over-fitting이 되어 이렇게 된 것인지, 학습 데이터셋에 대하여 기억(memorization)하여 만들어진 것인지&lt;/strong&gt;에 대해 판별해봐야 합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;overfitting?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;700&quot; alt=&quot;스크린샷 2020-12-25 오후 5 45 38&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103128506-00c9ba00-46d9-11eb-8b7a-e144208bbfe8.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;본 논문의 Fig.2 와 Fig.3을 통해 모델이 오히려 &lt;strong&gt;underfitting&lt;/strong&gt; 되어있음을 얘기합니다.&lt;/p&gt;

&lt;p&gt;underfitting이 이루어졌다고 여기는 이유는 아직 noise texture가 눈에 보이기 때문이라고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(엄청 가까이 보지 않는 이상은 잘 느끼지 못하겠는데 말이죠.)&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;memorization?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;사실 이게 가장 중요한 부분이라고 볼 수 있습니다. 새로 generate 하는 image가 사실 학습 데이터에서 기억(memorize)하여 만들어진 것이라면, &lt;strong&gt;진정한 의미의 Generate가 아니니깐요.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이에 대해서 본 논문의 저자들은 memorize하는 가능성을 줄이기 위해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;image de-duplication process&lt;/code&gt;(중복 제거 프로세스)를 거칩니다.&lt;/p&gt;

&lt;p&gt;de-duplication을 하기 위해 autoencoder를 하나 만듭니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We fit a 3072-128-3072 de-noising dropout regularized RELU autoencoder on 32x32 downsampled center-crops of training examples.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;empirical-validation-of-dcgans-capabilities&quot;&gt;EMPIRICAL VALIDATION OF DCGANs CAPABILITIES&lt;/h2&gt;

&lt;p&gt;맨 처음에 이 논문의 contributions 중 하나로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;학습된 Discriminator는 image classification 태스크에 사용이 가능하다&lt;/code&gt; 라고 얘기했었죠?&lt;/p&gt;

&lt;p&gt;이 목차에서는 정말 DCGAN이 &lt;strong&gt;feature extractor&lt;/strong&gt;로써의 역할이 가능한가, 그래서 CIFAR-10 데이터셋에 대해서도 &lt;strong&gt;classification task&lt;/strong&gt;를 잘 수행하는가를 확인해봅니다.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;696&quot; alt=&quot;스크린샷 2020-12-25 오후 6 03 36&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103129277-8484a600-46db-11eb-9e0f-de26d03694d4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결론적으로, 다른 unsupervised 알고리즘은 K-means model들보다 우수한 성능을 보였습니다. (Exemplar CNN 모델보다는 좀 못 미치지만요.)&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;542&quot; alt=&quot;스크린샷 2020-12-25 오후 6 05 55&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103129364-d6c5c700-46db-11eb-8f7b-388b5ae3128b.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;추가적으로 SVHN Digits 데이터셋에 대해서도 실험을 해보았습니다. test error 측면에서 SOTA를 달성하는 쾌거를 이루었습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;investigating-and-visualizing-the-internals-of-the-networks&quot;&gt;INVESTIGATING AND VISUALIZING THE INTERNALS Of THE NETWORKS&lt;/h2&gt;

&lt;p&gt;또 다른 contributions으로  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GAN이 학습한 filters를 시각화할 수 있고, 특정 오브젝트에 대해 특정 filter를 학습했다는 점을 보여줄 수 있다&lt;/code&gt; 와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Generator는 vector arithmetic properties를 가지고 있다&lt;/code&gt; 가 있었습니다.&lt;/p&gt;

&lt;p&gt;이 목차에서는 두 부분에 대해 설명할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;walking-in-the-latent-space&quot;&gt;Walking in the Latent Space&lt;/h3&gt;

&lt;p&gt;latent space를 변경했을 때 sharp transitions(급작스러운 변화)가 있으면 이는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memorization&lt;/code&gt;이 일어났다는 신호일 수 있습니다.&lt;/p&gt;

&lt;p&gt;반대로 부드러운 변화가 일어나면 memorization이 된 것이 아니라 제대로 학습이 되었다고 볼 수 있죠.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/103134579-83607280-46f5-11eb-800b-b4d5ea21bdad.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 사진을 보시면 DCGAN의 경우 sharp transition이 아닌 smooth한 변화가 이루어졌음을 볼 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;visualizing-the-discriminator-features&quot;&gt;Visualizing the Discriminator Features&lt;/h3&gt;

&lt;p&gt;이 내용에서는 Guided backpropagation을 통해 GAN이 학습한 filters를 시각화할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;696&quot; alt=&quot;스크린샷 2020-12-25 오후 9 17 38&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103134734-9f184880-46f6-11eb-9639-d0eb9878e74a.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;discriminator가 feature들을 학습해서, 특정 파트(bed, windows,…)들에 대하여 active 하고 있음을 볼 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;manipulating-the-generator-representation&quot;&gt;Manipulating the Generator Representation&lt;/h3&gt;

&lt;h4 id=&quot;forgetting-to-draw-certain-objects&quot;&gt;Forgetting to Draw Certain Objects&lt;/h4&gt;

&lt;p&gt;이건 매우 재미있는 실험입니다.&lt;/p&gt;

&lt;p&gt;간단하게 요약하자면, Generator가 무슨 representation을 학습했는지 알아보기 위하여 특정 filter(여기서는 window filter)를 삭제해봅니다.&lt;/p&gt;

&lt;p&gt;즉, Window라는 object를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Forget&lt;/code&gt; 하게 되는 것이죠.(기술적으로는 window filter를 dropout 시킨다고 말할 수 있습니다.)&lt;/p&gt;

&lt;p&gt;결과적으로 이 실험에서는 창문이 아닌 다른 representations, objects가 들어가게 됩니다!&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;684&quot; alt=&quot;스크린샷 2020-12-25 오후 9 22 03&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103134811-3da4a980-46f7-11eb-8d95-b990a0a18601.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 결과를 보면 아시다시피, 창문이었던 것이 문으로 바뀌는 등 다른 object를 생성합니다.&lt;/p&gt;

&lt;h4 id=&quot;vector-arithmetic-on-face-samples&quot;&gt;Vector Arithmetic On Face Samples&lt;/h4&gt;

&lt;p&gt;많은 분들이 가장 재밌어하실(?) 부분인 것 같습니다.&lt;/p&gt;

&lt;p&gt;word embedding 관련해서 vector(“King”) - vector(“Man”) + vector(“Woman”)가 vector(“Queen”)의 결과가 나오듯이, DCGAN에서도 이와 비슷한 &lt;strong&gt;arithmetic한 연산이 가능하다&lt;/strong&gt;고 밝혔습니다.&lt;/p&gt;

&lt;p&gt;Generator의 input인 Z vector에 대한 arithmetic operation을 하는데, single sample로는 불안정하여 3개의 Z vector를 평균한 값을 사용한다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;580&quot; alt=&quot;스크린샷 2020-12-25 오후 9 36 16&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103135086-3a122200-46f9-11eb-931d-7d737e8c5ed1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;smiling woman - neutral woman + neutral man = smiling man 이미지가 만들어지는 마법같은 기술을 보실 수 있습니다.&lt;/p&gt;

&lt;p&gt;앞서 말했다시피 3개의 Z vector를 average하여 새로운 Y벡터를 만든 것도 확인해볼 수 있죠.&lt;/p&gt;

&lt;p&gt;이게 다가 아닙니다. &lt;strong&gt;face pose&lt;/strong&gt; 또한 Z space에 선형적으로 모델링할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;557&quot; alt=&quot;스크린샷 2020-12-25 오후 9 38 01&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103135120-780f4600-46f9-11eb-8d7e-8d723c10e439.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;바로 이렇게 말이죠. 
이미 이전부터 scale, rotation, position에 대하여 conditional generative model은 학습할 수 있다고 연구되어왔습니다. 하지만 이 연구는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;purely unsupervised model&lt;/code&gt; 이라는 점에서 큰 변환점이 된 것이죠.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;FUTURE WORK&lt;/h2&gt;

&lt;p&gt;사실 stablity를 완전히 해결한 것은 아닙니다.&lt;/p&gt;

&lt;p&gt;DCGAN을 오랫동안 학습하게 되면 collapse mode, oscillating mode가 발생할 수 있습니다. 아직도 &lt;strong&gt;불안정성&lt;/strong&gt;이 남은 것이죠.&lt;/p&gt;

&lt;p&gt;그래서 이 논문에서는 해당 문제점을 Future work로 남기고 마무리하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://jaejunyoo.blogspot.com/2017/02/deep-convolutional-gan-dcgan-1.html_&quot;&gt;Jaejun Yoo’s Playground&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;DCGAN도 읽었으니, 구현도 해보고 후속 논문들도 찬찬히 읽어보려고 합니다.
긴 글 읽어주셔서 감사합니다 :)&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="GAN" /><summary type="html">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)을 읽고 개인적으로 정리한 글입니다.</summary></entry></feed>