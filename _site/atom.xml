<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-09-20T20:30:57+09:00</updated><id>http://localhost:4000/atom.xml</id><title type="html">Yeonsoo Kim’s blog</title><author><name>Yeonsoo Kim</name></author><entry><title type="html">StyleGAN:A Style-Based Generator Architecture for Generative Adversarial Networks</title><link href="http://localhost:4000/paper%20review/2020/09/20/StyleGAN/" rel="alternate" type="text/html" title="StyleGAN:A Style-Based Generator Architecture for Generative Adversarial Networks" /><published>2020-09-20T00:00:00+09:00</published><updated>2020-09-20T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/09/20/StyleGAN</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/09/20/StyleGAN/">&lt;p&gt;StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks 를 읽고 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;stylegan--a-style-based-generator-architecture-for-generative-adversarial-networks-cvpr-2019&quot;&gt;StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks (CVPR 2019)&lt;/h1&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;GAN의 generator 부분은 black box로 여겨져 이미지 생성 과정을 이해하기 어려웠음.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;style transfer&lt;/code&gt; 에서 기반한 generator 구조
    &lt;ul&gt;
      &lt;li&gt;각 레이어마다 style의 정보를 입힘. -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AdaIN&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;전체적인 스타일(머리 색, 인종, 성별 등), 세세한 부분(곱슬 등) 등까지 조정 가능 -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;noise&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;baseline : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;progressive GAN&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;latent vector로 부터 이미지 합성하고 점점 해상도를 올려서 high-resolution image 생성 =&amp;gt; scale-specific control&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;loss function, discriminator 등 수정하지 않고 오직 제너레이터에 대해서만 다룸.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;latent space의 interpolation quality 측정하는 measure 제안
    &lt;ul&gt;
      &lt;li&gt;perceptual path length&lt;/li&gt;
      &lt;li&gt;linear separability&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;FFHQ 데이터셋 오픈&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-work-basic-concepts&quot;&gt;Related work (Basic concepts)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Progressive GAN
    &lt;ul&gt;
      &lt;li&gt;GAN을 저해상도에서 고해상도로 점진적으로 학습&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;style transfer
    &lt;ul&gt;
      &lt;li&gt;content image &amp;amp; style image가 있을 때 content 이미지와 유사하게 style image에 입히는 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;
&lt;p&gt;(Explain one of the methods that the thesis used.)&lt;/p&gt;

&lt;h3 id=&quot;generator-architecture&quot;&gt;Generator Architecture&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93662640-90d5e300-fa9c-11ea-9a36-7312a058879d.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;left : traditional generaotr : latent code &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;z&lt;/code&gt;를 input layer에 바로 넣음.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;right : &lt;strong&gt;style-based generator&lt;/strong&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;first, map the input to an intermediate latent space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;then controls the generator through &lt;strong&gt;adaptive instance normalization (AdaIN)&lt;/strong&gt; at each conv. layer.&lt;/li&gt;
      &lt;li&gt;Gaussian noise is added after each conv.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;제안한 모델을 차근차근 뜯어보자면&lt;/p&gt;

&lt;h4 id=&quot;mapping-network&quot;&gt;Mapping Network&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/1400/0*6lEwRXKiA8WGRlEc.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;https://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;input vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;z&lt;/code&gt;를 바로 input layer에 넣는 것이 아니라, mapping network를 거쳐 &lt;strong&gt;intermediate vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w&lt;/code&gt;&lt;/strong&gt; 로 변환한 후 이미지를 생성한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;바로 인풋 레이어에 넣지 않는 이유 : 고정된 input distribution에 맞춰야 해서 non-linear하게 mapping이 되고, 이것은 머리 색등과 같은 attribute를 변경하기 힘들어지기 때문.&lt;/li&gt;
  &lt;li&gt;위처럼 intermediate vector를 사용하게 되면 유동적인 공간에 mapping 시킬 수 있기 때문에 visual attribute 조절이 쉬워진다. =&amp;gt; &lt;strong&gt;disentanglement&lt;/strong&gt; 하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;즉, 이 네트워크에서는 z로부터 만들어진 style &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w&lt;/code&gt;를 구하고, 이를 affine transformation을 거친 A를 synthesis network에 넘겨주어 AdaIN operation을 통해 레이어에 스타일을 입힌다.&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;style-modules-adain&quot;&gt;Style Modules (AdaIN)&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/1400/0*uqn4slMHrFYkFmjS.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;https://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93665356-c4bb0380-fab0-11ea-844d-348f19fc4e2f.png&quot; alt=&quot;스크린샷 2020-09-19 오후 7 46 09&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;위에서 생성된 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w&lt;/code&gt;는 style에 대한 정보를 가지고 있다.&lt;/li&gt;
  &lt;li&gt;Synthesis network는 학습가능한 constant tensor(4x4x512)를 &lt;strong&gt;upsampling, convolution&lt;/strong&gt;을 통해 1024x1024x3 이미지로 변환시킨다.&lt;/li&gt;
  &lt;li&gt;w의 affine transfomation을 통해 얻어진 A를 가지고 &lt;strong&gt;AdaIN operation&lt;/strong&gt;을 통해 스타일을 입힌다.
    &lt;ul&gt;
      &lt;li&gt;normalize하고, 이를 scale하고 bias를 더함. 이게 스타일을 입히는 효과를 낸다.&lt;/li&gt;
      &lt;li&gt;매 conv 레이어마다 하므로, 각각의 레이어마다 다른 스타일을 조정할 수 있다. 이 말은 곧, 각 레이어가 특정한 attribute만을 담당한다는 뜻.
        &lt;ul&gt;
          &lt;li&gt;세밀한 스타일 조정 가능해진다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;stochastic-variation&quot;&gt;Stochastic variation&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://bloglunit.files.wordpress.com/2019/02/1_gwchaliormc1xlj7bh0zmg.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/1400/1*GwchALioRMC1xlj7Bh0ZMg.png&quot; alt=&quot;스크린샷 2020-09-19 오후 7 48 31&quot; /&gt;&lt;/p&gt;

&lt;p&gt;머리카락, 수염 등 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stochastic&lt;/code&gt;한 요소들은 사진의 디테일에 매우 중요함.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;위의 architecture에서 noise가 이에 대한 역할을 한다.&lt;/li&gt;
  &lt;li&gt;synthesis network에서 &lt;strong&gt;by adding per-pixel noise after each convolution.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;style-mixing&quot;&gt;Style Mixing&lt;/h3&gt;

&lt;p&gt;two random latent codes(w1,w2)를 사용하는 regularization 기법&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;하나의 w로 학습할 경우 여러 레이어에 대한 style이 correlate되는 문제점이 생길 수 있음.&lt;/li&gt;
  &lt;li&gt;ex. w1 스타일로 입혀놓지만, 랜덤으로 몇 개는 w2 스타일을 사용한다 …&lt;/li&gt;
  &lt;li&gt;위와 같은 방법을 통해 각 레이어가 담당하는 스타일을 명확하게 구분지을 수 있다.&lt;/li&gt;
  &lt;li&gt;(dropout과 비슷한 원리라고 함)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;disentanglement-studies&quot;&gt;Disentanglement studies&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;이 내용이 어려워서 제대로 이해하지 못함. 짧게 요약하겠음.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93665682-7c511500-fab3-11ea-8333-be0512370654.png&quot; alt=&quot;스크린샷 2020-09-19 오후 8 05 35&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Disentanglment : latent space가 선형적인 구조를 가지게 되어, 하나의 factor가 움직였을 때 정해진 특성이 바뀌게 만드는 것.
    &lt;ul&gt;
      &lt;li&gt;예. z의 특정한 값을 바꿨을 때 생성되는 이미지의 하나의 특성(성별, 머리카락 길이 등)만 영향을 주게 되는 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;fixed distribution을 따르게 되면 억지로 끼워맞추게 되어 어색한 이미지가 만들어질 수 있음.&lt;/li&gt;
  &lt;li&gt;하지만 이 모델처럼 &lt;strong&gt;비선형 mapping function&lt;/strong&gt;을 가지게 될 경우, 고정된 분포를 따를 필요가 없음.
    &lt;ul&gt;
      &lt;li&gt;위 그림에서 (c)와 같은 형태가 됨. 어느정도 a와 생김새가 비슷하면서 자연스럽게 맞출 수 있게 된 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;A major beneﬁt of our generator architecture is that the intermediate latent space W does not have to support sam-pling according to any ﬁxed distribution; its sampling density is induced by the learned piecewise continuous mapping f(z).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;본 논문에서는 disentanglement를 학습할 수 있는 두 가지 평가 지표를 제안함.
    &lt;ul&gt;
      &lt;li&gt;Perceptual path length&lt;/li&gt;
      &lt;li&gt;Linear seperability&lt;/li&gt;
      &lt;li&gt;위의 내용을 자세히 알고 싶다면 이 곳을 참조
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://jayhey.github.io/deep%20learning/2019/01/16/style_based_GAN_2/#perceptual-path-length&quot;&gt;링크&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;our investigations to &lt;strong&gt;the separation of high-level attributes and stochastic effects&lt;/strong&gt;, as well as &lt;strong&gt;the linearity of the intermediate latent space&lt;/strong&gt; will prove fruitful in improving the understanding and controllability of GAN synthesis.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;appendix-truncation-trick-in-w&quot;&gt;Appendix. Truncation trick in W&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;트레이닝 중에 하는 게 아니고, generator가 만든 것 중에 더 나은 latent space 을 뽑는 법에 대한 trick&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;학습이 완료된 네트워크의 input을 제어하는 방법&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93665586-81fa2b00-fab2-11ea-858c-69a2f3ea5026.png&quot; alt=&quot;스크린샷 2020-09-19 오후 7 58 35&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 수식을 통한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w'&lt;/code&gt; vector를 뽑는다.&lt;/p&gt;

&lt;h2 id=&quot;additional-studies&quot;&gt;Additional studies&lt;/h2&gt;
&lt;p&gt;(If you have some parts that cannot understand, you have to do additional studies for them. It’s optional.)&lt;/p&gt;

&lt;p&gt;disentanglement에 대한 명확한 이해가 필요함.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;(References for your additional studies)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://www.youtube.com/watch?v=TWzEbMrH59o&amp;amp;feature=youtu.be&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431&lt;/li&gt;
  &lt;li&gt;https://jayhey.github.io/deep%20learning/2019/01/16/style_based_GAN_2/&lt;/li&gt;
  &lt;li&gt;https://blog.lunit.io/2019/02/25/a-style-based-generator-architecture-for-generative-adversarial-networks/&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Computer Vision" /><category term="GAN" /><category term="Generative Model" /><summary type="html">StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks 를 읽고 정리한 글입니다.</summary></entry><entry><title type="html">HarDNet:A Low Memory Traffic network</title><link href="http://localhost:4000/paper%20review/2020/09/11/HarDNet/" rel="alternate" type="text/html" title="HarDNet:A Low Memory Traffic network" /><published>2020-09-11T00:00:00+09:00</published><updated>2020-09-11T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/09/11/HarDNet</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/09/11/HarDNet/">&lt;p&gt;HarDNet : A Low Memory Traffic network 를 읽고 개인적으로 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;hardnet--a-low-memory-traffic-network-iccv-2019&quot;&gt;HarDNet : A Low Memory Traffic network (ICCV 2019)&lt;/h1&gt;

&lt;h2 id=&quot;key-idea&quot;&gt;Key Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;기존의 metrics들에서의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inference time&lt;/code&gt; 측정은 부정확하다.
    &lt;ul&gt;
      &lt;li&gt;새로운 metric =&amp;gt; &lt;strong&gt;memory traffic for accessing intermediate feature maps 측정&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;inference latency 측정에 유용할 것, especially in such tasks as &lt;em&gt;real-time object detection and semantic segmentation of high-resolution video.&lt;/em&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CIO&lt;/code&gt; : approximation of DRAM traffic이 될 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;computation, energy efficiency를 위해서는 fewer MACs, less DRAM이 좋은 것임&lt;/strong&gt; -&amp;gt; 연구 방향&lt;/li&gt;
  &lt;li&gt;각각의 레이어의 MoC에 soft constraint를 적용했음.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;low CIO network model&lt;/strong&gt; with a reasonable increase of MACs를 위해&lt;/li&gt;
      &lt;li&gt;방법 -&amp;gt; &lt;strong&gt;avoid&lt;/strong&gt; to employ a layer with a &lt;strong&gt;very low MoC such as a Conv1x1 layer&lt;/strong&gt; that has a &lt;strong&gt;very large input/output channel ratio.&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;input/output channel ratio가 크면 low MoC를 가진다는 사실을 알 수 있음.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Densely Connected Networks에 영감을 받아 모델 빌딩함.
    &lt;ol&gt;
      &lt;li&gt;DenseNet의 &lt;strong&gt;다수의 layer connections들을 줄였음.&lt;/strong&gt; =&amp;gt; concatenation cost를 줄이기 위해&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;balance the input/output channel ratio by increasing the channel width&lt;/strong&gt; of a layer according to its connections.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;659&quot; alt=&quot;스크린샷 2020-09-11 오후 9 30 55&quot; src=&quot;https://user-images.githubusercontent.com/48315997/92925832-14c51500-f476-11ea-93c3-6183adffd2e5.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;DRAM traffic&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;basic-concepts&quot;&gt;Basic Concepts&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;MAC : number of multiply-accumulate operations or floating point operations&lt;/li&gt;
  &lt;li&gt;DRAM : Dynamic Random-Access Memory
    &lt;ul&gt;
      &lt;li&gt;read/write model param. and feature maps&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CIO : Convolutional input/output
    &lt;ul&gt;
      &lt;li&gt;모든 conv layer에 대해 IN(C,W,H) X OUT(C,W,H) sum
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/92923605-b185b380-f472-11ea-81cc-a6d37d9bdd6c.png&quot; alt=&quot;image&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MoC : MACs over CIO of a layer = MACs/CIO&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-works&quot;&gt;Related Works&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;TREND : exploiting shortcuts&lt;/li&gt;
  &lt;li&gt;Highway networks, Residual Networks : add shortcuts to sum up a layer with multiple preceeding layers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DenseNet&lt;/code&gt;&lt;/strong&gt; : &lt;strong&gt;concatenates all preceeding layers as a shortcut&lt;/strong&gt; achieving more efficent deep supervision.&lt;/li&gt;
  &lt;li&gt;그러나 shortcuts는 large memory usage, heavy DRAM traffic을 유발할 수 있다.
    &lt;ul&gt;
      &lt;li&gt;
        &lt;blockquote&gt;
          &lt;p&gt;Using shortcuts elongates the lifetime of a tensor, which may result in frequent data exchanges betwwen DRAM and cache.&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DenseNet의 sparsified version : &lt;em&gt;LogDenseNet, SparseNet&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Sparse&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;The pros?&lt;/strong&gt; If you have a lot of zeros, &lt;strong&gt;you don’t have to compute some multiplications, and you don’t have to store them&lt;/strong&gt;. So you &lt;strong&gt;&lt;em&gt;may&lt;/em&gt;&lt;/strong&gt; gain on size and speed, for training and inference (more on this today).&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;The cons?&lt;/strong&gt; Of course, having all these zeros will probably have an impact on network accuracy/performance.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;increase the &lt;strong&gt;growth rate(output channel width) to recover the accuracy dropping from the connection pruning,&lt;/strong&gt; and the increase of growth rate &lt;strong&gt;can compromise the CIO reduction&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;즉 increase of growth rate는 좋게 작용된다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;harmonic-densenet&quot;&gt;Harmonic DenseNet&lt;/h2&gt;

&lt;h3 id=&quot;sparsification-and-weighting&quot;&gt;Sparsification and weighting&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;let layer &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; connect to layer &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k-2^n&lt;/code&gt; if 2^n divides k, where n is a non-negative integer and&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt; k-2^n &amp;gt;= 0&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;HarDBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_link&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_ch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;growth_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_ch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;growth_rate&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;link&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;dv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dv&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;link&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grmul&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;link&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;ch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_link&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_ch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;growth_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ch&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;link&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;2^n 개의 layer들이 이런 식으로 processed되면 layer [1 : 2^n -1]는 메모리에서 flush된다.&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;어떻게 flush 된다는 건지 잘 이해가 되지 않음.&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Power-of-two-th harmonic waves가 만들어짐. 그래서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Harmonic&lt;/code&gt; 이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;461&quot; alt=&quot;스크린샷 2020-09-11 오후 9 52 09&quot; src=&quot;https://user-images.githubusercontent.com/48315997/92927733-0cbaa480-f479-11ea-8194-3567f8ae83f1.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;이 방식은 concatenation cost를 눈에 띄게 감소시킨다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;layers with an index divided by a larger power of two are more influential&lt;/strong&gt; than those that divided by a smaller power of two.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;많이 connection되니까 당연히 influential 하다.&lt;/li&gt;
      &lt;li&gt;In this model, they amplify these key layers by increasing their channels, &lt;strong&gt;which can balance the channel ratio between the input and output of a layer to avoid a low MoC.&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;이런 key layer들을 &lt;strong&gt;amplify&lt;/strong&gt; 했음(channel 수를 늘리면서)&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;layer &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l&lt;/code&gt; has an initial growth rate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt;, we let its channel number to be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k * m^n&lt;/code&gt; , where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; is the max number satisfying that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l&lt;/code&gt; is divided by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2^n&lt;/code&gt;
&lt;img width=&quot;442&quot; alt=&quot;스크린샷 2020-09-11 오후 10 11 01&quot; src=&quot;https://user-images.githubusercontent.com/48315997/92929542-af742280-f47b-11ea-89b9-22e4e5e9331d.png&quot; /&gt;&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt;  은 low-dimensional compression factor 역할을 한다.&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt; 을 2보다 작게하면 input channel을 output channel보다 작게 할 수 있다.
            &lt;ul&gt;
              &lt;li&gt;Empirically, settin &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt; between 1.6 and 1.9&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;transition-and-bottleneck-layers&quot;&gt;Transition and Bottleneck Layers&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HDB(Harmonic Dense Block)&lt;/code&gt; : the proposed connection pattern forms a group of layers
    &lt;ul&gt;
      &lt;li&gt;is followed by a Conv1x1 layer as a transition&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;HDB의 depth는 2의 제곱수로 설정
    &lt;ul&gt;
      &lt;li&gt;HDB의 마지막 레이어가 가장 큰 채널수를 가지도록 하기 위해서&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DenseNet -&amp;gt; gradient할 때 모든 레이어를 다 pass함&lt;/li&gt;
  &lt;li&gt;논문의 HBD with depth L -&amp;gt; pass through at most &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log L layers&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;degradation을 완화시키기위해, depth-L HDB를 layer L과 all its preceeding &lt;strong&gt;odd numbered layers&lt;/strong&gt;  를 concatenation시킨다.&lt;/li&gt;
      &lt;li&gt;2~L-2의 all even layer들의 아웃풋은 HDB가 한번 끝날때마다 버려진다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Bottleneck layer
    &lt;ul&gt;
      &lt;li&gt;DenseNet에서는 param. efficiency를 위해 매 Conv3x3 layer전에 bottleneck을 두었다.&lt;/li&gt;
      &lt;li&gt;하지만 HarDnet에서는 위에서 &lt;strong&gt;이미 channel ratio(매 레이어마다 input&amp;amp;output 사이의)의 균형을 잡았으므로 bottleneck layer는 쓸모없어진다.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;그래서 HBD에서는 Bottleneck layer없이 &lt;strong&gt;Conv3x3 for all layers&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transition layer
    &lt;ul&gt;
      &lt;li&gt;&lt;img width=&quot;492&quot; alt=&quot;스크린샷 2020-09-11 오후 10 26 18&quot; src=&quot;https://user-images.githubusercontent.com/48315997/92931026-d16ea480-f47d-11ea-96df-eff3717796b9.png&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;inverted trainsition module
        &lt;ul&gt;
          &lt;li&gt;maps input tensor to an additional max pooling function along with the original average pooling, followed by concatenation and Conv1x1.&lt;/li&gt;
          &lt;li&gt;50% of CIO를 감소시킴&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;&lt;img width=&quot;483&quot; alt=&quot;스크린샷 2020-09-11 오후 10 27 05&quot; src=&quot;https://user-images.githubusercontent.com/48315997/92931109-ed724600-f47d-11ea-81a6-d3903463743b.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;CamVid Dataset&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;replace all the blocks in a FC-DenseNet with HDBs&lt;/li&gt;
      &lt;li&gt;the architecture of FC-DenseNet with an encoder-decoder structure and block level shortcuts to create models for sematic segmentation.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;blockquote&gt;
      &lt;p&gt;We propose FC-HarDNet84 as specified in Table 3 for comparing with FC-DenseNet103. &lt;strong&gt;The new network achieves CIO reduction by 41% and GPU inference time reduction by 35%.&lt;/strong&gt; A smaller version, FC-HarDNet68, also outperforms FC-DenseNet56 by a 65% less CIO and 52% less GPU inference time.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;1009&quot; alt=&quot;스크린샷 2020-09-11 오후 10 34 16&quot; src=&quot;https://user-images.githubusercontent.com/48315997/92931776-ee57a780-f47e-11ea-9201-93de47f4a12a.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ImageNet Datasets&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;1018&quot; alt=&quot;스크린샷 2020-09-11 오후 10 34 41&quot; src=&quot;https://user-images.githubusercontent.com/48315997/92931811-fca5c380-f47e-11ea-9c47-4b9738f47a92.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Object Detection
    &lt;ul&gt;
      &lt;li&gt;HarDNet-68 as &lt;strong&gt;a backbone model for a Single Shot Detector (SSD) and train it with PASCAL VOC 2007 and MS COCO datasets&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;479&quot; alt=&quot;스크린샷 2020-09-11 오후 10 35 26&quot; src=&quot;https://user-images.githubusercontent.com/48315997/92931875-1810ce80-f47f-11ea-9883-8d1d9ac87947.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;There is an assumption with the &lt;strong&gt;CIO&lt;/strong&gt;, which is &lt;strong&gt;a CNN model that is processed layer by layer without a fusion.&lt;/strong&gt; In contrast, &lt;strong&gt;fused-layer computation for multiple convolutional layers has been proposed.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;CIO still failed&lt;/strong&gt; to predict the actual inference time &lt;strong&gt;such as comparing two network models with significantly differnent architectures&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;In some of the layers CIO may dominate, but for the other layers, MACs can still be the key factor if its computational density is relatively higher. To precisely predict the inference latency of a network, we need to breakdown to each of the layers and investigate its MoC to predict the inference latency of the layer.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;어쨌거나 &lt;strong&gt;DRAM traffic의 중요성&lt;/strong&gt;을 강조하고 싶어함.&lt;/li&gt;
  &lt;li&gt;traffic reduction을 위한 가장 좋은 방법은 &lt;strong&gt;MoC를 증가시키는 것&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;which might be counter-intuitive to the widely-accepted knowledge of that using more Conv1x1 achieves a higher efficiency.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Traffic" /><category term="Computer Vision" /><category term="Semantic Segmentation" /><category term="Network" /><summary type="html">HarDNet : A Low Memory Traffic network 를 읽고 개인적으로 정리한 글입니다.</summary></entry><entry><title type="html">Neural Architecture Search With Reinforcement Learning</title><link href="http://localhost:4000/paper%20review/2020/08/29/NEURAL-ARCHITECTURE-SEARCH-WITH-REINFORCEMENT-LEARNING/" rel="alternate" type="text/html" title="Neural Architecture Search With Reinforcement Learning" /><published>2020-08-29T00:00:00+09:00</published><updated>2020-08-29T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/08/29/NEURAL%20ARCHITECTURE%20SEARCH%20WITH%20REINFORCEMENT%20LEARNING</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/08/29/NEURAL-ARCHITECTURE-SEARCH-WITH-REINFORCEMENT-LEARNING/">&lt;p&gt;Neural Architecture Search With Reinforcement Learning 논문을 읽고 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;(You should include contents of summary and introduction.)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;we use a re- current network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;RNN을 이용해서 neural network의 model description(하이퍼 파라미터: # of filters, stride length …)을 문자열로 생성한다.
강화학습을 통해 expected accuracy를 최대로 만든다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/658/1*KgICs1DPpGbqY2WWPn1kwg.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Controller에서 p의 확률로 A라는 Architecture를 생성한다.&lt;/li&gt;
  &lt;li&gt;자식 네트워크에서는 A 아키텍쳐를 훈련시켜 정확도 R을 구한다.&lt;/li&gt;
  &lt;li&gt;정확도를 리워드의 신호로 사용한다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;policy gradient&lt;/code&gt;를 계산해서 컨트롤러를 업데이트한다.&lt;/li&gt;
  &lt;li&gt;반복하다보면 더 높은 확률로 더 높은 정확도를 보이는 아키텍쳐를 찾을 수 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;related-work-basic-concepts&quot;&gt;Related work (Basic concepts)&lt;/h2&gt;

&lt;p&gt;NAS 부분의 거의 최초라고 볼 수 있음.&lt;/p&gt;

&lt;p&gt;이전 연구들 : Hyperparameter optimization&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;it is difficult to ask them to &lt;strong&gt;generate a variable-length configuration&lt;/strong&gt; that specifies the structure and connectivity of a network&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;유전자 알고리즘&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;search-based 방식이라 탐색속도가 느림.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;컨트롤러에서의 Neural Architecture 방식은 이전 예측값들을 input으로 받아 하이퍼 파라미터를 한 번에 하나씩 예측하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto-regressive&lt;/code&gt;한 방식이다.&lt;/p&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;
&lt;p&gt;(Explain one of the methods that the thesis used.)&lt;/p&gt;

&lt;p&gt;이 논문의 Key point : &lt;strong&gt;skip connection 예측하여 모델의 복잡도를 높인 것&lt;/strong&gt;, &lt;strong&gt;파라미터 접근방식을 사용해서 훈련 속도를 높인 것&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Generate Model with a Controller Recurrent Neural Network&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*9dgifjZ6BKyPqzIxR-EGwg.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It predicts filter height, filter width, stride height, stride width, and number of filters for one layer and repeats. Every prediction is carried out by a softmax classifier and then fed into the next time step as input.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;컨트롤러를 이용하여 CNN 모델에 사용하는 하이퍼파라미터들을 생성함.&lt;/p&gt;

&lt;p&gt;레이어마다 사용할 필터, Stride 값을 예측하고 반복함.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;하이퍼 파라미터 예측시에 softmax classifier를 거친값이 다음 스텝의 input으로 들어감.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;컨트롤러 RNN이 아키텍쳐를 생성하면 생성된 아키텍쳐의 뉴럴 네트워크를 훈련시킴.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The parameters of the controller RNN, θc, are then optimized in order to maximize the expected validation accuracy of the proposed architectures.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Validation set으로 네트워크의 정확도를 측정하고, 컨트롤러 RNN의 파라미터 세타C는 정확도의 기대값을 최대화하기 위해 최적화됨.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Training with Reinforce&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/456/1*T03ptXjYcHkOBLFfoEs89A.png&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;controller to maximize its expected reward&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;컨트롤러 token list a[1]:a[T] : Architecture predicted by the controller RNN viewed as a sequence of actions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;자식 네트워크는 생성된 구조의 정확도 R을 출력하고, 이 R을 강화학습의 리워드로 사용해서 컨트롤러를 강화학습 훈련시킴.&lt;/li&gt;
  &lt;li&gt;Layer 하나짜리 CNN에서의 T=3임.
    &lt;ul&gt;
      &lt;li&gt;a1 : filter height&lt;/li&gt;
      &lt;li&gt;a2 : filter width&lt;/li&gt;
      &lt;li&gt;a3 : # of filters&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/91626958-815c0080-e9ee-11ea-89cb-43ee634d253e.png&quot; alt=&quot;스크린샷 2020-08-29 오전 11 55 12&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/91627195-bc5f3380-e9f0-11ea-9bd2-3d2b8653075c.png&quot; alt=&quot;스크린샷 2020-08-29 오후 12 11 09&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/91627201-c84af580-e9f0-11ea-8824-88e3e1f87255.png&quot; alt=&quot;스크린샷 2020-08-29 오후 12 11 30&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In this work, we use the REINFORCE rule from Williams (1992)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Standard REINFORCE Update Rule&lt;/li&gt;
  &lt;li&gt;R은 미분 불가능함. =&amp;gt; policy gradient를 써서 세타 C를 업데이트한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;accelerate-training-with-parallelism-and-asynchronous-updates&quot;&gt;Accelerate Training with Parallelism and Asynchronous Updates&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;자식 네트워크 : 하나의 모델을 뜻함&lt;/li&gt;
  &lt;li&gt;여러 컨트롤러 * 여러 자식 네트워크 =&amp;gt; 많은 네트워크를 만들어냄
    &lt;ul&gt;
      &lt;li&gt;훈련 속도를 높이기 위해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;파라미터-서버&lt;/code&gt; 구조 사용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*9UQdtOqDpyef44nhKYsrcA.png&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;S개의 파라미터 서버가 있고 이 서버와 연결된 K개의 복제된 &lt;strong&gt;컨트롤러에 공유된 파라미터 값이 저장됨.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;각각의 컨트롤러는 m개의 자식 네트워크를 복제해서 &lt;strong&gt;병렬&lt;/strong&gt;로 훈련시킴.&lt;/p&gt;

&lt;p&gt;이 때 자식 네트워크의 정확도는 파라미터 서버에 보낼 세타 C에 대한 gradient를 계산하기 위해 기록됨.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Increase Architecture Complexity with Skip Connection and Other Layer Types&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/91627249-22e45180-e9f1-11ea-9774-73701c1e18c4.png&quot; alt=&quot;스크린샷 2020-08-29 오후 12 14 03&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Skip connection을 추가해서 탐생 공간을 넓힌다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;레이어마다 anchor point를 더해서 이전 레이어들 중 어떤 레이어를 현재 레이어의 input으로 할지 결정함.&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Generate Recurrent Cell Architectures&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;지금까지 CNN을 위한 Neural Architecture, 지금은 RNN&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*MFYHqx5BOad936u6QfhmrQ.png&quot; alt=&quot;4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RNN, LSTM은 x(t), h(t-1)을 input으로 하고 h(t)를 output으로 하는 트리구조로 나타낼 수 있음(맨 오른쪽)&lt;/p&gt;

&lt;p&gt;RNN 컨트롤러에서는 트리 노드들의 결합방석(addition, elementwise multiplication)과 활성화함수(sigmoid,tanh)를 선택할 수 있음.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;그림 (b)의 Cell indices 의 왼쪽 1부분이 의미하는 것은 다음 메모리구조 C_t와 연결되는것은 Tree index 1 이며 오른쪽 0부분은 h_t 를 구할때 사용되는 것이 Tree index 0 이라는 것입니다. 그림 (b)의 Tree index 2 는 Tree0과 Tree1의 결합방식을 나타내는 것으로 그림에선 elementwise multiplication와 sigmoid의 결합이 됩니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;기존 SOTA 모델과 비교했을 때 약간의 성능 감소는 있었지만 &lt;strong&gt;더 작은 파라미터로 구현이 되었음,&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CNN (CIFAR-10 dataset)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/91627442-4fe53400-e9f2-11ea-98be-cb4432fd5b75.png&quot; alt=&quot;스크린샷 2020-08-29 오후 12 22 27&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RNN (Penn Treebank dataset)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/91627448-64c1c780-e9f2-11ea-8047-2596427ffe02.png&quot; alt=&quot;스크린샷 2020-08-29 오후 12 23 03&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Transfer Learning on Neural Machine Translation
    &lt;ul&gt;
      &lt;li&gt;LSTM을 빼고 NAS를 통해 만든 cell을 넣었음.&lt;/li&gt;
      &lt;li&gt;LSTM에 특화된 하이퍼파라미터들을 튜닝하지 않음&lt;/li&gt;
      &lt;li&gt;BELU score 0.5 오름&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;additional-studies&quot;&gt;Additional studies&lt;/h2&gt;
&lt;p&gt;(If you have some parts that cannot understand, you have to do additional studies for them. It’s optional.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Understanding Deep Learning Requires Rethinking Generalization&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Designing Neural Network Architectures Using RL&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;(References for your additional studies)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=XP3vyVrrt3Q&quot;&gt;https://www.youtube.com/watch?v=XP3vyVrrt3Q&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/@sunwoopark/slow-paper-neural-architecture-search-with-reinforcement-learning-6de601560522&quot;&gt;https://medium.com/@sunwoopark/slow-paper-neural-architecture-search-with-reinforcement-learning-6de601560522&lt;/a&gt;&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Reinforcement Learning" /><category term="Neural Architecture Search" /><summary type="html">Neural Architecture Search With Reinforcement Learning 논문을 읽고 정리한 글입니다.</summary></entry><entry><title type="html">딥러닝 모델을 이용한 수화 교육 웹 어플리케이션-Handlang(1)</title><link href="http://localhost:4000/project/2020/08/26/Handlang/" rel="alternate" type="text/html" title="딥러닝 모델을 이용한 수화 교육 웹 어플리케이션-Handlang(1)" /><published>2020-08-26T00:00:00+09:00</published><updated>2020-08-26T00:00:00+09:00</updated><id>http://localhost:4000/project/2020/08/26/Handlang</id><content type="html" xml:base="http://localhost:4000/project/2020/08/26/Handlang/">&lt;p&gt;DSC EWHA에서 2019.9~2020.8 까지 진행한 팀프로젝트로, 딥러닝 모델을 이용한 수화 학습 웹 어플리케이션입니다.
이 포스팅에서는 수화 인식 딥러닝 모델에 대해서만 다룹니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;handlang---aslamerican-sign-language-education-by-using-deep-learning-model&quot;&gt;Handlang - ASL(American Sign Language) Education by using deep learning model&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90604151-60015480-e237-11ea-8092-65387889b31d.png&quot; alt=&quot;home-page&quot; /&gt;&lt;/p&gt;

&lt;p&gt;딥러닝으로 학습된 수화 인식 모델을 바탕으로 알파벳, 숫자에 해당되는 수화를 학습 및 연습 할 수 있는 웹 어플리케이션입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;모델-정확도-개선을-위한-여러-시도들&quot;&gt;모델 정확도 개선을 위한 여러 시도들&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;[About models]&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;yolo-darknet&quot;&gt;YOLO darknet&lt;/h4&gt;

&lt;p&gt;YOLO is the model with excellent performance in Object detection.
We used darkflow, not yolo darknet, to take advantage of tensorflow.&lt;/p&gt;

&lt;p&gt;https://github.com/thtrieu/darkflow&lt;/p&gt;

&lt;p&gt;The most attempts were made at darkflow.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;YOLO_experiment_1
    &lt;ul&gt;
      &lt;li&gt;[a~z] 600 images each. training 500 epochs&lt;/li&gt;
      &lt;li&gt;acc : 0.42&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Feedback -&amp;gt; Predict performance is poor.&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;YOLO_experiment_2
    &lt;ul&gt;
      &lt;li&gt;pretrained weight - hand tracking model (https://github.com/Abdul-Mukit/dope_with_hand_tracking)&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;excluding `j`&quot;&gt;a~y&lt;/a&gt; 600 images each. 140 epochs&lt;/li&gt;
      &lt;li&gt;acc : 0.47&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;https://github.com/yskim0/GoogleSolutionChallenge_Handlang/raw/master/img/47.png&quot; alt=&quot;047&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;YOLO_experiment_3
    &lt;ul&gt;
      &lt;li&gt;pretrained weight - yolov2-tiny.weight(https://pjreddie.com/darknet/yolov2/)&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;excluding `j`&quot;&gt;a~y&lt;/a&gt; 600 images each. 220 epochs&lt;/li&gt;
      &lt;li&gt;acc : 0.56&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;https://github.com/yskim0/GoogleSolutionChallenge_Handlang/raw/master/img/56.png&quot; alt=&quot;056&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Feedback -&amp;gt; It is still not a satisfactory performance.&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;inception-v3&quot;&gt;Inception-v3&lt;/h4&gt;

&lt;p&gt;We tried transfer learning by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inception-v3&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;excluding `j`&quot;&gt;a~y&lt;/a&gt; 600 images each. 1000 steps&lt;/li&gt;
  &lt;li&gt;test acc. : about 88%
    &lt;ul&gt;
      &lt;li&gt;but not that much at real-time…&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;https://github.com/yskim0/GoogleSolutionChallenge_Handlang/raw/master/img/88.png&quot; alt=&quot;088&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;tensorflow-object-detection-api&quot;&gt;Tensorflow-Object-Detection-API&lt;/h4&gt;

&lt;p&gt;We tried transfer learning by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fast r-cnn&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;excluding `j`&quot;&gt;a~y&lt;/a&gt; 600 images each. 6000 steps&lt;/li&gt;
  &lt;li&gt;test acc. : about 80%
    &lt;ul&gt;
      &lt;li&gt;not test by images, but test by webcam.
        &lt;ul&gt;
          &lt;li&gt;poor performance&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Self-feedback: It is still not a satisfactory performance.&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;finally-custom-cnn-modelour-current-model&quot;&gt;Finally Custom CNN model(our current model)!&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;handlang_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;handlang_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;handlang_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;handlang_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;handlang_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;handlang_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;handlang_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;handlang_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;handlang_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;handlang_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;handlang_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;handlang_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;handlang_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'softmax'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;handlang_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;handlang_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'adam'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'categorical_crossentropy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;accuracy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Feedback : 위 모델을 사용하고, 웹 코드 내에서의 trick을 이용하여 조금 더 빠른 인식과 높은 정확도를 가질 수 있었음.&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;datasets&quot;&gt;Datasets&lt;/h2&gt;

&lt;p&gt;아래 데이터 셋들은 모델 트레이닝에 사용되었습니다.&lt;/p&gt;

&lt;p&gt;참고로, 우리 모델에서는 알파벳 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i,z&lt;/code&gt; 제외했습니다. (손동작이 포함되었기 때문에)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://www.kaggle.com/grassknoted/asl-alphabet
    &lt;ul&gt;
      &lt;li&gt;가장 성능이 좋은 모델에 사용된 데이터 셋입니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다른 모델에서는 아래의 데이터셋들을 사용했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/rajarshighoshal/asltestimages&quot;&gt;https://www.kaggle.com/rajarshighoshal/asltestimages&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/muhammadkhalid/sign-language-for-alphabets&quot;&gt;https://www.kaggle.com/muhammadkhalid/sign-language-for-alphabets&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/ayuraj/asl-dataset&quot;&gt;https://www.kaggle.com/ayuraj/asl-dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;team-handlang&quot;&gt;Team Handlang&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/yskim0/Handlang&quot;&gt;Project Github Link&lt;/a&gt;&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Object Detection" /><category term="Flask" /><category term="Education Application" /><summary type="html">DSC EWHA에서 2019.9~2020.8 까지 진행한 팀프로젝트로, 딥러닝 모델을 이용한 수화 학습 웹 어플리케이션입니다. 이 포스팅에서는 수화 인식 딥러닝 모델에 대해서만 다룹니다.</summary></entry><entry><title type="html">딥러닝 모델을 이용한 수화 교육 웹 어플리케이션-Handlang(2)</title><link href="http://localhost:4000/project/2020/08/26/Handlang2/" rel="alternate" type="text/html" title="딥러닝 모델을 이용한 수화 교육 웹 어플리케이션-Handlang(2)" /><published>2020-08-26T00:00:00+09:00</published><updated>2020-08-26T00:00:00+09:00</updated><id>http://localhost:4000/project/2020/08/26/Handlang2</id><content type="html" xml:base="http://localhost:4000/project/2020/08/26/Handlang2/">&lt;p&gt;DSC EWHA에서 2019.9~2020.8 까지 진행한 팀프로젝트로, 딥러닝 모델을 이용한 수화 학습 웹 어플리케이션입니다.
이 포스팅에서는 웹에 관련된 것을 다룹니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;handlang---aslamerican-sign-language-education-by-using-deep-learning-model&quot;&gt;Handlang - ASL(American Sign Language) Education by using deep learning model&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;딥러닝으로 학습된 수화 인식 모델을 바탕으로 알파벳, 숫자에 해당되는 수화를 학습 및 연습 할 수 있는 웹 어플리케이션입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;wireframe---figma&quot;&gt;Wireframe - Figma&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fn8Hg1%2FbtqCPBw0VEs%2F6fRBkKw5iK71fcigASJIC0%2Fimg.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93708908-f5fd0780-fb74-11ea-950e-adbd44260edd.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figma를 사용하여 팀원들과 홈페이지 와이어프레임을 구상하였습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;flask&quot;&gt;Flask&lt;/h2&gt;

&lt;p&gt;웹 개발 초보자에게 비교적 쉬운 Flask를 사용하여 구현하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;model-deploy&quot;&gt;Model Deploy&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;학습시킨 모델을 불러오는 법&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_model&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'model/handlang_model_4.h5'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 지문자 모델
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'model/su_adamax.h5'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 숫자 모델
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;ajax&quot;&gt;Ajax&lt;/h2&gt;

&lt;p&gt;웹캠으로 받은 이미지를 실시간으로 Detect해야하기 때문에 페이지를 새로 고치지 않아도 데이터를 로드할 수 있는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ajax&lt;/code&gt;를 사용하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;translation&quot;&gt;Translation&lt;/h2&gt;

&lt;p&gt;한글/영어 버전의 웹페이지를 구현하기 위해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flask_babel&lt;/code&gt;을 사용했습니다.&lt;/p&gt;

&lt;h2 id=&quot;study--quiz&quot;&gt;Study &amp;amp; Quiz&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93709264-fa76ef80-fb77-11ea-99c2-33eccceb6f65.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93709269-05ca1b00-fb78-11ea-85c1-ad523fd94f95.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93709270-0cf12900-fb78-11ea-9d04-4dbf3a648ea3.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93709273-12e70a00-fb78-11ea-999f-71a7f8876db6.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;team-handlang&quot;&gt;Team Handlang&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/yskim0/Handlang&quot;&gt;Project Github Link&lt;/a&gt;&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Object Detection" /><category term="Flask" /><category term="Education Application" /><summary type="html">DSC EWHA에서 2019.9~2020.8 까지 진행한 팀프로젝트로, 딥러닝 모델을 이용한 수화 학습 웹 어플리케이션입니다. 이 포스팅에서는 웹에 관련된 것을 다룹니다.</summary></entry><entry><title type="html">Meta Reinforcement Learning As Task Inference</title><link href="http://localhost:4000/paper%20review/2020/08/22/Meta-RL-as-task-inference/" rel="alternate" type="text/html" title="Meta Reinforcement Learning As Task Inference" /><published>2020-08-22T00:00:00+09:00</published><updated>2020-08-22T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/08/22/Meta%20RL%20as%20task%20inference</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/08/22/Meta-RL-as-task-inference/">&lt;p&gt;Tensorflow KR 논문 읽기 모임 PR12(Season 3)의 Meta Reinforcement Learning As Task Inference (&lt;a href=&quot;https://www.youtube.com/watch?v=phi7_QIhfJ4&quot;&gt;PR-239&lt;/a&gt;)를 발표하신 정창훈님 영상을 보고 정리하였습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;meta-reinforcement-learning-as-task-inference&quot;&gt;Meta Reinforcement Learning As Task Inference&lt;/h1&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;(You should include contents of summary and introduction.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;접근 방법 : Meta-RL을 하나의 &lt;strong&gt;paritally observed&lt;/strong&gt;로 본다.
    &lt;ul&gt;
      &lt;li&gt;MDP의 모든 정보를 agent가 전부 받는 게 아니라 부분적으로만 관찰&lt;/li&gt;
      &lt;li&gt;RL은 하나의 control문제로 볼 수 있는데, 여기에 &lt;strong&gt;inference problem&lt;/strong&gt;이 추가된 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;즉 POMDP(Partially-Observable Markov Decision Processes)문제를 해결하는 문제가 됨.
    &lt;ul&gt;
      &lt;li&gt;POMDP의 솔루션 : observation trajetory를 가지고 optimal policy를 찾는 것(미래 보상이 최대가 되는 쪽으로) == Reinforcement learning
        &lt;ul&gt;
          &lt;li&gt;그러나 주어진 데이터가 partially observation이기 때문에 또 하나의 모듈이 필요함 -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;belief state&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Belief State : 어떤 observation trajectory가 주어졌을 때 실제 true state의 probability
    &lt;ul&gt;
      &lt;li&gt;이걸 구할 수 있으면 POMDP가 구해짐
        &lt;ul&gt;
          &lt;li&gt;POMDP가 구해지면 Meta-RL 문제 해결!&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Observation이 주어졌을 때 optimal policy를 구하는 것은 belief state를 구할 수 있으면 문제가 풀린다!&lt;/strong&gt;
-&amp;gt; 그러면 Meta-RL 문제도 풀린다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이 논문의 key point : 두 가지 Neural Network를 사용
    &lt;ul&gt;
      &lt;li&gt;control하는 policy Network&lt;/li&gt;
      &lt;li&gt;belief state를 예측하는 inference Belief Module
        &lt;ul&gt;
          &lt;li&gt;auxilary supervised learning 로 해결 (Meta-learning때에만)&lt;/li&gt;
          &lt;li&gt;즉, belief module은 meta-learning 때 supervised learning으로 학습됨.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;off-policy 사용(Meta-RL 에서는 대개 on-policy)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-work-basic-concepts&quot;&gt;Related work (Basic concepts)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Meta Learning : Learning to Learn&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90916720-8c29fa80-e41c-11ea-9484-b6129a1688d3.png&quot; alt=&quot;스크린샷 2020-08-22 오전 2 09 41&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;trajectory&lt;/code&gt; is just a sequence of states and actions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Meta RL
    &lt;ul&gt;
      &lt;li&gt;env.는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MDP&lt;/code&gt;로 표현됨. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;M = {S,A,P,r}&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Agent &amp;lt;-&amp;gt; env. : 서로 interact하면서 future reward를 Max. 시키는 세타 찾기&lt;/li&gt;
      &lt;li&gt;Meta learning : 여러 개의 task들을 sampling하여 meta-learning한 후, meta-test시 빠르게 adaption될 수 있어야 함.
        &lt;ul&gt;
          &lt;li&gt;RL -&amp;gt; 그 task들 각각이 하나의 MDP로 정의 가능!&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90917047-170af500-e41d-11ea-9791-74d540738406.png&quot; alt=&quot;스크린샷 2020-08-22 오전 2 13 34&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;MDP를 여러개 sampling해서 학습하고, 그걸 통해 optimal theta얻는 것이 목표&lt;/li&gt;
          &lt;li&gt;test시에는 처음 보는 태스크(전체적인 쉐잎은 비슷해야)에 잘 적용되어야 함.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;접근 방법&lt;/strong&gt;
        &lt;ol&gt;
          &lt;li&gt;Recurrent policies : RNN&lt;/li&gt;
          &lt;li&gt;Optimization problem : MAML&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;partially observed RL&lt;/strong&gt;
            &lt;ul&gt;
              &lt;li&gt;MDP의 모든 정보를 받는 게 아니라 부분적으로만 받는다 -&amp;gt; inference problem 추가됨&lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90917816-93520800-e41e-11ea-81ba-5cd461f513e4.png&quot; alt=&quot;스크린샷 2020-08-22 오전 2 24 11&quot; /&gt;&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;z : task들의 모든 정보를 담고 있는 true information&lt;/li&gt;
              &lt;li&gt;z를 inference하면서 RL 컨트롤도 할 수 있는 해석하는 관점&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MDP(Markov Decision Processes) : (X,A,P,p0, R, discount factor)
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90918158-2428e380-e41f-11ea-8e6b-8803cce60db7.png&quot; alt=&quot;스크린샷 2020-08-22 오전 2 28 15&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;control이 있다 -&amp;gt; 대표적으로 RL&lt;/li&gt;
      &lt;li&gt;state가 완전히 관측되냐, 부분적으로 관측이 되냐 -&amp;gt; MDP/POMDP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;POMDP(Partially-Observable Markov Decision Processes)
    &lt;ul&gt;
      &lt;li&gt;MDP의 general한 버전&lt;/li&gt;
      &lt;li&gt;MDP에서 omega, O가 추가됨&lt;/li&gt;
      &lt;li&gt;X : state space. Agent 입장에서는 부분적으로 관측/아예 관측할 수 없게 됨.&lt;/li&gt;
      &lt;li&gt;그래서 Agent는 부분적으로 관측되는 observation state를 통해서만 학습할 수 있음.
  &lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90918340-70742380-e41f-11ea-9513-7f6c21fb4280.png&quot; alt=&quot;스크린샷 2020-08-22 오전 2 30 23&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;off-policy algorithm : 현재 학습하는 policy가 과거에 했던 experience도 학습에 사용이 가능하고, 심지어는 해당 policy가 아니라 예를 들어 사람이 한 데이터로부터도 학습을 시킬 수가 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;on-policy : 1번이라도 학습을 해서 policy improvement를 시킨 순간, 그 policy가 했던 과거의 experience들은 모두 사용이 불가능하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;
&lt;p&gt;(Explain one of the methods that the thesis used.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Solution to POMDP
&lt;img width=&quot;400&quot; alt=&quot;스크린샷 2020-08-22 오전 2 35 32&quot; src=&quot;https://user-images.githubusercontent.com/48315997/90918725-293a6280-e420-11ea-958a-b2c10565cbcc.png&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;observed trajectory를 가지고 optimal policy를 찾는 것(미래 보상의 합이 최대가 되는 action set, policy) == RL&lt;/li&gt;
      &lt;li&gt;그러나 주어진 데이터가 &lt;strong&gt;partially&lt;/strong&gt; 하기 때문에 또 하나의 모듈이 필요함&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Belief State&lt;/strong&gt; 를 구할 수 있으면 위의 문제가 풀림.&lt;/li&gt;
      &lt;li&gt;Belief State -&amp;gt; POMDP -&amp;gt; Meta-RL 해결&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;최근 Meta-RL에서의 POMDP 해석 문제
    &lt;ul&gt;
      &lt;li&gt;방금까지는 unobserved &lt;strong&gt;state&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;But 우리는 unobserved &lt;strong&gt;task&lt;/strong&gt;
  &lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90919150-e331ce80-e420-11ea-8cc8-2c009ca75d8e.png&quot; alt=&quot;스크린샷 2020-08-22 오전 2 40 44&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;내가 어느 task를 풀고 있는지 모르는 상태&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;state를 완전히 관측을 못한다는 것이 아니라!! 내가 어떤 task를 풀고 있는지를 관측하지 못한다는 관점으로 문제를 푸는 것임&lt;/strong&gt; == 어떤 task를 풀어야 하는지 task 정보를 완전하게 관측할 수 X -&amp;gt; 그래서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Task Inference&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img width=&quot;400&quot; alt=&quot;스크린샷 2020-08-22 오전 2 46 36&quot; src=&quot;https://user-images.githubusercontent.com/48315997/90919603-b5995500-e421-11ea-9939-c4dc28820f91.png&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;State, Action은 모든 MDP간에 sharing되어 있음.&lt;/strong&gt; W가 붙은 것들은 task-specific함.
        &lt;ul&gt;
          &lt;li&gt;이 세가지에 접근해서 Meta RL을 푼다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;목표 : meta-test시 처음 보는 task에도 적은 interaction으로 reward를 Max. 시키는 optimal policy 찾기&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Meta-RL using POMDP
&lt;img width=&quot;400&quot; alt=&quot;스크린샷 2020-08-22 오전 2 50 42&quot; src=&quot;https://user-images.githubusercontent.com/48315997/90919921-4839f400-e422-11ea-83ac-7e557be96989.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;A, S는 sharing
      - S는 true state와 Agent 입장에서 모르는 task에 대한 w를 concat해서 만듦
    &lt;ul&gt;
      &lt;li&gt;나머지들은 task-specific하게 정의&lt;/li&gt;
      &lt;li&gt;w만이 agent입장에서는 unobserved state라고 정의&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;optimal agent pi*는 아래와 같은 문제를 푼다.
&lt;img width=&quot;347&quot; alt=&quot;스크린샷 2020-08-22 오전 2 55 53&quot; src=&quot;https://user-images.githubusercontent.com/48315997/90920278-01003300-e423-11ea-803a-2b74a4d6a2d7.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;agent입장에서는 실제 task label인 w에 access 할 수 없다고 가정. (task에 대한 MDP를 다 모르는 것)
    &lt;ul&gt;
      &lt;li&gt;과거 observation trajectories은 LSTM, GRU등으로 agent network는 학습할 수 있음.&lt;/li&gt;
      &lt;li&gt;POMDP 문제를 해결하려면 task에 대한 belief state를 계산할 수 있어야 함.
        &lt;ul&gt;
          &lt;li&gt;observation trajectory가 주어졌을 때 실제 true task일 확률 (posterior)
            &lt;ul&gt;
              &lt;li&gt;이걸 계산할 수 있으면 POMDP 문제 해결, 그러나 계산 어려움.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;appendix)
  &lt;img width=&quot;707&quot; alt=&quot;스크린샷 2020-08-22 오전 3 00 31&quot; src=&quot;https://user-images.githubusercontent.com/48315997/90920660-a74c3880-e423-11ea-9b68-b972b3d4da7d.png&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;belief state의 posterior는 bayes Rule과 유사&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;policy가 식 안에 없음 -&amp;gt; task에 대한 posterior는 policy와 independent하다.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;off-policy algorithm 사용 가능 (보통 meta-RL에서는 on-policy : 데이터를 모으자마자 바로 업데이트)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;train&quot;&gt;Train&lt;/h3&gt;

&lt;p&gt;어떻게 모델을 학습시키는가&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;current state &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt;와 current belief &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;b_t(w)&lt;/code&gt;만 있으면 가능
&lt;img width=&quot;196&quot; alt=&quot;스크린샷 2020-08-22 오전 3 03 34&quot; src=&quot;https://user-images.githubusercontent.com/48315997/90920902-145fce00-e424-11ea-9bf4-9fc28d3c02f7.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;앞서 언급한 z 가 여기서는 belief state(task에 대한 모든 정보를 담고 있다)&lt;/li&gt;
  &lt;li&gt;belief state estimate 가능 -&amp;gt; optimal policy 구할 수 있다.&lt;/li&gt;
  &lt;li&gt;컴퓨팅 어려움 -&amp;gt; 2가지 NN 모델로 approximate 해야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Control하는 Policy Network&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Belief Module&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;어떻게 학습시키는가?
    &lt;ul&gt;
      &lt;li&gt;Meta-Learning 안에서는 Supervised Learning이 된다.
        &lt;ul&gt;
          &lt;li&gt;label 가능성
            &lt;ol&gt;
              &lt;li&gt;&lt;strong&gt;Task Description : task를 잘 표현하는 representation.&lt;/strong&gt;&lt;/li&gt;
              &lt;li&gt;Expert actions&lt;/li&gt;
              &lt;li&gt;Task embeddings
  &lt;img width=&quot;500&quot; alt=&quot;스크린샷 2020-08-22 오전 3 10 40&quot; src=&quot;https://user-images.githubusercontent.com/48315997/90921367-124a3f00-e425-11ea-8784-c78d7e116bd5.png&quot; /&gt;&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;meta-learning 때에만 사용.&lt;/strong&gt; 실제 우리가 원하는 meta-test에서 적은 에피소드만으로 빠르게 adapt할 때는 이 정보들을 더이상 사용하지 않음.&lt;/li&gt;
      &lt;li&gt;task를 supervised learning으로 풀었다!&lt;/li&gt;
      &lt;li&gt;w, h_t : independent of policy =&amp;gt; belief network도 off-policy로 효율적으로 학습 가능&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;&lt;img width=&quot;736&quot; alt=&quot;스크린샷 2020-08-22 오전 3 25 18&quot; src=&quot;https://user-images.githubusercontent.com/48315997/90922528-1d05d380-e427-11ea-8e3a-9fe4b1417e21.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;LSTM, IB : optional (IB : regularization 효과)&lt;/p&gt;

&lt;p&gt;A. Baseline LSTM Agent : belief network가 없는 일반 agent&lt;/p&gt;

&lt;p&gt;B. 논문에서 제안된 모델 : trajectory 넣어서 &lt;strong&gt;Belief network 학습&lt;/strong&gt;
    - Belief feature + trajectory 해서 &lt;strong&gt;policy network 학습&lt;/strong&gt;
    - 각자 역할에 집중&lt;/p&gt;

&lt;p&gt;C. Mixed : Network 하나에 합친 것&lt;/p&gt;

&lt;h3 id=&quot;experiment&quot;&gt;Experiment&lt;/h3&gt;

&lt;p&gt;&lt;img width=&quot;755&quot; alt=&quot;스크린샷 2020-08-22 오전 3 28 30&quot; src=&quot;https://user-images.githubusercontent.com/48315997/90922806-900f4a00-e427-11ea-8b7e-6c06f607daf5.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Multi-armed bandit : 20 arms and 100 horizon.
    &lt;ul&gt;
      &lt;li&gt;20번 당겼을 때 reward probability =&amp;gt; task description. (arm들의 vec.)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Semicircle : 반원 안에서 naviagtion
    &lt;ul&gt;
      &lt;li&gt;task desription : 각도&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;736&quot; alt=&quot;스크린샷 2020-08-22 오전 3 33 40&quot; src=&quot;https://user-images.githubusercontent.com/48315997/90923240-483cf280-e428-11ea-9280-d01cc4fba786.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;cheetah : task description -&amp;gt; velocity&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;634&quot; alt=&quot;스크린샷 2020-08-22 오전 3 40 23&quot; src=&quot;https://user-images.githubusercontent.com/48315997/90923769-3871de00-e429-11ea-893d-b52b23dcbb83.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;main-contributions&quot;&gt;Main contributions&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Supervised Learning을 통해서 performance 높임&lt;/li&gt;
  &lt;li&gt;Belief network 학습시킬 때 off-policy 알고리즘 사용 가능&lt;/li&gt;
  &lt;li&gt;continuous, sparse rewards 환경에서도 좋았다&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;additional-studies&quot;&gt;Additional studies&lt;/h2&gt;
&lt;p&gt;(If you have some parts that cannot understand, you have to do additional studies for them. It’s optional.)&lt;/p&gt;

&lt;p&gt;POMDP&lt;/p&gt;

&lt;p&gt;Belief State?&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;(References for your additional studies)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=phi7_QIhfJ4&quot;&gt;https://www.youtube.com/watch?v=phi7_QIhfJ4&lt;/a&gt; - 논문 설명&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://newsight.tistory.com/250&quot;&gt;https://newsight.tistory.com/250&lt;/a&gt; - policy 부분 개념&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Meta Learning" /><category term="Reinforcement Learning" /><category term="Meta RL" /><summary type="html">Tensorflow KR 논문 읽기 모임 PR12(Season 3)의 Meta Reinforcement Learning As Task Inference (PR-239)를 발표하신 정창훈님 영상을 보고 정리하였습니다.</summary></entry><entry><title type="html">Model-Agnostic Meta-Learning for fast adaptation of deep networks</title><link href="http://localhost:4000/paper%20review/2020/08/14/Model-Agnostic-Meta/" rel="alternate" type="text/html" title="Model-Agnostic Meta-Learning for fast adaptation of deep networks" /><published>2020-08-14T00:00:00+09:00</published><updated>2020-08-14T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/08/14/Model-Agnostic%20Meta</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/08/14/Model-Agnostic-Meta/">&lt;p&gt;Model-Agnostic Meta-Learning for fast adaptation of deep networks 를 읽고 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks&quot;&gt;Model-Agnostic Meta-Learning for fast adaptation of deep networks&lt;/h1&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;(You should include contents of summary and introduction.)&lt;/p&gt;

&lt;p&gt;좋은 Weight로 initialize하는 방법에 대한 것.
&lt;strong&gt;어떻게 하면 좋은 initial weight를 찾을 수 있는가?&lt;/strong&gt;
&lt;strong&gt;어떤 inital weight를 가지면 모르는 태스크들에 대해서도 빨리 적응시킬 수 있는가?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Key Point&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90264311-df83d200-de8b-11ea-8f03-8475e54d2ff7.png&quot; alt=&quot;스크린샷 2020-08-15 오전 12 11 28&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- **주어진 태스크들에 대해서 1 step 갔을 때, 모든 태스크에 대해서 로스가 미니멈이 되는 현재의 세타를 찾는 것!**
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;related-work-basic-concepts&quot;&gt;Related work (Basic concepts)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Model-Agnostic
    &lt;ul&gt;
      &lt;li&gt;학습에 사용된 model이 무엇인지에 구애받지 않고 독립적으로 모델을 해석할 수 있다는 뜻&lt;/li&gt;
      &lt;li&gt;즉, 학습에 사용되는 모델과 설명에 사용되는 모델을 분리&lt;/li&gt;
      &lt;li&gt;이 방법은 어떤 모델이든 상관 없이 적용할 수 있는 방법이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Meta learning
    &lt;ul&gt;
      &lt;li&gt;learning to learn&lt;/li&gt;
      &lt;li&gt;좋은 메타 러닝 모델 = 트레이닝 때 접하지 않았던 새로운 태스크나 환경에 대해서 잘 적응되거나 일반화가 잘 됨.&lt;/li&gt;
      &lt;li&gt;Reinforcement learning과 결합한 &lt;strong&gt;meta-learning&lt;/strong&gt;(meta reinforcement learning) 얘기가 많이 나오고 있음&lt;/li&gt;
      &lt;li&gt;Few-shot classification은 supervised-learning 상황에서 meta-learning을 활용한 예시임.
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;하나의 데이터셋 자체가 하나의 data sample로 활용되고 있음.&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90264389-fd513700-de8b-11ea-8325-d244ed7ce734.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;즉 Meta-learning에서는 training, test의 개념이 일반과 약간 다르고, 그 때 들어가는 데이터셋도 다르다.&lt;/li&gt;
          &lt;li&gt;약간의 fine-tuning 과 유사한 접근법&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Few-shot learning
    &lt;ul&gt;
      &lt;li&gt;적은 수의 데이터로 학습 시키는 것&lt;/li&gt;
      &lt;li&gt;one-shot learning : 한 장의 데이터만으로 학습 시키는 것&lt;/li&gt;
      &lt;li&gt;K-shot learning이라고 많이 부르는 듯&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;
&lt;p&gt;(Explain one of the methods that the thesis used.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90264311-df83d200-de8b-11ea-8f03-8475e54d2ff7.png&quot; alt=&quot;스크린샷 2020-08-15 오전 12 11 28&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;세타 1,2,3 -&amp;gt; 1,2,3번 태스크라고 보자.&lt;/li&gt;
  &lt;li&gt;만약 1번 태스크에 대해서 학습을 시킨다 그러면, 1의 optimum point로 가게끔 학습시켜야 함.&lt;/li&gt;
  &lt;li&gt;근데 샘플이 많지 않으니까 중간에 Local min.에 빠지는 경우 등 원하는 방향으로 흘러가지 않을 가능성이 더 큼.&lt;/li&gt;
  &lt;li&gt;메타러닝을 통해 세타를 저 점(화살표가 가리키는점)에 가지고 오면 1,2,3에 가장 가까운 포인트. 즉, 여기를 어떻게 보낼거냐는 문제!&lt;/li&gt;
  &lt;li&gt;예를 들어, 3번 태스크에 대한 One-shot이 주어졌을 때 &lt;strong&gt;gradient 1번해서 3번쪽으로 딱 가고 싶은 것&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;수식으로-보는-아이디어&quot;&gt;수식으로 보는 아이디어&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90264428-0f32da00-de8c-11ea-8f6f-c95ea78fe789.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;i번째 태스크에 대한 세타프라임 정의&lt;/li&gt;
  &lt;li&gt;1 step gradient를 갔을 때의 포인트임.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;우리는 이 세타 프라임의 포인트에서 loss가 최소가 되게 하고 싶음! 
–&amp;gt; &lt;strong&gt;세타 프라임에서 로스가 미니멈이 되는 세타&lt;/strong&gt;를 찾고 싶다!!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90264444-13f78e00-de8c-11ea-92b2-353d4b21a491.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;세타 프라임을 위에서 정의했으므로, loss 식에 넣을 수 있다.&lt;/li&gt;
  &lt;li&gt;세타에서 1 스텝 더 간 포인트의 미니멈을 정의하게 되는 것(== 세타프라임)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90264455-178b1500-de8c-11ea-9ddf-c9e5ae387250.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;세타에 대해서 미분함!&lt;/li&gt;
  &lt;li&gt;태스크가 여러 가지 있으니까 여러 가지에 대해 &lt;strong&gt;전체가 미니멈이 되는 포인트&lt;/strong&gt;를 찾아야 함.&lt;/li&gt;
  &lt;li&gt;세타 프라임 안에는 이미 세타에 대한 미분이 들어가있으므로 여기서 미분을 또 하게 되면 &lt;strong&gt;hessian&lt;/strong&gt;이 나올 것이다(?)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다시 요약하자면,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;우리가 찾고 싶은 세타는 태스크 각각을 minimize하는 세타가 아니라,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;주어진 태스크들에 대해서 1 step 갔을 때 모든 태스크에 대해서 minimum이 되는&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;지금 현재의 세타를 찾는 것.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Model-Agnostic Meta-Learning&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90264464-1b1e9c00-de8c-11ea-81c4-7d5263555fc9.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;우선 파라미터들을(세타) 랜덤하게 initialize&lt;/li&gt;
  &lt;li&gt;태스크들을 sampling하고, for문 안에서는 각각의 태스크에 대한 그래디언트를 찾음
    &lt;ul&gt;
      &lt;li&gt;1 step 더 가는 그래디언트&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;모든 태스크들에 대해서 다시 그래디언트를 해서 sum함.&lt;/li&gt;
  &lt;li&gt;원래 파라미터 세타를 업데이트&lt;/li&gt;
  &lt;li&gt;위의 과정 반복&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;MAML for Few-shot Supervised learning&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90264528-3093c600-de8c-11ea-94a2-81a9e899ca75.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;regression loss&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;classification loss&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Few-shot 이미지 classification일 경우 loss를 크로스 엔트로피로 계산&lt;/li&gt;
  &lt;li&gt;메타 업데이트를 위한 샘플링을 함.
    &lt;ul&gt;
      &lt;li&gt;최종 메타 파라미터(세타)를 찾기 위해서 쓰이는 샘플들&lt;/li&gt;
      &lt;li&gt;이전의 샘플 D는 세타 프라임을 위한 샘플들임.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;MAML for Reinforcement Learning&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90264539-34bfe380-de8c-11ea-88c6-4c1635a874f1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reward는 미분이 가능해야하니까 policy gradient를 사용함.&lt;/li&gt;
  &lt;li&gt;f(theta)는 policy를 나타내는 뉴럴 넷&lt;/li&gt;
  &lt;li&gt;negative reward를 loss로 사용&lt;/li&gt;
  &lt;li&gt;에피소드 길이만큼 쭉 진행해서 sum한 것이 loss가 됨.&lt;/li&gt;
  &lt;li&gt;마찬가지로 각 태스크에 대해서 샘플을 하고, 전체 에피소드 length 만큼 K번 trajecctories 샘플&lt;/li&gt;
  &lt;li&gt;그래디언트 계산해서 1 step 더 간 포인트를 찾아내고&lt;/li&gt;
  &lt;li&gt;1 step 더 간 파라미터 셋에서의 샘플 trajectories들을 샘플링 한 다음에&lt;/li&gt;
  &lt;li&gt;다 하고 바깥으로 나와서 파라미터 업데이트를 위해 loss를 계산하고 그래디언트를 구함.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;1,2,3번 모두 크게 다르지 않다.&lt;/p&gt;

&lt;h3 id=&quot;experimental-result&quot;&gt;Experimental Result&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The goal of our experimental evaluation
    &lt;ul&gt;
      &lt;li&gt;얼마나 빨리 learning을 할 수 있는가&lt;/li&gt;
      &lt;li&gt;서로 다른 도메인에서 사용이 될 수 있는가 -&amp;gt; supervised regression, classification, reinforcement learning&lt;/li&gt;
      &lt;li&gt;여러 번 gradeint update를 할수록 더 좋아지는가&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Regression&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;sine wave fitting 을 실험.
    &lt;ul&gt;
      &lt;li&gt;임의의 sine wave 를 만들어서, 그것을 fitting 하는 예제&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90264553-3a1d2e00-de8c-11ea-85ad-e10a89dd15a8.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;삼각형 : 트레이닝 샘플&lt;/li&gt;
  &lt;li&gt;빨간색 = ground truth&lt;/li&gt;
  &lt;li&gt;연두색 = 메타러닝을 통해 학습된 pre-weight&lt;/li&gt;
  &lt;li&gt;초록색 = 그래디언트를 1 step / 10 steps 밟았을 때&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;a) K = 5(5개의 샘플이 주어졌을 때)&lt;/p&gt;

&lt;p&gt;b) K = 10 (10개의 샘플이 주어졌을 때)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;그래디언트 10번하면 거의 똑같이 됨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;c,d) Pre-traineed model 사용&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;fitting하는 뉴럴 넷을 sine wave task로 잔뜩 만들어서 평균적인 sine wave에 대해 학습된 것&lt;/li&gt;
  &lt;li&gt;pre-update는 meta-learning으로 만들어진 모델과 유사하나 1 step 간다고 해서 막 변하지 않음.
    &lt;ul&gt;
      &lt;li&gt;fitting이 잘 안된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Classification&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;One-shot, few-shot learning에서 주로 쓰이는 데이터 셋 : Omniglot dataset
    &lt;ul&gt;
      &lt;li&gt;few-shot learning의 mnist같은 데이터셋&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90264620-51f4b200-de8c-11ea-9a4c-e7bc5aaa7a0b.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90264625-54efa280-de8c-11ea-9b16-7154ffe38326.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First order approx
    &lt;ul&gt;
      &lt;li&gt;두 번 미분하기 위해 hessian이 들어간다고 했음.&lt;/li&gt;
      &lt;li&gt;근데 ReLU는 중간에 미분 불가능한 포인트가 있고, 이를 제외하면 Linear함.&lt;/li&gt;
      &lt;li&gt;따라서 이것을 first order까지만 계산하고 업데이트해도 성능이 그렇게 떨어지지 않는다고 함.&lt;/li&gt;
      &lt;li&gt;정석대로 두 번 미분하면 33%정도 더 오래걸린다고 함.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Reinforcement learning&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;2D navigation 실험 : 위치를 정해주고 goal까지 가기&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/90264693-6e90ea00-de8c-11ea-8f12-c1399d0668e4.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;3step update하면 잘 간다&lt;/li&gt;
  &lt;li&gt;pre-train하고 fine-tuning하는 방법과 비교…&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;p&gt;from &lt;a href=&quot;https://github.com/dragen1860/MAML-Pytorch/blob/master/meta.py&quot;&gt;dragen1860 / MAML-Pytorch&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_spt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_spt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_qry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_qry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        :param x_spt:   [b, setsz, c_, h, w]
        :param y_spt:   [b, setsz]
        :param x_qry:   [b, querysz, c_, h, w]
        :param y_qry:   [b, querysz]
        :return:
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;task_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;setsz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_spt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;querysz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_qry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;losses_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# losses_q[i] is the loss on step i
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;corrects&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;


        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# 1. run the i-th task and compute loss for k=0
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_spt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;vars&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bn_training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_spt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autograd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;fast_weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())))&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# this is the loss and accuracy before first update
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
                &lt;span class=&quot;c1&quot;&gt;# [setsz, nway]
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;logits_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_qry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bn_training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;loss_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_qry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;losses_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_q&lt;/span&gt;

                &lt;span class=&quot;n&quot;&gt;pred_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_qry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;corrects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;corrects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# this is the loss and accuracy after the first update
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
                &lt;span class=&quot;c1&quot;&gt;# [setsz, nway]
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;logits_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_qry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fast_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bn_training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;loss_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_qry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;losses_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_q&lt;/span&gt;
                &lt;span class=&quot;c1&quot;&gt;# [setsz]
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;pred_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_qry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;corrects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;corrects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt;

            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;c1&quot;&gt;# 1. run the i-th task and compute loss for k=1~K-1
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_spt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fast_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bn_training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_spt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
                &lt;span class=&quot;c1&quot;&gt;# 2. compute grad on theta_pi
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autograd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fast_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;c1&quot;&gt;# 3. theta_pi = theta_pi - train_lr * grad
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;fast_weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fast_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

                &lt;span class=&quot;n&quot;&gt;logits_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_qry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fast_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bn_training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;c1&quot;&gt;# loss_q will be overwritten and just keep the loss_q on last update step.
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;loss_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_qry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;losses_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_q&lt;/span&gt;

                &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;pred_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_qry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# convert to numpy
&lt;/span&gt;                    &lt;span class=&quot;n&quot;&gt;corrects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;corrects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt;



        &lt;span class=&quot;c1&quot;&gt;# end of all tasks
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# sum over all losses on query set across all tasks
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;loss_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;losses_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;task_num&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# optimize theta parameters
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meta_optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# print('meta update')
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# for p in self.net.parameters()[:5]:
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# 	print(torch.norm(p).item())
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meta_optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;


        &lt;span class=&quot;n&quot;&gt;accs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corrects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;querysz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;task_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;additional-studies&quot;&gt;Additional studies&lt;/h2&gt;
&lt;p&gt;(If you have some parts that cannot understand, you have to do additional studies for them. It’s optional.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Advanced researches&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Meta-SGD : 성능이 더 괜찮음&lt;/li&gt;
  &lt;li&gt;Bayesian Model-Agnostic Meta-Learning
    &lt;ul&gt;
      &lt;li&gt;한 포인트로 지정하는 것이 아니라 probability를 이용&lt;/li&gt;
      &lt;li&gt;optimum point들이 서로 가깝고 몰려있으면 좋겠지만, 여러 곳에 있을 수도 있고 확률적으로 분포할 수도 있음.&lt;/li&gt;
      &lt;li&gt;이런 것들을 어떻게 잘 정의할 수 있느냐에 대한 approach인 듯&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Gradient-based meta-learning with learned layerwise metric and subspace
    &lt;ul&gt;
      &lt;li&gt;그래디언트 포인트들이 마구잡이로 갈 수 있지만, optimum 포인트들이 분포해있는 subspace가 있을 수 있음.&lt;/li&gt;
      &lt;li&gt;subspace 안에서만 그래디언트를 가면 훨씬 더 빨리 갈 수 있음. 그것 관련한 논문&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ICML 2019 : Online Meta-Learning (같은 저자!)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;(References for your additional studies)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=fxJXXKZb-ik&quot;&gt;https://www.youtube.com/watch?v=fxJXXKZb-ik&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://talkingaboutme.tistory.com/entry/DL-Meta-Learning-Learning-to-Learn-Fast&quot;&gt;https://talkingaboutme.tistory.com/entry/DL-Meta-Learning-Learning-to-Learn-Fast&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://elapser.github.io/machine-learning/2019/03/08/Model-Agnostic-Interpretation.html&quot;&gt;https://elapser.github.io/machine-learning/2019/03/08/Model-Agnostic-Interpretation.html&lt;/a&gt;&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Meta Learning" /><summary type="html">Model-Agnostic Meta-Learning for fast adaptation of deep networks 를 읽고 정리한 글입니다.</summary></entry><entry><title type="html">Attention Is All You Need</title><link href="http://localhost:4000/paper%20review/2020/08/10/Attention/" rel="alternate" type="text/html" title="Attention Is All You Need" /><published>2020-08-10T00:00:00+09:00</published><updated>2020-08-10T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/08/10/Attention</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/08/10/Attention/">&lt;p&gt;Attention Is All You Need 를 읽고 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;attention-is-all-you-need&quot;&gt;Attention Is All You Need&lt;/h1&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;(You should include contents of summary and introduction.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;recurrent, convolution 을 사용하지 않고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Attention&lt;/code&gt;을 이용해서 더 빠른 성능에 도달&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-work-basic-concepts&quot;&gt;Related work (Basic concepts)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;layer normalization&lt;/li&gt;
  &lt;li&gt;RNN&lt;/li&gt;
  &lt;li&gt;Attention&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;
&lt;p&gt;(Explain one of the methods that the thesis used.)&lt;/p&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;&lt;img width=&quot;480&quot; alt=&quot;스크린샷 2020-08-10 오후 12 03 51&quot; src=&quot;https://user-images.githubusercontent.com/48315997/89760462-69980780-db27-11ea-81a1-77e7881cf135.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;the encoder maps an input sequence of symbol representations (x1,…,xn) to a sequence of continuous representations z = (z1,…,zn)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;6개의 identicla layer로 구성되어 있음.
        &lt;ul&gt;
          &lt;li&gt;each layer has 2 sub-layers.
            &lt;ul&gt;
              &lt;li&gt;&lt;em&gt;The first is a multi-head self-attention mechanism,&lt;/em&gt; and &lt;em&gt;the second is a simple, position- wise fully connected feed-forward network.&lt;/em&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Add : residual connection을 의미함.&lt;/li&gt;
      &lt;li&gt;Layer Norm. : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LayerNorm(x + Sublayer(x))&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;produce outputs of dimension d_model = &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;512&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Encoder와 동일하게 6개의 동일한 레이어로 구성됨.&lt;/li&gt;
      &lt;li&gt;Masked Multi-Head Attention
        &lt;blockquote&gt;
          &lt;p&gt;We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. &lt;strong&gt;This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.&lt;/strong&gt;&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;attention&quot;&gt;Attention&lt;/h3&gt;

&lt;p&gt;&lt;img width=&quot;680&quot; alt=&quot;스크린샷 2020-08-10 오후 12 11 26&quot; src=&quot;https://user-images.githubusercontent.com/48315997/89760471-6dc42500-db27-11ea-98ad-348e14e7cd30.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Scaled Dot-Product Attention&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Input : Q(Queries), K(Keys), V(Values)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;compute the dot products query with all keys&lt;/strong&gt;, divide each by root(dk), and apply a softmax function to obtain the weights on the values.
  &lt;img width=&quot;332&quot; alt=&quot;스크린샷 2020-08-10 오후 12 13 01&quot; src=&quot;https://user-images.githubusercontent.com/48315997/89760492-7b79aa80-db27-11ea-8bb9-0b99805583e1.png&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;blockquote&gt;
      &lt;p&gt;We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Multi-Head Attention&lt;/strong&gt;
    &lt;blockquote&gt;
      &lt;p&gt;linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively.&lt;/p&gt;
    &lt;/blockquote&gt;
    &lt;ul&gt;
      &lt;li&gt;perform the attention function in parallel yielding dv -dimensional output values.&lt;/li&gt;
      &lt;li&gt;위의 과정이 완료된 후에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;concatenate&lt;/code&gt; 시킴
  &lt;img width=&quot;487&quot; alt=&quot;스크린샷 2020-08-10 오후 12 14 53&quot; src=&quot;https://user-images.githubusercontent.com/48315997/89760505-83d1e580-db27-11ea-9402-fb704b743ab5.png&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;position-wise-feed-forward-networks&quot;&gt;Position-wise Feed-Forward Networks&lt;/h3&gt;

&lt;p&gt;&lt;img width=&quot;303&quot; alt=&quot;스크린샷 2020-08-10 오후 12 15 47&quot; src=&quot;https://user-images.githubusercontent.com/48315997/89760526-8c2a2080-db27-11ea-971a-627a7fe808a4.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;positional-encoding&quot;&gt;Positional Encoding&lt;/h3&gt;

&lt;p&gt;Transformer에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;시간의 연속성&lt;/code&gt;을 모델의 핵심부에서 다루지 않음. -&amp;gt; 그러나 시간의 순서는 실제 언어에서 중요하므로 단어의 위치 정보를 포함시키기 위해 Positional Encoding을 사용&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;342&quot; alt=&quot;스크린샷 2020-08-10 오후 12 19 13&quot; src=&quot;https://user-images.githubusercontent.com/48315997/89760545-951af200-db27-11ea-8b7d-07338663b53d.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;695&quot; alt=&quot;스크린샷 2020-08-10 오후 12 19 18&quot; src=&quot;https://user-images.githubusercontent.com/48315997/89760549-96e4b580-db27-11ea-817b-da7c7a09f7d9.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;why-self-attention&quot;&gt;Why Self-Attention&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Total computational complexity per layer&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The amount of computation that can be &lt;strong&gt;parallelized, as measured by the minimum number of sequential operations required.&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The path length between long-range dependencies in the network.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A self-attention layer connects all positions with &lt;strong&gt;a constant number&lt;/strong&gt; of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;논문 참조.&lt;/p&gt;

&lt;h2 id=&quot;additional-studies&quot;&gt;Additional studies&lt;/h2&gt;
&lt;p&gt;(If you have some parts that cannot understand, you have to do additional studies for them. It’s optional.)&lt;/p&gt;

&lt;p&gt;[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.&lt;/p&gt;

&lt;p&gt;[2] BERT&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;(References for your additional studies)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=EyXehqvkfF0&quot;&gt;https://www.youtube.com/watch?v=EyXehqvkfF0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=mxGCEWOxfe8&quot;&gt;https://www.youtube.com/watch?v=mxGCEWOxfe8&lt;/a&gt;&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="NLP" /><summary type="html">Attention Is All You Need 를 읽고 정리한 글입니다.</summary></entry><entry><title type="html">Generative Adversarial Nets</title><link href="http://localhost:4000/paper%20review/2020/08/08/GAN/" rel="alternate" type="text/html" title="Generative Adversarial Nets" /><published>2020-08-08T00:00:00+09:00</published><updated>2020-08-08T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/08/08/GAN</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/08/08/GAN/">&lt;p&gt;Generative Adversarial Nets 를 읽고 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;generative-adversarial-nets&quot;&gt;Generative Adversarial Nets&lt;/h1&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;(You should include contents of summary and introduction.)&lt;/p&gt;

&lt;p&gt;GAN에는 두 가지 모델이 존재함.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Discriminator&lt;/li&gt;
  &lt;li&gt;Generator&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Image를 만들어 내는 Generator(G)가 이 만들어진 모델을 평가하는 Discriminator(D)를 최대한 속일 수 있도록, 확률 분포의 차이를 줄이는 것이 목적&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;즉, G는 D를 최대한 속이려고 노력하고, D는 G가 만든 이미지를 최대한 감별하려고 노력함.&lt;/li&gt;
  &lt;li&gt;이 경쟁 속에서 두 모델은 모두 발전하게 되고, 결과적으로는 G가 만든 이미지를 구별할 수 없는 상태에 도달하게 됨.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/89702019-b5a44a00-d977-11ea-971b-f3e800d5ac7b.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위의 목표를 이루기 위해서는, &lt;em&gt;(ref. output -&amp;gt; [0,1] : 0==false, 1==true)&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;D : 진짜 이미지를 진짜 이미지라고 인식(분류)하도록 학습&lt;/li&gt;
  &lt;li&gt;G : random한 코드를 받아서 img를 생성한 후, 그 이미지가 D를 속여야 함.
    &lt;ul&gt;
      &lt;li&gt;즉, D(G(z)) = 1(진짜라 인식)이 나오도록 학습.
        &lt;ul&gt;
          &lt;li&gt;학습할수록 진짜같은 가짜 img가 만들어지는 것&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-work-basic-concepts&quot;&gt;Related work (Basic concepts)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;generative model&lt;/li&gt;
  &lt;li&gt;Adversarial&lt;/li&gt;
  &lt;li&gt;VAE&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;
&lt;p&gt;(Explain one of the methods that the thesis used.)&lt;/p&gt;

&lt;h3 id=&quot;gan-lossobjective-function&quot;&gt;GAN loss/objective function&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/89702021-b806a400-d977-11ea-87ad-6f12b78a0939.png&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;D 입장에서는 위 수식이 0인게 Maximize&lt;/li&gt;
  &lt;li&gt;G 입장에서는 속이는 게 좋으니 Mininmize&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;+) G는 처음에 형편없는 이미지를 만듦.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;D는 그 이미지를 가짜라 확신. =&amp;gt; D(G(z))=0&lt;/li&gt;
  &lt;li&gt;하지만 위의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log(1-x)&lt;/code&gt; 로는 그때 기울기의 절댓값이 작음.&lt;/li&gt;
  &lt;li&gt;practical use : D가 가짜라 확신하는 상황을 최대한 빨리 벗어나려면, D(G(z))=0인 점에서 기울기가 거의 무한인 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log(x)&lt;/code&gt;를 씀&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/89702023-b937d100-d977-11ea-97d7-1245192f4975.png&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모델이 생성한 이미지 분포와 실제 이미지 분포 간의 차이를 계산해주는 함수로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Jenson-Shannon divergence&lt;/code&gt; 사용함.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;approach&quot;&gt;Approach&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;The minimax problem of GAN has a global opt. at p(g) = p(data)&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Proposition 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/89702024-bb9a2b00-d977-11ea-99ab-a850c92cf5f6.png&quot; alt=&quot;4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/89702026-bd63ee80-d977-11ea-9d89-6f2b99c6b7f4.png&quot; alt=&quot;5&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Main Theorem.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위를 이용해서 D가 optimal 가정.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The global minimum of the virtual training criterion C(G) is achieved if and only if p(g)=p(data). 
At that point, C(G) achieves the value −log(4).
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/89702260-6b709800-d97a-11ea-9553-fc2ccaddfea2.jpeg&quot; alt=&quot;IMG_51B0698FF18C-1&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The proposed algorithm can find that global opt.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;그래서 알고리즘이 위 문제를 풀 수 있는가를 확인&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/89702027-bf2db200-d977-11ea-85e9-4a4a6685a0da.png&quot; alt=&quot;6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1번==&amp;gt;minimax problem -&amp;gt; global opt. 가진다는 증명이었음.&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;global opt. -&amp;gt; 모델의 분포 == 실제 분포&lt;/li&gt;
  &lt;li&gt;즉 우리가 풀려는 문제 C(G)가 convex문제임을 확인했음.
    &lt;ul&gt;
      &lt;li&gt;minimization problem이 쉬워짐.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;MLP로 충분히 가능하다.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;vector-arithmetic-하다&quot;&gt;Vector arithmetic 하다.&lt;/h3&gt;

&lt;p&gt;안경 낀 남자 - 안경 안 낀 남자 + 안경 안 낀 여자 = 안경 낀 여자&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/generative_adversarial_network/main.py
&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;save_image&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# Device configuration
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda'&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_available&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'cpu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Hyper-parameters
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latent_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;image_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;784&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sample_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'samples'&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Create a directory if not exists
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;makedirs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Image processing
# transform = transforms.Compose([
#                 transforms.ToTensor(),
#                 transforms.Normalize(mean=(0.5, 0.5, 0.5),   # 3 for RGB channels
#                                      std=(0.5, 0.5, 0.5))])
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Compose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# 1 for greyscale channels
&lt;/span&gt;                                     &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# MNIST dataset
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchvision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MNIST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'../../data/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                   &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                   &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                   &lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Data loader
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_loader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                          &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                          &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Discriminator
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LeakyReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LeakyReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Generator 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latent_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Device setting
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Binary cross entropy loss and optimizer
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BCELoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;d_optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0002&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g_optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0002&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;denorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;reset_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;d_optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g_optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Start training
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Create the labels which are later used as input for the BCE loss
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;real_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fake_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# ================================================================== #
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;#                      Train the discriminator                       #
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# ================================================================== #
&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# Second term of the loss is always zero since real_labels == 1
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;d_loss_real&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;real_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;real_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Compute BCELoss using fake images
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# First term of the loss is always zero since fake_labels == 0
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latent_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fake_images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fake_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;d_loss_fake&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fake_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fake_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Backprop and optimize
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;d_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_loss_real&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_loss_fake&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;reset_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;d_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;d_optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# ================================================================== #
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;#                        Train the generator                         #
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# ================================================================== #
&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Compute loss with fake images
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latent_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fake_images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fake_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;g_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;real_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Backprop and optimize
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;reset_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g_optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}'&lt;/span&gt; 
                  &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; 
                          &lt;span class=&quot;n&quot;&gt;real_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fake_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()))&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Save real images
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;save_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;denorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'real_images.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Save sampled images
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;fake_images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fake_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fake_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;save_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;denorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fake_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'fake_images-{}.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Save the model checkpoints 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'G.ckpt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'D.ckpt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;additional-studies&quot;&gt;Additional studies&lt;/h2&gt;
&lt;p&gt;(If you have some parts that cannot understand, you have to do additional studies for them. It’s optional.)&lt;/p&gt;

&lt;p&gt;이후 GAN 논문들&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;(References for your additional studies)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=L3hz57whyNw&quot;&gt;https://www.youtube.com/watch?v=L3hz57whyNw&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=odpjk7_tGY0&quot;&gt;https://www.youtube.com/watch?v=odpjk7_tGY0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://jaejunyoo.blogspot.com/2017/01/generative-adversarial-nets-2.html&quot;&gt;http://jaejunyoo.blogspot.com/2017/01/generative-adversarial-nets-2.html&lt;/a&gt;&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Computer Vision" /><category term="GAN" /><category term="Generative Model" /><summary type="html">Generative Adversarial Nets 를 읽고 정리한 글입니다.</summary></entry><entry><title type="html">PEGASUS:Pre-training with Extracted Gap-sentences for Abstractive Summarization</title><link href="http://localhost:4000/paper%20review/2020/08/05/PEGASUS/" rel="alternate" type="text/html" title="PEGASUS:Pre-training with Extracted Gap-sentences for Abstractive Summarization" /><published>2020-08-05T00:00:00+09:00</published><updated>2020-08-05T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/08/05/PEGASUS</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/08/05/PEGASUS/">&lt;p&gt;PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization 를 읽고 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;pegasus-pre-training-with-extracted-gap-sentences-for-abstractive-summarization&quot;&gt;PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization&lt;/h1&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;What is the good pre-training objectives tailored for abstractive text summarization?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;–&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GSG&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;self- supervised objective&lt;/li&gt;
  &lt;li&gt;important sentences are removed/masked from an input doc- ument and are generated together as one output sequence from the remaining sentences, similar to an extractive summary.
    &lt;ul&gt;
      &lt;li&gt;중요한 문장은 인풋 과정에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;masked&lt;/code&gt;되고, 남은 문장들 중에서 extractive summary처럼 가져오는 건가 봄.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;scoring : ROUGE&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GSG&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MLM&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-work-basic-concepts&quot;&gt;Related Work (Basic Concepts)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;논문 참조&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/89368717-2b5dab00-d717-11ea-9500-dff52b43c1ad.png&quot; alt=&quot;스크린샷 2020-08-05 오후 12 21 15&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;gap-sentences-generationgsg&quot;&gt;Gap Sentences Generation(GSG)&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;our proposed pre-training objec- tive involves generating summary-like text from an input document. In order to leverage massive text corpora for pre- training, we design a sequence-to-sequence self-supervised objective in the absence of abstactive summaries.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;we select and mask whole sentences from documents, and concatenate the gap-sentences into a pseudo-summary. The corresponding position of each selected gap sentence is replaced by a mask token [MASK1] to inform the model. Gap sentences ratio, or GSR, refers to the number of selected gap sentences to the total number of sentences in the document, which is similar to mask rate in other works.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;워드를 마스킹하는 게 아니라 센텐스 단위로 마스킹
    &lt;ul&gt;
      &lt;li&gt;이것을 트랜스포머 모델에 넣고 마스킹한 문장을 추론하는 태스크 수행&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;어떻게 하면 중요한 센텐스를 선택해서 마스킹할 수 있는가?
    &lt;ul&gt;
      &lt;li&gt;ROUGE1-F1 score 사용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;rouge&quot;&gt;ROUGE&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;ROUGE + F1 score&lt;/li&gt;
  &lt;li&gt;정답 센텐스가 있다고 가정하는 것
    &lt;ul&gt;
      &lt;li&gt;pretraining에 활용하고 싶은 것이기 때문에 정답 센텐스가 없음&lt;/li&gt;
      &lt;li&gt;셀렉트 센텐스와 나머지 센텐스를 비교해서 ROUGE1-F1 스코어를 구함.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;기존의 ROUGE 정의와는 약간 다르다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/89368734-357fa980-d717-11ea-9c8a-2cdc2794e532.png&quot; alt=&quot;스크린샷 2020-08-05 오후 12 24 22&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;masked-language-modelmlm&quot;&gt;Masked Language Model(MLM)&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;Following BERT, we select 15% tokens in the input text, and the selected tokens are (1) 80% of time replaced by a mask token [MASK2], or (2) 10% of time replaced by a random token, or (3) 10% of time unchanged.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;랜덤하게 특정 단어를 masking 시킴.
    &lt;ul&gt;
      &lt;li&gt;인풋을 넣어줄 때 이 단어들을 넣어주지 않는다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;마스킹된 단어가 무엇인지 추측하는 …
-&amp;gt; Can get gnereal linguistic knowledge
-&amp;gt; labeling, data를 가질 필요없이 모든 데이터로 사용할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pre-training-each-corpus&quot;&gt;Pre-training each corpus&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;C4&lt;/li&gt;
  &lt;li&gt;HugeNews&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fine-tuning-datasets&quot;&gt;Fine-tuning datasets&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;XSum&lt;/li&gt;
  &lt;li&gt;CNN/DailyMail&lt;/li&gt;
  &lt;li&gt;NEWSROOM&lt;/li&gt;
  &lt;li&gt;Multi-News&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;tfds에서 가져옴.&lt;/p&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Pre-training ablation experiments to choices of pre-training corpus, objective, and vocabulary size Using PEGASUS_Base instead of PEGASUS_Large&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ind-Orig&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;masking : 30%&lt;/li&gt;
  &lt;li&gt;Unigram 96K&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Larger Model Results
    &lt;ul&gt;
      &lt;li&gt;base모델로 찾은 다음에 large모델을 생성&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Fine-tuning with low-resource
    &lt;ul&gt;
      &lt;li&gt;적은 데이터셋에 대해서도 성능이 좋다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Qualitative Observations
    &lt;ul&gt;
      &lt;li&gt;사람이 봤을 때 진짜 좋은 성능인지 확인&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;실험결과 -&amp;gt; 실제로는 GSG 방식만 사용하는 것이 더 좋다.&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="NLP" /><category term="Text Summarization" /><summary type="html">PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization 를 읽고 정리한 글입니다.</summary></entry></feed>