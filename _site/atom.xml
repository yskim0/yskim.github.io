<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-12-25T21:58:36+09:00</updated><id>http://localhost:4000/atom.xml</id><title type="html">Yeonsoo Kim’s blog</title><author><name>Yeonsoo Kim</name></author><entry><title type="html">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)</title><link href="http://localhost:4000/paper%20review/2020/12/25/DCGAN/" rel="alternate" type="text/html" title="Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)" /><published>2020-12-25T00:00:00+09:00</published><updated>2020-12-25T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/12/25/DCGAN</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/12/25/DCGAN/">&lt;p&gt;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)을 읽고 개인적으로 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;unsupervised-representation-learning-with-deep-convolutional-generative-adversarial-networks-dcgan&quot;&gt;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;GAN은 Representation Learning에 효과적이라고 합니다. (본 논문에서는 이것이 GAN의 learning process와 관련이 있고, lack of heuristic cost function 덕분이라고 말합니다.)&lt;/p&gt;

&lt;p&gt;그럼에도 불구하고 처음 나온 GAN은 학습하는 것에 있어 매우 &lt;strong&gt;unstable&lt;/strong&gt;하다는 문제점을 가지고 있었습니다.&lt;/p&gt;

&lt;p&gt;본 논문의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DCGAN&lt;/code&gt;은 해당 문제점 해결을 포함하여 총 4가지의 Contributions로 정리할 수 있습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;DCGAN은 stable training이 가능하다&lt;/li&gt;
  &lt;li&gt;학습된 Discriminator는 image classification 태스크에 사용이 가능하다. (다른 unsupervised 알고리즘들과 비교할 것임.)&lt;/li&gt;
  &lt;li&gt;GAN이 학습한 filters를 시각화할 수 있고, 특정 오브젝트에 대해 특정 filter를 학습했다는 점을 보여줄 수 있다.&lt;/li&gt;
  &lt;li&gt;Generator는 vector arithmetic properties를 가지고 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;approach-and-model-architecture&quot;&gt;APPROACH AND MODEL ARCHITECTURE&lt;/h2&gt;

&lt;p&gt;&lt;img width=&quot;705&quot; alt=&quot;스크린샷 2020-12-25 오후 5 31 03&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103127996-f73f5280-46d6-11eb-9936-996a9ae0e84f.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우선 DCGAN이 기존의 GAN과 architecture 측면에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;어떻게&lt;/code&gt; 달랐기에 큰 성과를 이룰 수 있었는지 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;DCGAN이 나오기 전까지에도 CNN을 이용한 GAN을 만드는 시도는 계속 있었습니다. 하지만 이 시도들은 모두 성공적이지 못했죠.&lt;/p&gt;

&lt;p&gt;DCGAN은 &lt;strong&gt;CNN Architecture에서의 최신 변화(?) 3가지를 적용&lt;/strong&gt;하여 성공하였습니다.&lt;/p&gt;

&lt;h3 id=&quot;all-convolutional-net-사용&quot;&gt;All Convolutional Net 사용&lt;/h3&gt;

&lt;p&gt;우선 가장 큰 변화점은 &lt;a href=&quot;https://arxiv.org/abs/1412.6806&quot;&gt;Striving for Simplicity: The All Convolutional Net&lt;/a&gt; 을 참고하여 All Convoultional Net을 사용했다는 점입니다.&lt;/p&gt;

&lt;p&gt;All conv. net은 &lt;strong&gt;pooling functions(예를 들어, max pooling)를 strided conv. 으로 바꾼 네트워크 구조&lt;/strong&gt;입니다. Max pooling은 미분되지 않는 성질을 가진다고 합니다.&lt;/p&gt;

&lt;p&gt;이 네트워크 구조를 통해 Generator와 Discriminator 모두 자신들의 spatial downsampling을 학습하기에 적합해집니다.&lt;/p&gt;

&lt;h3 id=&quot;eliminating-fully-connected-layersfc-layers-on-top-of-convolutional-features&quot;&gt;Eliminating fully connected layers(FC layers) on top of convolutional features&lt;/h3&gt;

&lt;p&gt;이 시점 trend는 마지막에 FC layer를 제거하고, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;global average pooling&lt;/code&gt;를 쓰는 방식이었다고 합니다.&lt;/p&gt;

&lt;p&gt;다만, 이 global average pooling은 &lt;strong&gt;모델의 안정성은 올리는 반면에 convergence speed는 떨어뜨리는&lt;/strong&gt; trade-off 관계를 가지고 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;batch-norm--relu-activation&quot;&gt;Batch Norm &amp;amp; ReLU activation&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Batch Normalization&lt;/strong&gt;는 학습을 안정화시킬 수 있는 방법 중 하나입니다. (normalizing the input to each unit to have zero mean and unit variance)&lt;/p&gt;

&lt;p&gt;이것은 학습 문제점 중 하나인 initialization과 deep model의 gradient flow에 큰 도움을 줄 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;하지만, 모든 layer에 BN을 적용시키는 것은 아닙니다.&lt;/strong&gt; Generator의 output layer와 Discriminator의 input layer에는 BN을 적용시키지 않습니다.&lt;/p&gt;

&lt;p&gt;마지막으로 &lt;strong&gt;ReLU를 사용했다&lt;/strong&gt;는 점이 언급되어 있는데, Generator와 Discriminator에 적용되는 function이 약간 다릅니다.&lt;/p&gt;

&lt;p&gt;Generator에는 기본적으로 ReLU를 사용하지만, output layer에는 따로 ReLU가 아닌 Tanh function을 적용시킵니다.&lt;/p&gt;

&lt;p&gt;Discriminator에는 Leaky ReLU를 적용합니다.&lt;/p&gt;

&lt;p&gt;(여러 실험 결과 이 functions들이 좋은 성능을 냈기 때문이겠죠? 확실한 이유, 논증은 모르겠습니다.)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;details-of-adversarial-training&quot;&gt;DETAILS OF ADVERSARIAL TRAINING&lt;/h2&gt;

&lt;p&gt;DCGAN은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LSUN, FACES, IMAGENET-1K&lt;/code&gt; 데이터셋에 대하여 학습하였습니다.&lt;/p&gt;

&lt;p&gt;training setting(parameters, …)에 대해서는 논문을 참고해주시고, 위 데이터셋 중 LSUN에 대해서만 알아보겠습니다 :)&lt;/p&gt;

&lt;h3 id=&quot;lsun&quot;&gt;LSUN&lt;/h3&gt;

&lt;p&gt;LSUN은 &lt;em&gt;Large-scale Scene Understanding&lt;/em&gt;의 줄임말로, bedroom 사진들을 모은 데이터 셋입니다.&lt;/p&gt;

&lt;p&gt;이 데이터셋을 가지고 학습시킨 모델로 생성된 이미지는 quality가 상당히 향상되었습니다.&lt;/p&gt;

&lt;p&gt;하지만 이것이 &lt;strong&gt;over-fitting이 되어 이렇게 된 것인지, 학습 데이터셋에 대하여 기억(memorization)하여 만들어진 것인지&lt;/strong&gt;에 대해 판별해봐야 합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;overfitting?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;700&quot; alt=&quot;스크린샷 2020-12-25 오후 5 45 38&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103128506-00c9ba00-46d9-11eb-8b7a-e144208bbfe8.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;본 논문의 Fig.2 와 Fig.3을 통해 모델이 오히려 &lt;strong&gt;underfitting&lt;/strong&gt; 되어있음을 얘기합니다.&lt;/p&gt;

&lt;p&gt;underfitting이 이루어졌다고 여기는 이유는 아직 noise texture가 눈에 보이기 때문이라고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(엄청 가까이 보지 않는 이상은 잘 느끼지 못하겠는데 말이죠.)&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;memorization?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;사실 이게 가장 중요한 부분이라고 볼 수 있습니다. 새로 generate 하는 image가 사실 학습 데이터에서 기억(memorize)하여 만들어진 것이라면, &lt;strong&gt;진정한 의미의 Generate가 아니니깐요.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이에 대해서 본 논문의 저자들은 memorize하는 가능성을 줄이기 위해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;image de-duplication process&lt;/code&gt;(중복 제거 프로세스)를 거칩니다.&lt;/p&gt;

&lt;p&gt;de-duplication을 하기 위해 autoencoder를 하나 만듭니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We fit a 3072-128-3072 de-noising dropout regularized RELU autoencoder on 32x32 downsampled center-crops of training examples.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;empirical-validation-of-dcgans-capabilities&quot;&gt;EMPIRICAL VALIDATION OF DCGANs CAPABILITIES&lt;/h2&gt;

&lt;p&gt;맨 처음에 이 논문의 contributions 중 하나로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;학습된 Discriminator는 image classification 태스크에 사용이 가능하다&lt;/code&gt; 라고 얘기했었죠?&lt;/p&gt;

&lt;p&gt;이 목차에서는 정말 DCGAN이 &lt;strong&gt;feature extractor&lt;/strong&gt;로써의 역할이 가능한가, 그래서 CIFAR-10 데이터셋에 대해서도 &lt;strong&gt;classification task&lt;/strong&gt;를 잘 수행하는가를 확인해봅니다.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;696&quot; alt=&quot;스크린샷 2020-12-25 오후 6 03 36&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103129277-8484a600-46db-11eb-9e0f-de26d03694d4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결론적으로, 다른 unsupervised 알고리즘은 K-means model들보다 우수한 성능을 보였습니다. (Exemplar CNN 모델보다는 좀 못 미치지만요.)&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;542&quot; alt=&quot;스크린샷 2020-12-25 오후 6 05 55&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103129364-d6c5c700-46db-11eb-8f7b-388b5ae3128b.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;추가적으로 SVHN Digits 데이터셋에 대해서도 실험을 해보았습니다. test error 측면에서 SOTA를 달성하는 쾌거를 이루었습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;investigating-and-visualizing-the-internals-of-the-networks&quot;&gt;INVESTIGATING AND VISUALIZING THE INTERNALS Of THE NETWORKS&lt;/h2&gt;

&lt;p&gt;또 다른 contributions으로  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GAN이 학습한 filters를 시각화할 수 있고, 특정 오브젝트에 대해 특정 filter를 학습했다는 점을 보여줄 수 있다&lt;/code&gt; 와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Generator는 vector arithmetic properties를 가지고 있다&lt;/code&gt; 가 있었습니다.&lt;/p&gt;

&lt;p&gt;이 목차에서는 두 부분에 대해 설명할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;walking-in-the-latent-space&quot;&gt;Walking in the Latent Space&lt;/h3&gt;

&lt;p&gt;latent space를 변경했을 때 sharp transitions(급작스러운 변화)가 있으면 이는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memorization&lt;/code&gt;이 일어났다는 신호일 수 있습니다.&lt;/p&gt;

&lt;p&gt;반대로 부드러운 변화가 일어나면 memorization이 된 것이 아니라 제대로 학습이 되었다고 볼 수 있죠.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/103134579-83607280-46f5-11eb-800b-b4d5ea21bdad.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 사진을 보시면 DCGAN의 경우 sharp transition이 아닌 smooth한 변화가 이루어졌음을 볼 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;visualizing-the-discriminator-features&quot;&gt;Visualizing the Discriminator Features&lt;/h3&gt;

&lt;p&gt;이 내용에서는 Guided backpropagation을 통해 GAN이 학습한 filters를 시각화할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;696&quot; alt=&quot;스크린샷 2020-12-25 오후 9 17 38&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103134734-9f184880-46f6-11eb-9639-d0eb9878e74a.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;discriminator가 feature들을 학습해서, 특정 파트(bed, windows,…)들에 대하여 active 하고 있음을 볼 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;manipulating-the-generator-representation&quot;&gt;Manipulating the Generator Representation&lt;/h3&gt;

&lt;h4 id=&quot;forgetting-to-draw-certain-objects&quot;&gt;Forgetting to Draw Certain Objects&lt;/h4&gt;

&lt;p&gt;이건 매우 재미있는 실험입니다.&lt;/p&gt;

&lt;p&gt;간단하게 요약하자면, Generator가 무슨 representation을 학습했는지 알아보기 위하여 특정 filter(여기서는 window filter)를 삭제해봅니다.&lt;/p&gt;

&lt;p&gt;즉, Window라는 object를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Forget&lt;/code&gt; 하게 되는 것이죠.(기술적으로는 window filter를 dropout 시킨다고 말할 수 있습니다.)&lt;/p&gt;

&lt;p&gt;결과적으로 이 실험에서는 창문이 아닌 다른 representations, objects가 들어가게 됩니다!&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;684&quot; alt=&quot;스크린샷 2020-12-25 오후 9 22 03&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103134811-3da4a980-46f7-11eb-8d95-b990a0a18601.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 결과를 보면 아시다시피, 창문이었던 것이 문으로 바뀌는 등 다른 object를 생성합니다.&lt;/p&gt;

&lt;h4 id=&quot;vector-arithmetic-on-face-samples&quot;&gt;Vector Arithmetic On Face Samples&lt;/h4&gt;

&lt;p&gt;많은 분들이 가장 재밌어하실(?) 부분인 것 같습니다.&lt;/p&gt;

&lt;p&gt;word embedding 관련해서 vector(“King”) - vector(“Man”) + vector(“Woman”)가 vector(“Queen”)의 결과가 나오듯이, DCGAN에서도 이와 비슷한 &lt;strong&gt;arithmetic한 연산이 가능하다&lt;/strong&gt;고 밝혔습니다.&lt;/p&gt;

&lt;p&gt;Generator의 input인 Z vector에 대한 arithmetic operation을 하는데, single sample로는 불안정하여 3개의 Z vector를 평균한 값을 사용한다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;580&quot; alt=&quot;스크린샷 2020-12-25 오후 9 36 16&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103135086-3a122200-46f9-11eb-931d-7d737e8c5ed1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;smiling woman - neutral woman + neutral man = smiling man 이미지가 만들어지는 마법같은 기술을 보실 수 있습니다.&lt;/p&gt;

&lt;p&gt;앞서 말했다시피 3개의 Z vector를 average하여 새로운 Y벡터를 만든 것도 확인해볼 수 있죠.&lt;/p&gt;

&lt;p&gt;이게 다가 아닙니다. &lt;strong&gt;face pose&lt;/strong&gt; 또한 Z space에 선형적으로 모델링할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;557&quot; alt=&quot;스크린샷 2020-12-25 오후 9 38 01&quot; src=&quot;https://user-images.githubusercontent.com/48315997/103135120-780f4600-46f9-11eb-8d7e-8d723c10e439.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;바로 이렇게 말이죠. 
이미 이전부터 scale, rotation, position에 대하여 conditional generative model은 학습할 수 있다고 연구되어왔습니다. 하지만 이 연구는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;purely unsupervised model&lt;/code&gt; 이라는 점에서 큰 변환점이 된 것이죠.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;FUTURE WORK&lt;/h2&gt;

&lt;p&gt;사실 stablity를 완전히 해결한 것은 아닙니다.&lt;/p&gt;

&lt;p&gt;DCGAN을 오랫동안 학습하게 되면 collapse mode, oscillating mode가 발생할 수 있습니다. 아직도 &lt;strong&gt;불안정성&lt;/strong&gt;이 남은 것이죠.&lt;/p&gt;

&lt;p&gt;그래서 이 논문에서는 해당 문제점을 Future work로 남기고 마무리하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://jaejunyoo.blogspot.com/2017/02/deep-convolutional-gan-dcgan-1.html_&quot;&gt;Jaejun Yoo’s Playground&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;DCGAN도 읽었으니, 구현도 해보고 후속 논문들도 찬찬히 읽어보려고 합니다.
긴 글 읽어주셔서 감사합니다 :)&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="GAN" /><summary type="html">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)을 읽고 개인적으로 정리한 글입니다.</summary></entry><entry><title type="html">Multimodal Unsupervised Image-to-Image Translation (MUNIT)</title><link href="http://localhost:4000/paper%20review/2020/12/22/MUNIT/" rel="alternate" type="text/html" title="Multimodal Unsupervised Image-to-Image Translation (MUNIT)" /><published>2020-12-22T00:00:00+09:00</published><updated>2020-12-22T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/12/22/MUNIT</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/12/22/MUNIT/">&lt;p&gt;Multimodal Unsupervised Image-to-Image Translation (MUNIT) 을 읽고 간략하게 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;multimodal-unsupervised-image-to-image-translation-munit&quot;&gt;Multimodal Unsupervised Image-to-Image Translation (MUNIT)&lt;/h1&gt;

&lt;h2 id=&quot;keywords--gan-image-to-image-translation-style-transfer&quot;&gt;Keywords : GAN, Image-to-Image translation, style transfer&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;image representation = content code + style code
    &lt;ul&gt;
      &lt;li&gt;content code : domain invariant&lt;/li&gt;
      &lt;li&gt;style code : domain specific&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;657&quot; alt=&quot;스크린샷 2020-12-22 오전 12 14 37&quot; src=&quot;https://user-images.githubusercontent.com/48315997/102791653-adaaeb00-43ea-11eb-9dd6-fc7d4e6c6e6e.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Image Translation == Recombine its content code with a random style code
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;random style code&lt;/code&gt; : style space of the target domain&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;651&quot; alt=&quot;스크린샷 2020-12-22 오전 12 14 23&quot; src=&quot;https://user-images.githubusercontent.com/48315997/102791642-a5eb4680-43ea-11eb-8ea5-8af9ebc97f17.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;auto-encoder-architecture&quot;&gt;Auto-encoder Architecture&lt;/h3&gt;

&lt;p&gt;&lt;img width=&quot;624&quot; alt=&quot;스크린샷 2020-12-22 오전 12 19 33&quot; src=&quot;https://user-images.githubusercontent.com/48315997/102792116-61ac7600-43eb-11eb-8a7b-3f4cc735c0f1.png&quot; /&gt;&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="GAN" /><category term="Domain Adaptation" /><category term="Style Transfer" /><category term="Image-to-Image Translation" /><summary type="html">Multimodal Unsupervised Image-to-Image Translation (MUNIT) 을 읽고 간략하게 정리한 글입니다.</summary></entry><entry><title type="html">Unsupervised Intra-domain Adaptation for Semantic Segmentation</title><link href="http://localhost:4000/paper%20review/2020/12/22/intraDA/" rel="alternate" type="text/html" title="Unsupervised Intra-domain Adaptation for Semantic Segmentation" /><published>2020-12-22T00:00:00+09:00</published><updated>2020-12-22T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/12/22/intraDA</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/12/22/intraDA/">&lt;p&gt;Unsupervised Intra-domain Adaptation for Semantic Segmentation 을 읽고 간략하게 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;unsupervised-intra-domain-adaptation-for-semantic-segmentation&quot;&gt;Unsupervised Intra-domain Adaptation for Semantic Segmentation&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;automatically annotated data&lt;/code&gt; has a problem.
    &lt;ul&gt;
      &lt;li&gt;synthetic data -&amp;gt; real data&lt;/li&gt;
      &lt;li&gt;directly adapting models from the source data to the unlabeled target data (to reduce the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inter-domain gap&lt;/code&gt;)&lt;/li&gt;
      &lt;li&gt;But result? ==&amp;gt; bad :(
        &lt;ul&gt;
          &lt;li&gt;there is the large distribution gap among the target data itself(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intra-domain gap&lt;/code&gt;)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;p&gt;&lt;img width=&quot;300&quot; src=&quot;https://user-images.githubusercontent.com/48315997/102793576-7984f980-43ed-11eb-8cbe-ca0d2986d14e.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;595&quot; alt=&quot;스크린샷 2020-12-22 오전 12 44 17&quot; src=&quot;https://user-images.githubusercontent.com/48315997/102794504-d3d28a00-43ee-11eb-9b61-4eb50fde5d1d.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;two-step self-supervised domain adaptation approach to minimize the inter-domain and intra-domain gap together.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;dl&gt;
      &lt;dt&gt;inter-domain gap&lt;/dt&gt;
      &lt;dd&gt;separate the target domain into an easy &amp;amp; hard split (using entropy-based ranking function)&lt;/dd&gt;
    &lt;/dl&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;dl&gt;
      &lt;dt&gt;intra-domain gap&lt;/dt&gt;
      &lt;dd&gt;self-supervised adaption from the easy to hard split&lt;/dd&gt;
    &lt;/dl&gt;
    &lt;ul&gt;
      &lt;li&gt;segmentation predictions of easy split data (from G_inter) =&amp;gt; pseudo labels 로 사용&lt;/li&gt;
      &lt;li&gt;Given easy split data &amp;amp; pseudo labels, hard split data =&amp;gt; D_intra는 easy? hard? 판별&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;&lt;img width=&quot;698&quot; alt=&quot;스크린샷 2020-12-22 오전 12 46 37&quot; src=&quot;https://user-images.githubusercontent.com/48315997/102794749-26ac4180-43ef-11eb-853c-45c2e9a5cfd4.png&quot; /&gt;&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Domain Adaptation" /><category term="GAN" /><category term="Semantic Segmentation" /><summary type="html">Unsupervised Intra-domain Adaptation for Semantic Segmentation 을 읽고 간략하게 정리한 글입니다.</summary></entry><entry><title type="html">DETR:End-to-End Object Detection with Transformers</title><link href="http://localhost:4000/paper%20review/2020/11/17/DETR/" rel="alternate" type="text/html" title="DETR:End-to-End Object Detection with Transformers" /><published>2020-11-17T00:00:00+09:00</published><updated>2020-11-17T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/11/17/DETR</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/11/17/DETR/">&lt;p&gt;DETR : End-to-End Object Detection with Transformers 를 읽고 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;올해 가장 큰 인기를 몰고 있는 논문 중 하나인 DETR을 읽어보겠습니다.&lt;/p&gt;

&lt;p&gt;Yann Lecun 교수님께서 삼성 AI 포럼에서 DETR 논문을 언급하면서 Transformer + Vision 의 연구가 이제 주 과제일 것이라고 말씀하셨습니다. 
DETR은 나오자마자 큰 화제가 되었기에 살짝 읽어보고 리뷰는 안하려 했으나, ViT 논문을 읽고 저도 이 분야의 장래가 유망하다고 인식하게 되어 이렇게 하나하나 뜯어봤습니다!&lt;/p&gt;

&lt;p&gt;피드백은 언제나 환영합니다! 댓글창을 아직 열지 못해서 메일로 부탁드립니다. 꾸벅(__)&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 논문에서는 object detection을 하나의 &lt;strong&gt;direct set prediction&lt;/strong&gt; 문제로 본다.&lt;/p&gt;

&lt;p&gt;이 접근법은 &lt;strong&gt;Non-maximum suppression, anchor generation&lt;/strong&gt;과 같은 작업들을 사용하지 않는 방법으로, detection pipeline을 매우 간소화시킬 수 있다.&lt;/p&gt;

&lt;p&gt;논문에서 제안하는 모델은 DEtection Transformer(이하 DETR)이다. 이 모델의 주요 특징으로는 object detection을 set prediction problem으로 본다 하였으므로, &lt;strong&gt;set 기반의 global loss를 사용&lt;/strong&gt;한다는 점이다. Transformer encoder-decoder 구조를 사용하여 bipartite matching을 통해 unique prediction을 한다.&lt;/p&gt;

&lt;p&gt;학습된 object 쿼리들의 small set이 주어지면, DETR은 객체들과 전체 이미지 context 간의 관계에 대해 &lt;strong&gt;병렬적(parallel)&lt;/strong&gt; 으로 final set에 대한 예측을 directly output으로 내보낸다.&lt;/p&gt;

&lt;p&gt;이 모델은 library도 필요없고, 개념적으로 매우 심플하다.
COCO dataset에 대해서 Faster R-CNN baseline 급의 정확도와 런타임 성능을 보여줬다고 한다.&lt;/p&gt;

&lt;p&gt;게다가, DETR은 &lt;strong&gt;panoptic segmentation&lt;/strong&gt;을 쉽게 일반화할 수 있다!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Object detection의 목표는 바운딩 박스 set과 label을 맞추는 것이다.&lt;/p&gt;

&lt;p&gt;최근 대부분의 detector들은 이를 set of proposals, anchors, or window centers에 대해 regression/classification을 사용하는 ‘간접적인’방식으로 해결하려 한다.&lt;/p&gt;

&lt;p&gt;하지만 이런 방식들은 postprocessing 단계에서 중복된 예측값들을 collapse 시키는 과정이나(NMS를 얘기하는 듯하다), 앵커 셋에 대해 미리 assign하는 휴리스틱한 방식 등에 의해 performance가 영향 받을 수 있다.&lt;/p&gt;

&lt;p&gt;그래서 이 논문에서는 위와 같은 &lt;em&gt;Indirect&lt;/em&gt; 방식이 아닌 &lt;strong&gt;&lt;em&gt;Direct&lt;/em&gt;&lt;/strong&gt; 방식을 사용한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We propose a direct set prediction approach to bypass the surrogate tasks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이와 같은 end-to-end 방식은 기존의 object detection 연구에서는 자주 다뤄지지 않았던 것이다.&lt;/p&gt;

&lt;p&gt;Obejct detection 문제를 direct set prediction problem으로 만들어 training pipeline을 만든다.
이 때, transformer 기반의 인코더-디코더 구조를 사용한다. 트랜스포머의 self-attention 기법은 elements들 간의 모든 pairwise interactions를 통해 중복된 prediction을 제거할 수 있게 해준다.&lt;/p&gt;

&lt;p&gt;DETR은 모든 object들을 &lt;strong&gt;한번에&lt;/strong&gt; predict한다. 이는 end-to-end 특성인데, 실제값과 예측값 사이에서 set loss를 통해 bipartite matching(이분 매칭)을 훈련한다. 이로 인해 DETR은 hand-designed 작업 없이 간단한 detection 파이프라인을 가질 수 있는 것이다.&lt;/p&gt;

&lt;p&gt;다른 detection model들과 다르게 DETR은 customized layer도 필요하지 않아, 모델 자체도 굉장히 간단하다. CNN과 transformer만 가지고 있다면 쉽게 reproduce 할 수 있다.&lt;/p&gt;

&lt;p&gt;이전의 direct set prediction 연구들과 비교할 때, DETR의 주요점은 bipartite matching(이분 매칭)과 parallel decoding을 결합시킨 것이다. 
DETR의 Matching loss function은 예측값을 ground truth에 uniquely 할당할 수 있고, predicted objects의 순열이 변하지 않기 때문에 병렬적으로 predict할 수 있다.&lt;/p&gt;

&lt;p&gt;DETR은 COCO dataset에 evaluate 해봤을 때, Faster R-CNN baseline에 대하여 경쟁력 있는 결과를 보여주었다. 특히 큰 object들에 대하여 상당히 좋은 결과를 보여주었다고 한다. &lt;em&gt;하지만, small object들에 대해서는 low performance를 보였다.&lt;/em&gt; 이는 아마 Faster R-CNN에 FPN을 적용하여 좋은 성과를 이뤘듯이 future work에서도 해결법이 생기지 않을까 기대한다.&lt;/p&gt;

&lt;p&gt;DETR은 복잡한 문제를 쉽게 확장시킬 수 있다. 실험에서는 op pretrained DETR을 이용하여 segmentation head를 학습시켰더니 Panoptic Segmentation에 경쟁력을 가졌다고 한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Our work build on prior work in several domains: bipartite matching losses for
set prediction, encoder-decoder architectures based on the transformer, parallel
decoding, and object detection methods.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;the-detr-model&quot;&gt;The DETR model&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;set prediction을 위해 가장 중요한 요소는 2개이다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;a set prediction loss&lt;/strong&gt; that forces &lt;strong&gt;unique matching&lt;/strong&gt; between predicted and ground truth boxes&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;an architecture&lt;/strong&gt; that &lt;strong&gt;predicts&lt;/strong&gt; (in a single pass) a set of objects and models their relation&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;object-detection-set-prediction-loss&quot;&gt;Object detection set prediction loss&lt;/h2&gt;

&lt;p&gt;DETR은 fixed-size set에서 N개의 prediction을 한다. &lt;strong&gt;이 때 N은 이미지의 object 개수보다 상당히 크게 설정한다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ground truth에 대하여 predicted objects(class, position, size)를 scoring하는 것이 학습과정에서 어려웠다고 한다.
여기서 loss는 예측값과 ground truth 사이의 최적의 이분 매칭을 만들어 내고, 이를 통해 bounding box loss를 최적화한다.&lt;/p&gt;

&lt;p&gt;$y$를 ground truth, $\hat{y} = {\hat{y_i}}_{i=1}^N$ 은 N개의 prediction set이라 하자.&lt;/p&gt;

&lt;p&gt;N은 이미지내의 object 개수보다 크다고 가정하면, $y$도 a set of size N에 $\varnothing$ (no object)가 padded 됐다고 볼 수 있다.&lt;/p&gt;

&lt;p&gt;이러한 y, y_hat 사이의 bipartite matching(이는 ground truth와 prediction 사이의 일대일 매칭이라 생각하면 편할 듯) 찾기 위해서, matching cost가 minimum이 되는 바운딩 박스의 순서들, 즉 순열을 찾아야 한다.&lt;/p&gt;

&lt;p&gt;이러한 optimal assignent는 헝가리안 알고리즘에 의해 효율적으로 계산될 수 있음.&lt;/p&gt;

&lt;p&gt;이제 다시 matching cost에 대해 알아보자. each element of &lt;em&gt;i&lt;/em&gt; of the ground truth set은 $y_i = (c_i, b_i)$ 로 나타낼 수 있음. 이 때 c_i는 target class label, b_i는 box center coordinates(xy의 center, height, width) 임.&lt;/p&gt;

\[L_{Hungarian}(y,\hat{y} = \mathbb{-1}_{c_i\neq\varnothing} + \mathbb{1}L_{box}(b_i, \hat{b_{\sigma(i)}})\]

&lt;p&gt;앞의 항에 -1을 곱하는 이유는 해당 클래스 probability가 높을수록 loss를 낮게 산정하기 위함이고, 뒷 항은 bounding box의 로스를 크로스 엔트로피와 같은 과정없이 그대로 적용한다는 것이 조금 특이한 지점이다.&lt;/p&gt;

&lt;p&gt;여기서 이 논문의 차별점 혹은 중요한 지점은 중복(duplicates)없이 direct set prediction을 위한 일대일 매칭이 된다는 점이다. 즉, 중복되는 prediction이 존재하지 않다는 뜻. &lt;strong&gt;(일대일 매칭을 하기 때문에 중복되는 예측이 생기지 않음 -&amp;gt; NMS가 필요없다!!)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;다른 모델에서는 앵커박스를 많이 뽑아서 하나 물체에 여러 개의 바운딩 박스를 그려서 NMS(Non-maximum suppression)을 사용하는 방식이었음!!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이렇게 일대일 매칭된 prediction과 ground truth 사이에서는 위에서 언급했듯이 헝가리안 기반의 loss를 사용해서 학습함. (즉 방금했던 것은 matching cost를 계산하는 loss였고, 이번이 진짜 예측값과 Ground Truth사이의 loss인듯?)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99262847-c8c19280-2861-11eb-8c4a-ea37362777ae.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;L_match와 약간 비슷해보이지만, class 확률 값을 no-object 일 때도 계산한다는 점, log를 사용해 cross entropy같은 성질이 추가되었다는 점이 다르다.
클래스가 no_object일 때 앞의 항의 loss를 1/10으로 줄여 class imbalance를 조절했다고 한다.&lt;/p&gt;

&lt;p&gt;마지막으로 bounding box loss를 살펴보자. 다른 detector들은 미리 정해놓은 앵커 박스 candidate들을 얼마나 움직일지에 대한 &lt;strong&gt;델타&lt;/strong&gt;를 학습했다. 그러나 DETR에서는 바운딩 박스를 directly 예측한다. L1 Loss를 사용하는데, L1 loss의 단점은 바운딩 박스가 크면 작은 거에 비해 loss가 더 큼. (단순히 좌표의차이만을 고려하니까)&lt;/p&gt;

&lt;p&gt;이 단점을 보완하기 위해 Generalized IoU loss를 L1 Loss에 linear combination 했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99262815-c0695780-2861-11eb-8170-8b6271d35ddd.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;detr-architecture&quot;&gt;DETR Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99263078-11794b80-2862-11eb-9452-ca8b498d812c.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;[요약]&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CNN으로 feature map 생성&lt;/li&gt;
  &lt;li&gt;트랜스포머 인코더에 들어가기 위해서 positional encoding 더함&lt;/li&gt;
  &lt;li&gt;트랜스포머 인코더의 출력은 디코더에 들어감
    &lt;ul&gt;
      &lt;li&gt;이 값들이 &lt;strong&gt;key, value&lt;/strong&gt; 역할을 함.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;디코더의 input으로는 N개의 object &lt;strong&gt;query&lt;/strong&gt;가 들어간다.
    &lt;ul&gt;
      &lt;li&gt;이 쿼리도 학습하게 됨. 처음에는 랜덤값으로 채우지만, 점점 학습하는 것&lt;/li&gt;
      &lt;li&gt;일종의 positional encoding 역할을 함 (어느 부분을 관심있게 볼 것인지)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;각각 FFN을 통해서 N개를 예측하게 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;DETR의 전체적인 구조는 위 Figure과 같다. 크게 3가지 컴포넌트로 구성된다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;CNN backbone : CNN을 이용해 compact feature representation을 추출한다.(feature map)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Encoder-Decoder Transformer&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A simple Feed Forward Network(FFN) : 결과값 출력&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;DETR의 구조는 매우 간단해서 베이스라인 모델을 파이토치로 구현하면 50줄정도이다! (글 하단부에 첨부하겠음.)&lt;/p&gt;

&lt;h3 id=&quot;backbone&quot;&gt;Backbone&lt;/h3&gt;

&lt;p&gt;실험에서는 ResNet을 사용함. H, W는 5번 정도의 down-scale을 했다.(H0/32, W0/32)
최종 C = 2048로 사용(ResNet 모델의 마지막 레이어가 2048이기 때문)&lt;/p&gt;

&lt;p&gt;&lt;img width=&quot;609&quot; alt=&quot;스크린샷 2020-11-17 오전 12 10 58&quot; src=&quot;https://user-images.githubusercontent.com/48315997/99269155-61a7dc00-2869-11eb-9ee9-9d13ef280c23.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;transformer-encoder&quot;&gt;Transformer encoder&lt;/h3&gt;

&lt;p&gt;우선 1x1 conv.를 통해 채널을 $d$ dimension으로 줄인다.&lt;/p&gt;

&lt;p&gt;또 인코더에 들어가기 위해선 벡터로 바뀌어야 하니까 3차원을 2차원으로 바꿔야 한다. 
H,W를 다 합쳐서 spatial dimensions of d x HW로 만들어준다.
이 때 HW는 시퀀스의 개수가 되고, 각 시퀀스 벡터의 사이즈가 d가 되는 것이다.&lt;/p&gt;

&lt;p&gt;트랜스포머는 attention기반이기 때문에 permutation-invariant하다. 즉, 순서를 무시한다는 것이다.&lt;/p&gt;

&lt;p&gt;때문에, 순서를 부여하기 위해 fixed positional encoding을 input of each attention layer에 추가한다. 
&lt;em&gt;(참고) x, y축 따로 d/2씩 해서 인코딩해서 concat하는 식으로 positional encoding을 해준다.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;transformer-decoder&quot;&gt;Transformer decoder&lt;/h3&gt;

&lt;p&gt;transformer의 스탠다드 아키텍쳐를 따랐다고 한다.&lt;/p&gt;

&lt;p&gt;다만 original과의 차이점은 각 디코더 레이어에서 N개의 객체를 &lt;strong&gt;parallel하게 디코딩&lt;/strong&gt;한다는 것이다.&lt;/p&gt;

&lt;p&gt;디코더를 통과하게 되면 N개의 오브젝트가 parallel하게 나오는데, 이 때 디코더의입력으로 들어가는 것은 object queries이다.&lt;/p&gt;

&lt;p&gt;디코더 역시 &lt;strong&gt;permutation-invariant&lt;/strong&gt; 하기 때문에 일종의 positional encoding이 필요하다. (입력으로 들어가는 것들이 다 달라야 다른 결과를 뽑아줄 수 있기 때문이다)&lt;/p&gt;

&lt;p&gt;여기서 object query가 positional encoding 역할을 한다.
이 쿼리는 학습의 대상이 되며 each attention layer의 인풋으로 추가된다.&lt;/p&gt;

&lt;p&gt;이것들은 &lt;strong&gt;independently FFN을 지나&lt;/strong&gt; 박스 좌표, 클래스 라벨로 디코드된다. (그렇게 N개의 final prediction을 얻게 된다.)&lt;/p&gt;

&lt;p&gt;개인적으로 가장 중요한 부분이라고 생각하는게, 이러한 self- and encoder-decoder attention 모델은 image를 &lt;strong&gt;global 추론할 수 있게 해준다.&lt;/strong&gt; 즉 use the whole image as context로 사용할 수 있는 것이다!!&lt;/p&gt;

&lt;h3 id=&quot;prediction-feed-forward-networks-ffns&quot;&gt;Prediction feed-forward networks (FFNs)&lt;/h3&gt;

&lt;p&gt;3-layer 퍼셉트론 with ReLU 구조이다.&lt;/p&gt;

&lt;p&gt;이 FFN은 bounding box의 normalized center coordinates, height, width를 예측하며, softmax function을 통해 class label을 예측한다.&lt;/p&gt;

&lt;p&gt;no object 클래스는 background role을 하기도 한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다는 살펴보지 않고, 몇 개만 살펴볼 것임.&lt;/p&gt;

&lt;h2 id=&quot;comparison-with-faster-r-cnn&quot;&gt;Comparison with Faster R-CNN&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99273109-eb58a900-286b-11eb-9c47-c2bbeae303b1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;AP_S : Small scale images&lt;/p&gt;

&lt;p&gt;AP_L : Large scale images&lt;/p&gt;

&lt;p&gt;CNN은 local 특성을 잘 반영하므로 small img.들에 DETR보다 더 효과적이다.&lt;/p&gt;

&lt;p&gt;하지만 DETR은 Attention 기반이기 때문에 모든 position을 볼 수 있고, 때문에 Large img.에 매우 효과적이다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;그러나 최근 나온 논문 Deformable DETR에서 AP_S도 Faster R-CNN보다 높은 성과를 거두었다! (다음 논문은 이걸 읽어보려고 한다.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Attention이 global한 reason이 된다는 장점이 있기 때문에 panoptic segmentation에 잘 맞지 않을까하는 생각이 든다…)&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;number-of-encoder-layers&quot;&gt;Number of encoder layers&lt;/h2&gt;

&lt;p&gt;&lt;img width=&quot;644&quot; alt=&quot;스크린샷 2020-11-17 오전 12 33 56&quot; src=&quot;https://user-images.githubusercontent.com/48315997/99273597-95383580-286c-11eb-9501-87a86806a63f.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;인코더의 블럭수가 많을수록 AP는 올라간다.&lt;/p&gt;

&lt;p&gt;또한 &lt;strong&gt;Encoder는 disentangling&lt;/strong&gt;(이미지 분해, 해체) 역할에 아주중요한 역할을 한다는 것을 알 수 있었다고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99273849-e3e5cf80-286c-11eb-8f92-c330bddc9e51.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;image disentanglement 란 위의 사진과 같이 객체별로(인스탄스별로) 잘 분리가 됐음을 표현하는 듯하다.&lt;/p&gt;

&lt;p&gt;이렇게 객체별로 attention이 잘 나누어진다면 디코더에서 객체의 bounding box, class를 예측하는 것은 매우 쉽다고 한다.&lt;/p&gt;

&lt;h2 id=&quot;number-of-decoder-layers&quot;&gt;Number of decoder layers&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99274139-4dfe7480-286d-11eb-9d6d-e06ab83853f3.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이것은 decoder layer의 개수가 어느정도 되면, NMS를 쓸 때와 성능상 별 차이가 없다는 결과이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99274340-9322a680-286d-11eb-811d-ae3aeb96712f.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;개인적으로 정말 놀랐던 결과이다. 객체의 head, edge를 정말 잘 attention하고 있음을 알 수 있다. 
&lt;strong&gt;인코더는 global attention을 통해 인스턴스들을 분리한다면, 디코더는 클래스와 객체의 바운더리(가장자리)를 추출한다고 한다&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;panoptic-segmentation&quot;&gt;Panoptic Segmentation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99275057-60c57900-286e-11eb-87fa-4c29646eb60c.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;DETR은 object detection 문제를 direct set prediction으로 보았고, 이를 Transformer와 bipartite matching을 통한 end-to-end 방식으로 풀어냈다. (많은 연구자들이 모든 딥러닝 모델의 최종 지향점은 end-to-end라고 한다.)&lt;/p&gt;

&lt;p&gt;transformer 특성상, DETR 또한 architecture를 flexible 하게 확장시킬 수 있다.(Panoptic segmentation처럼)&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;code&quot;&gt;Code&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/99277842-ab94c000-2871-11eb-803f-d34e1bc3406b.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;additional-studies&quot;&gt;Additional studies&lt;/h1&gt;
&lt;p&gt;(If you have some parts that cannot understand, you have to do additional studies for them. It’s optional.)&lt;/p&gt;

&lt;p&gt;Deformable DETR&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;(References for your additional studies)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://keyog.tistory.com/32&quot;&gt;인간지능이 인공지능을 공부하는 장소&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kp1994.tistory.com/15&quot;&gt;KP’s blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=lXpBcW_I54U&amp;amp;feature=youtu.be&amp;amp;fbclid=IwAR25dYNnKxCsN5apneKrmPus-umovk7fziMedDQMqBfhg28eaBj6u-tRxzI&amp;amp;ab_channel=JinWonLee&quot;&gt;TF 논문 읽기 모임 PR-284&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Computer Vision" /><category term="Object Detection" /><category term="Encoder-Decoder" /><category term="Transformer" /><category term="Panoptic Segmentation" /><category term="DETR" /><category term="ViT" /><summary type="html">DETR : End-to-End Object Detection with Transformers 를 읽고 정리한 글입니다.</summary></entry><entry><title type="html">MMDetection 사용하기</title><link href="http://localhost:4000/computer%20vision/2020/11/03/mmdetection-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/" rel="alternate" type="text/html" title="MMDetection 사용하기" /><published>2020-11-03T00:00:00+09:00</published><updated>2020-11-03T00:00:00+09:00</updated><id>http://localhost:4000/computer%20vision/2020/11/03/mmdetection%20%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0</id><content type="html" xml:base="http://localhost:4000/computer%20vision/2020/11/03/mmdetection-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/">&lt;p&gt;Dacon의 K-Fashion AI 경진대회의 baseline 설명에 따라 &lt;strong&gt;MMdetection&lt;/strong&gt; toolkit을 설치해보고 학습까지 진행해보겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;저는 오늘부터 시작된 &lt;a href=&quot;https://dacon.io/competitions/official/235672/overview/&quot;&gt;K-Fashion AI 경진대회&lt;/a&gt;에 참가합니다.&lt;/p&gt;

&lt;p&gt;이 대회는 제가 관심있는 분야인 &lt;strong&gt;&lt;em&gt;컴퓨터 비전&lt;/em&gt;&lt;/strong&gt; 대회라 관심을 가지게 되었고, 실제로 CV 관련 대회는 처음이니 배운다는 마음가짐으로 열심히 공부해보려 합니다.&lt;/p&gt;

&lt;h1 id=&quot;mmdectection&quot;&gt;MMdectection?&lt;/h1&gt;

&lt;p&gt;이 대회의 베이스라인을 참고하면서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mmdetection&lt;/code&gt; 이라는 것을 처음 알게 되었습니다. 
논문도 있으니 시간 남을 때 봐야겠네요&lt;/p&gt;

&lt;p&gt;깃허브는 &lt;a href=&quot;https://github.com/open-mmlab/mmdetection&quot;&gt;여기&lt;/a&gt;에 들어가면 되고, 리드미 파일에 mmdetection이 어떤 건지 나와있습니다.
짧게 요약하자면,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;MMDetection&lt;/strong&gt; is an open source object detection toolbox based on PyTorch.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이라 하네요.&lt;/p&gt;

&lt;p&gt;object detection에서 다루는 다양한 모델들을 한 곳에 모아뒀다고 하니 정말 간편해보입니다. (이런 걸 이제 알게된 게 아쉬울 정도로…)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmdetection#benchmark-and-model-zoo&quot;&gt;여기&lt;/a&gt; 에 support 하는 모델들이 나열되어 있습니다.&lt;/p&gt;

&lt;p&gt;또한 버전 업데이트 및 유지보수도 굉장히 잘 되고 있다고 합니다!&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;installation&quot;&gt;Installation&lt;/h1&gt;

&lt;p&gt;데이콘 베이스 라인을 보고 따라하는 것이므로 버전도 맞춰서 해봅니다.
우선 mmdetection 깃허브 링크에 가서 브랜치를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v2.3.0&lt;/code&gt; 으로 이동합니다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmdetection/tree/v2.3.0&quot;&gt;바로 가기&lt;/a&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/97955665-0f3ada00-1dea-11eb-93d6-a233996a32f8.png&quot; alt=&quot;스크린샷 2020-11-03 오후 3 34 25&quot; /&gt;&lt;/p&gt;

&lt;p&gt;README.md 를 조금 내리다 보면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;install.md&lt;/code&gt; 로 넘어갈 수 있는 하이퍼링크가 있습니다.
그걸 눌러 install.md로 이동합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/97955738-3db8b500-1dea-11eb-9988-2a6afd19ec79.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/97955982-dbac7f80-1dea-11eb-9aa4-e7ff1a77c779.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt; 잘 확인하시고, install mmdetection 순서대로 진행합니다.&lt;/p&gt;

&lt;h3 id=&quot;1-가상환경-생성&quot;&gt;1. 가상환경 생성&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda create -n 가상환경이름 python=3.7 -y
conda activate 가상환경이름 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이렇게 가상 환경 생성하고 activate 시킵니다.&lt;/p&gt;

&lt;h3 id=&quot;2-pytorch-torchvision-설치-버전-주의&quot;&gt;2. pytorch, torchvision 설치 (버전 주의)&lt;/h3&gt;

&lt;p&gt;베이스라인 따라가려면 (즉, mmdetection version==2.3.0) torch version == &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1.5.0&lt;/code&gt; 이어야 합니다.
pytorch를 원하는 이전 버전으로 설치해야 할 때 공식홈페이지에 커맨드가 다 나와있습니다. &lt;a href=&quot;https://pytorch.org/get-started/previous-versions/&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda install pytorch==1.5.0 torchvision==0.6.0 -c pytorch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3-mmdetection-repo-clone&quot;&gt;3. MMdetection repo. clone&lt;/h3&gt;

&lt;p&gt;mmdetection 레포지터리를 clone 해야 합니다.
이 때도 이 도큐먼트와 다르게 주의해야 할 점이 있습니다. 앞서 말했듯이 저는 베이스라인 따라 branch == v2.3.0으로 이동했습니다. 이 브랜치에 해당되는 버전으로 git clone 해야 합니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone --branch v2.3.0 https://github.com/open-mmlab/mmdetection.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;4-requirements-설치&quot;&gt;4. Requirements 설치&lt;/h3&gt;

&lt;p&gt;우선 클론한 디렉토리로 갑니다.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd ./mmdetection
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;저와 같이 다른 사람들과 같이 쓰는 서버를 쓰시는 환경이라면 마음대로 패키지/라이버리를 다운 받았을 때 충돌될 가능성이 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;그래서 이 requirements 들을 가상 환경 내에 설치해야 합니다.&lt;/strong&gt;
현재 가상환경의 절대 주소를 모르시면, 터미널에&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda env list
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;를 치시면 가상환경 이름 뒤에 경로가 나옵니다.
그 path를 copy 해두세요.
가상환경의 경로를 {PATH}라고 가정하고 적어보겠습니다.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{PATH}/bin/pip install -r requirements/build.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이렇게 하면 requirements가 잘 설치될 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;5-set-up&quot;&gt;5. Set up&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install -v -e .  # or &quot;python setup.py develop&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;앞에 명령어 해보고 안되면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python setup.py develop&lt;/code&gt; 하시면 됩니다.&lt;/p&gt;

&lt;p&gt;(저 같은 경우는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install -v -e .&lt;/code&gt; 했을 때 안됐어서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python setup.py develop&lt;/code&gt; 명령어 해서 정상 설치 됐습니다.)&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;training-하기-전-준비-해야-할-것들&quot;&gt;Training 하기 전 준비 해야 할 것들&lt;/h1&gt;

&lt;h3 id=&quot;1-dataset-관련&quot;&gt;1. Dataset 관련&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mmdetection/mmdet/datasets&lt;/code&gt; 에서 데이터셋에 대한 기본 설정을 할 수 있습니다.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;custom.py&lt;/code&gt; 로 직접 personal 한 설정을 할 수 있겠지만 이 대회는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;coco.py&lt;/code&gt; 에서 설정된 것에서 크게 벗어나지 않아 이대로 사용해도 된다고 합니다.&lt;/p&gt;

&lt;p&gt;그래서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;coco.py&lt;/code&gt; 에서 미리 설정된 CLASSES 들을 이 대회에서 쓰일 클래스로 바꾸겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/97957150-c71db680-1ded-11eb-88e2-3b503374e352.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;또한, mmdetection의 경우 train/test 폴더가 한 곳에 flatten 하게 존재해야 한다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/train/a/*.jpg&lt;/code&gt; 이런 형태가 아닌 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/train/a_*.jpg&lt;/code&gt; 이런 식으로요.&lt;/p&gt;

&lt;p&gt;이건 파이썬 코드로 쉽게 수정 가능하니 잊지 않고 하시길 바립니다.&lt;/p&gt;

&lt;h3 id=&quot;2-training-options&quot;&gt;2. training options&lt;/h3&gt;

&lt;p&gt;트레이닝할 때 필요한 기본 옵션들을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mmdetection/config/_base_/default_runtime.py&lt;/code&gt; 에서 가능합니다.&lt;/p&gt;

&lt;p&gt;이 config는 우선 베이스라인 코드에서 제공한 것을 붙여 써서 지금 제 환경에 맞게 조금씩 고쳤습니다. (data root, gpu 개수, epoch 등)&lt;/p&gt;

&lt;p&gt;자세한 건 &lt;a href=&quot;https://github.com/dacon-ai/K-fashion-baseline/blob/master/configs/_base_/default_runtime.py&quot;&gt;여기&lt;/a&gt;를 참고해주세요.&lt;/p&gt;

&lt;p&gt;train/test set 경로 등등 잘 설정하고 나면 바로 학습 시작 가능합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;training-&quot;&gt;Training !&lt;/h1&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python tools/train.py configs/_base_/default_runtime.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이걸 치시면 config로 해놨던 설정들이 주루룩 뜨면서 학습이 시작됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/97957533-bf124680-1dee-11eb-93b9-a3b3921adbe6.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;troubleshooting&quot;&gt;Troubleshooting&lt;/h1&gt;

&lt;h3 id=&quot;attributeerror-coco-object-has-no-attribute-get_cat_ids&quot;&gt;AttributeError: ‘COCO’ object has no attribute ‘get_cat_ids’&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pycocotools&lt;/code&gt; 관련 오류입니다.&lt;/p&gt;

&lt;p&gt;저같은 경우에는&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{PATH}/bin/pip install &quot;git+https://github.com/open-mmlab/

cocoapi.git#subdirectory=pycocotools&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;로 해결했습니다.&lt;/p&gt;

&lt;h3 id=&quot;modulenotfounderror-no-module-named-mmcv_ext&quot;&gt;ModuleNotFoundError: No module named ‘mmcv._ext’&lt;/h3&gt;

&lt;p&gt;mmcv가 제대로 설치되지 않은 경우입니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{PATH}/bin pip uninstall mmcv
{PATH}/bin pip install mmcv-full==1.0.5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;저는 이렇게 해결했습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=UEu73ew7mSY&amp;amp;feature=youtu.be&amp;amp;ab_channel=%EB%8D%B0%EC%9D%B4%EC%BD%98&quot;&gt;데이콘 베이스라인&lt;/a&gt;&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Computer Vision" /><category term="MMdetection" /><category term="Mask R-CNN" /><summary type="html">Dacon의 K-Fashion AI 경진대회의 baseline 설명에 따라 MMdetection toolkit을 설치해보고 학습까지 진행해보겠습니다.</summary></entry><entry><title type="html">An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale(ViT)</title><link href="http://localhost:4000/paper%20review/2020/10/31/ViT/" rel="alternate" type="text/html" title="An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale(ViT)" /><published>2020-10-31T00:00:00+09:00</published><updated>2020-10-31T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/10/31/ViT</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/10/31/ViT/">&lt;p&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT) 를 읽고 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale&quot;&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/h1&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;image classification task에 기존의 transformer 모델을 이용한다.
    &lt;ul&gt;
      &lt;li&gt;transformer의 장점들을 사용할 수 있음.&lt;/li&gt;
      &lt;li&gt;simple, computational efficiency, scalability&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Vision Transformer(ViT)
    &lt;ul&gt;
      &lt;li&gt;원본 이미지를 &lt;strong&gt;patches&lt;/strong&gt;들로 split한다.
        &lt;ul&gt;
          &lt;li&gt;이 때 이미지 패치들을 NLP에서의 &lt;strong&gt;token(word)&lt;/strong&gt;와 같은 역할임.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;이 패치들의 sequence of linear embedding을 Transformer의 input으로 feed한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;크지않은 데이터 셋에서는 ResNet보다 약간 낮은 정확도
    &lt;ul&gt;
      &lt;li&gt;Transformer는 CNN과 다르게 &lt;em&gt;translation equivariance, locality&lt;/em&gt; 같은 &lt;strong&gt;inductive biases&lt;/strong&gt;(=weight sharing)이 없기 때문에 데이터셋이 적으면 generalize되기 어려움.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;하지만 large scale 데이터에서는 CNN의 inductive bias를 능가함.
    &lt;ul&gt;
      &lt;li&gt;image recognition benchmarks에서 여러 SOTA 달성함.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-work-basic-concepts&quot;&gt;Related work (Basic concepts)&lt;/h2&gt;

&lt;p&gt;제안한 모델이 최근 연구 중에서는 iGPT 논문과 유사하다고 함.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;reducing image resolution and color space한 후 transformer를 적용&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;
&lt;p&gt;(Explain one of the methods that the thesis used.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Model overview
  &lt;img width=&quot;777&quot; alt=&quot;스크린샷 2020-11-01 오전 11 51 34&quot; src=&quot;https://user-images.githubusercontent.com/48315997/97794098-97797d80-1c38-11eb-8221-30ea0e5d6ea7.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Vision Transformer(ViT)&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;image를 patch 단위로 잘라서 flatten시킨 후, 그것을 linear projection하여 encoder에 feed한다.
  &lt;img src=&quot;https://user-images.githubusercontent.com/48315997/97794316-8da54980-1c3b-11eb-963b-2c07da280861.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Equation
  &lt;img width=&quot;737&quot; alt=&quot;스크린샷 2020-11-01 오후 12 32 37&quot; src=&quot;https://user-images.githubusercontent.com/48315997/97794501-55ebd100-1c3e-11eb-97cd-cc00bde85b4d.png&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;(Eq.1) trainable linear projection은 flatten된 patch들을 &lt;em&gt;D&lt;/em&gt; dimension에 &lt;strong&gt;mapping시킨다.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;(Eq.4) BERT의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[class]&lt;/code&gt; 토큰처럼, embedded patch의 sequence(image representation &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y&lt;/code&gt;) 전에 &lt;strong&gt;learnable embedding&lt;/strong&gt;을 추가한다. (prepend)
        &lt;ul&gt;
          &lt;li&gt;both during pre-training and fine-tuning, a classification head is attached to $\left({z}_{L}^{0} \right)$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Position embeddings
        &lt;ul&gt;
          &lt;li&gt;positional information을 유지하기 위해 patch embedding에 붙여짐. (자세한건 Appendix.D.3)&lt;/li&gt;
          &lt;li&gt;standard learnable 1D position embedding 사용&lt;/li&gt;
          &lt;li&gt;이렇게 position embedding까지 더해진 sequence of embedding vector들은 encoder의 input이 됨.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Encoder
        &lt;ul&gt;
          &lt;li&gt;alternating layers of multiheaded self-attention(MSA)&lt;/li&gt;
          &lt;li&gt;MLP blocks
            &lt;ul&gt;
              &lt;li&gt;contains 2 layer with a GELU non-linearity&lt;/li&gt;
              &lt;li&gt;MLP 부분 code&lt;/li&gt;
            &lt;/ul&gt;

            &lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;      &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MlpBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Transformer MLP / feed-forward block.&quot;&quot;&quot;&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;mlp_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jnp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;out_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;deterministic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;kernel_init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initializers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xavier_uniform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;bias_init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;initializers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stddev&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Applies Transformer MlpBlock module.&quot;&quot;&quot;&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;actual_out_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_dim&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_dim&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;mlp_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;kernel_init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;bias_init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gelu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deterministic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deterministic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;actual_out_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;kernel_init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;bias_init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deterministic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deterministic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;            &lt;/div&gt;
          &lt;/li&gt;
          &lt;li&gt;Layernorm(LN) is applied before every block&lt;/li&gt;
          &lt;li&gt;residual connections after every block&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Hybrid Architecture
        &lt;ul&gt;
          &lt;li&gt;patch embedding projection E (Eq.1)이 CNN feature map으로 대체될 수 있다.
            &lt;ul&gt;
              &lt;li&gt;즉, ResNet과 같은 CNN구조의 모델을 가지고, 2D feature map 중 하나를 1D로 flatten시킨 후 transformer dimension에 projecting 시킴.&lt;/li&gt;
              &lt;li&gt;위에서 만들어진 sequence에 classification input embedding, position embedding를 추가시켜 encoder에 input으로써 feed 시킬 수 있음.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fine-tuning-and-higher-resolution&quot;&gt;Fine-Tuning and Higher Resolution&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;large dataset으로 pre-training하고, smaller downstream task에 대해 fine-tune 하려고 함.&lt;/li&gt;
  &lt;li&gt;이를 위해서 pre-trained prediction head를 지우고, 0으로 initializedgks D x K feedforward layer를 추가함.
    &lt;ul&gt;
      &lt;li&gt;K : # of downstream classes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;pre-training 보다 높은 resolution으로 fine-tuning하는 것은 beneficial할 때도 있음.&lt;/li&gt;
  &lt;li&gt;higher resolution을 feed하게 되면, patch size는 동일하므로 sequence length가 길어짐&lt;/li&gt;
  &lt;li&gt;ViT는 임의적인 sequence length를 다룰 수 있음(메모리 제약에 따라서)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;하지만 pre-trained position embedding이 의미 없어질 수 있음&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;원본 이미지의 location에 따라 pre-trained position embedding의 2D interpolation을 수행함.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;위와 같은 resolution adjustment와 patch extraction은 이미지의 2D 구조에 대해 inductive bias를 &lt;strong&gt;manually&lt;/strong&gt; ViT에 주입시키는 유일한 포인트임.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model.py&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/google-research/vision_transformer/blob/master/vit_jax/models.py&quot;&gt;Github&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;(References for your additional studies)&lt;/p&gt;

&lt;p&gt;https://jeonsworld.github.io/vision/vit/&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Computer Vision" /><category term="Transformer" /><summary type="html">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT) 를 읽고 정리한 글입니다.</summary></entry><entry><title type="html">StyleGAN:A Style-Based Generator Architecture for Generative Adversarial Networks</title><link href="http://localhost:4000/paper%20review/2020/09/20/StyleGAN/" rel="alternate" type="text/html" title="StyleGAN:A Style-Based Generator Architecture for Generative Adversarial Networks" /><published>2020-09-20T00:00:00+09:00</published><updated>2020-09-20T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/09/20/StyleGAN</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/09/20/StyleGAN/">&lt;p&gt;StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks 를 읽고 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;stylegan--a-style-based-generator-architecture-for-generative-adversarial-networks-cvpr-2019&quot;&gt;StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks (CVPR 2019)&lt;/h1&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;GAN의 generator 부분은 black box로 여겨져 이미지 생성 과정을 이해하기 어려웠음.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;style transfer&lt;/code&gt; 에서 기반한 generator 구조
    &lt;ul&gt;
      &lt;li&gt;각 레이어마다 style의 정보를 입힘. -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AdaIN&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;전체적인 스타일(머리 색, 인종, 성별 등), 세세한 부분(곱슬 등) 등까지 조정 가능 -&amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;noise&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;baseline : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;progressive GAN&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;latent vector로 부터 이미지 합성하고 점점 해상도를 올려서 high-resolution image 생성 =&amp;gt; scale-specific control&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;loss function, discriminator 등 수정하지 않고 오직 제너레이터에 대해서만 다룸.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;latent space의 interpolation quality 측정하는 measure 제안
    &lt;ul&gt;
      &lt;li&gt;perceptual path length&lt;/li&gt;
      &lt;li&gt;linear separability&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;FFHQ 데이터셋 오픈&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-work-basic-concepts&quot;&gt;Related work (Basic concepts)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Progressive GAN
    &lt;ul&gt;
      &lt;li&gt;GAN을 저해상도에서 고해상도로 점진적으로 학습&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;style transfer
    &lt;ul&gt;
      &lt;li&gt;content image &amp;amp; style image가 있을 때 content 이미지와 유사하게 style image에 입히는 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;
&lt;p&gt;(Explain one of the methods that the thesis used.)&lt;/p&gt;

&lt;h3 id=&quot;generator-architecture&quot;&gt;Generator Architecture&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93662640-90d5e300-fa9c-11ea-9a36-7312a058879d.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;left : traditional generaotr : latent code &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;z&lt;/code&gt;를 input layer에 바로 넣음.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;right : &lt;strong&gt;style-based generator&lt;/strong&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;first, map the input to an intermediate latent space &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;then controls the generator through &lt;strong&gt;adaptive instance normalization (AdaIN)&lt;/strong&gt; at each conv. layer.&lt;/li&gt;
      &lt;li&gt;Gaussian noise is added after each conv.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;제안한 모델을 차근차근 뜯어보자면&lt;/p&gt;

&lt;h4 id=&quot;mapping-network&quot;&gt;Mapping Network&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/1400/0*6lEwRXKiA8WGRlEc.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;https://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;input vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;z&lt;/code&gt;를 바로 input layer에 넣는 것이 아니라, mapping network를 거쳐 &lt;strong&gt;intermediate vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w&lt;/code&gt;&lt;/strong&gt; 로 변환한 후 이미지를 생성한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;바로 인풋 레이어에 넣지 않는 이유 : 고정된 input distribution에 맞춰야 해서 non-linear하게 mapping이 되고, 이것은 머리 색등과 같은 attribute를 변경하기 힘들어지기 때문.&lt;/li&gt;
  &lt;li&gt;위처럼 intermediate vector를 사용하게 되면 유동적인 공간에 mapping 시킬 수 있기 때문에 visual attribute 조절이 쉬워진다. =&amp;gt; &lt;strong&gt;disentanglement&lt;/strong&gt; 하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;즉, 이 네트워크에서는 z로부터 만들어진 style &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w&lt;/code&gt;를 구하고, 이를 affine transformation을 거친 A를 synthesis network에 넘겨주어 AdaIN operation을 통해 레이어에 스타일을 입힌다.&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;style-modules-adain&quot;&gt;Style Modules (AdaIN)&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/1400/0*uqn4slMHrFYkFmjS.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;https://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93665356-c4bb0380-fab0-11ea-844d-348f19fc4e2f.png&quot; alt=&quot;스크린샷 2020-09-19 오후 7 46 09&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;위에서 생성된 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w&lt;/code&gt;는 style에 대한 정보를 가지고 있다.&lt;/li&gt;
  &lt;li&gt;Synthesis network는 학습가능한 constant tensor(4x4x512)를 &lt;strong&gt;upsampling, convolution&lt;/strong&gt;을 통해 1024x1024x3 이미지로 변환시킨다.&lt;/li&gt;
  &lt;li&gt;w의 affine transfomation을 통해 얻어진 A를 가지고 &lt;strong&gt;AdaIN operation&lt;/strong&gt;을 통해 스타일을 입힌다.
    &lt;ul&gt;
      &lt;li&gt;normalize하고, 이를 scale하고 bias를 더함. 이게 스타일을 입히는 효과를 낸다.&lt;/li&gt;
      &lt;li&gt;매 conv 레이어마다 하므로, 각각의 레이어마다 다른 스타일을 조정할 수 있다. 이 말은 곧, 각 레이어가 특정한 attribute만을 담당한다는 뜻.
        &lt;ul&gt;
          &lt;li&gt;세밀한 스타일 조정 가능해진다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;stochastic-variation&quot;&gt;Stochastic variation&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://bloglunit.files.wordpress.com/2019/02/1_gwchaliormc1xlj7bh0zmg.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/1400/1*GwchALioRMC1xlj7Bh0ZMg.png&quot; alt=&quot;스크린샷 2020-09-19 오후 7 48 31&quot; /&gt;&lt;/p&gt;

&lt;p&gt;머리카락, 수염 등 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stochastic&lt;/code&gt;한 요소들은 사진의 디테일에 매우 중요함.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;위의 architecture에서 noise가 이에 대한 역할을 한다.&lt;/li&gt;
  &lt;li&gt;synthesis network에서 &lt;strong&gt;by adding per-pixel noise after each convolution.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;style-mixing&quot;&gt;Style Mixing&lt;/h3&gt;

&lt;p&gt;two random latent codes(w1,w2)를 사용하는 regularization 기법&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;하나의 w로 학습할 경우 여러 레이어에 대한 style이 correlate되는 문제점이 생길 수 있음.&lt;/li&gt;
  &lt;li&gt;ex. w1 스타일로 입혀놓지만, 랜덤으로 몇 개는 w2 스타일을 사용한다 …&lt;/li&gt;
  &lt;li&gt;위와 같은 방법을 통해 각 레이어가 담당하는 스타일을 명확하게 구분지을 수 있다.&lt;/li&gt;
  &lt;li&gt;(dropout과 비슷한 원리라고 함)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;disentanglement-studies&quot;&gt;Disentanglement studies&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;이 내용이 어려워서 제대로 이해하지 못함. 짧게 요약하겠음.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93665682-7c511500-fab3-11ea-8333-be0512370654.png&quot; alt=&quot;스크린샷 2020-09-19 오후 8 05 35&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Disentanglment : latent space가 선형적인 구조를 가지게 되어, 하나의 factor가 움직였을 때 정해진 특성이 바뀌게 만드는 것.
    &lt;ul&gt;
      &lt;li&gt;예. z의 특정한 값을 바꿨을 때 생성되는 이미지의 하나의 특성(성별, 머리카락 길이 등)만 영향을 주게 되는 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;fixed distribution을 따르게 되면 억지로 끼워맞추게 되어 어색한 이미지가 만들어질 수 있음.&lt;/li&gt;
  &lt;li&gt;하지만 이 모델처럼 &lt;strong&gt;비선형 mapping function&lt;/strong&gt;을 가지게 될 경우, 고정된 분포를 따를 필요가 없음.
    &lt;ul&gt;
      &lt;li&gt;위 그림에서 (c)와 같은 형태가 됨. 어느정도 a와 생김새가 비슷하면서 자연스럽게 맞출 수 있게 된 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;A major beneﬁt of our generator architecture is that the intermediate latent space W does not have to support sam-pling according to any ﬁxed distribution; its sampling density is induced by the learned piecewise continuous mapping f(z).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;본 논문에서는 disentanglement를 학습할 수 있는 두 가지 평가 지표를 제안함.
    &lt;ul&gt;
      &lt;li&gt;Perceptual path length&lt;/li&gt;
      &lt;li&gt;Linear seperability&lt;/li&gt;
      &lt;li&gt;위의 내용을 자세히 알고 싶다면 이 곳을 참조
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://jayhey.github.io/deep%20learning/2019/01/16/style_based_GAN_2/#perceptual-path-length&quot;&gt;링크&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;our investigations to &lt;strong&gt;the separation of high-level attributes and stochastic effects&lt;/strong&gt;, as well as &lt;strong&gt;the linearity of the intermediate latent space&lt;/strong&gt; will prove fruitful in improving the understanding and controllability of GAN synthesis.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;appendix-truncation-trick-in-w&quot;&gt;Appendix. Truncation trick in W&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;트레이닝 중에 하는 게 아니고, generator가 만든 것 중에 더 나은 latent space 을 뽑는 법에 대한 trick&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;학습이 완료된 네트워크의 input을 제어하는 방법&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93665586-81fa2b00-fab2-11ea-858c-69a2f3ea5026.png&quot; alt=&quot;스크린샷 2020-09-19 오후 7 58 35&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 수식을 통한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w'&lt;/code&gt; vector를 뽑는다.&lt;/p&gt;

&lt;h2 id=&quot;additional-studies&quot;&gt;Additional studies&lt;/h2&gt;
&lt;p&gt;(If you have some parts that cannot understand, you have to do additional studies for them. It’s optional.)&lt;/p&gt;

&lt;p&gt;disentanglement에 대한 명확한 이해가 필요함.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;(References for your additional studies)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://www.youtube.com/watch?v=TWzEbMrH59o&amp;amp;feature=youtu.be&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431&lt;/li&gt;
  &lt;li&gt;https://jayhey.github.io/deep%20learning/2019/01/16/style_based_GAN_2/&lt;/li&gt;
  &lt;li&gt;https://blog.lunit.io/2019/02/25/a-style-based-generator-architecture-for-generative-adversarial-networks/&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Computer Vision" /><category term="GAN" /><category term="Generative Model" /><summary type="html">StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks 를 읽고 정리한 글입니다.</summary></entry><entry><title type="html">HarDNet:A Low Memory Traffic network</title><link href="http://localhost:4000/paper%20review/2020/09/11/HarDNet/" rel="alternate" type="text/html" title="HarDNet:A Low Memory Traffic network" /><published>2020-09-11T00:00:00+09:00</published><updated>2020-09-11T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/09/11/HarDNet</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/09/11/HarDNet/">&lt;p&gt;HarDNet : A Low Memory Traffic network 를 읽고 개인적으로 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;hardnet--a-low-memory-traffic-network-iccv-2019&quot;&gt;HarDNet : A Low Memory Traffic network (ICCV 2019)&lt;/h1&gt;

&lt;h2 id=&quot;key-idea&quot;&gt;Key Idea&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;기존의 metrics들에서의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inference time&lt;/code&gt; 측정은 부정확하다.
    &lt;ul&gt;
      &lt;li&gt;새로운 metric =&amp;gt; &lt;strong&gt;memory traffic for accessing intermediate feature maps 측정&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;inference latency 측정에 유용할 것, especially in such tasks as &lt;em&gt;real-time object detection and semantic segmentation of high-resolution video.&lt;/em&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CIO&lt;/code&gt; : approximation of DRAM traffic이 될 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;computation, energy efficiency를 위해서는 fewer MACs, less DRAM이 좋은 것임&lt;/strong&gt; -&amp;gt; 연구 방향&lt;/li&gt;
  &lt;li&gt;각각의 레이어의 MoC에 soft constraint를 적용했음.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;low CIO network model&lt;/strong&gt; with a reasonable increase of MACs를 위해&lt;/li&gt;
      &lt;li&gt;방법 -&amp;gt; &lt;strong&gt;avoid&lt;/strong&gt; to employ a layer with a &lt;strong&gt;very low MoC such as a Conv1x1 layer&lt;/strong&gt; that has a &lt;strong&gt;very large input/output channel ratio.&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;input/output channel ratio가 크면 low MoC를 가진다는 사실을 알 수 있음.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Densely Connected Networks에 영감을 받아 모델 빌딩함.
    &lt;ol&gt;
      &lt;li&gt;DenseNet의 &lt;strong&gt;다수의 layer connections들을 줄였음.&lt;/strong&gt; =&amp;gt; concatenation cost를 줄이기 위해&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;balance the input/output channel ratio by increasing the channel width&lt;/strong&gt; of a layer according to its connections.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;659&quot; alt=&quot;스크린샷 2020-09-11 오후 9 30 55&quot; src=&quot;https://user-images.githubusercontent.com/48315997/92925832-14c51500-f476-11ea-93c3-6183adffd2e5.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;DRAM traffic&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;basic-concepts&quot;&gt;Basic Concepts&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;MAC : number of multiply-accumulate operations or floating point operations&lt;/li&gt;
  &lt;li&gt;DRAM : Dynamic Random-Access Memory
    &lt;ul&gt;
      &lt;li&gt;read/write model param. and feature maps&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CIO : Convolutional input/output
    &lt;ul&gt;
      &lt;li&gt;모든 conv layer에 대해 IN(C,W,H) X OUT(C,W,H) sum
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/92923605-b185b380-f472-11ea-81cc-a6d37d9bdd6c.png&quot; alt=&quot;image&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MoC : MACs over CIO of a layer = MACs/CIO&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;related-works&quot;&gt;Related Works&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;TREND : exploiting shortcuts&lt;/li&gt;
  &lt;li&gt;Highway networks, Residual Networks : add shortcuts to sum up a layer with multiple preceeding layers.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DenseNet&lt;/code&gt;&lt;/strong&gt; : &lt;strong&gt;concatenates all preceeding layers as a shortcut&lt;/strong&gt; achieving more efficent deep supervision.&lt;/li&gt;
  &lt;li&gt;그러나 shortcuts는 large memory usage, heavy DRAM traffic을 유발할 수 있다.
    &lt;ul&gt;
      &lt;li&gt;
        &lt;blockquote&gt;
          &lt;p&gt;Using shortcuts elongates the lifetime of a tensor, which may result in frequent data exchanges betwwen DRAM and cache.&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DenseNet의 sparsified version : &lt;em&gt;LogDenseNet, SparseNet&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Sparse&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;The pros?&lt;/strong&gt; If you have a lot of zeros, &lt;strong&gt;you don’t have to compute some multiplications, and you don’t have to store them&lt;/strong&gt;. So you &lt;strong&gt;&lt;em&gt;may&lt;/em&gt;&lt;/strong&gt; gain on size and speed, for training and inference (more on this today).&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;The cons?&lt;/strong&gt; Of course, having all these zeros will probably have an impact on network accuracy/performance.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;increase the &lt;strong&gt;growth rate(output channel width) to recover the accuracy dropping from the connection pruning,&lt;/strong&gt; and the increase of growth rate &lt;strong&gt;can compromise the CIO reduction&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;즉 increase of growth rate는 좋게 작용된다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;harmonic-densenet&quot;&gt;Harmonic DenseNet&lt;/h2&gt;

&lt;h3 id=&quot;sparsification-and-weighting&quot;&gt;Sparsification and weighting&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;let layer &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt; connect to layer &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k-2^n&lt;/code&gt; if 2^n divides k, where n is a non-negative integer and&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt; k-2^n &amp;gt;= 0&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;HarDBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_link&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_ch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;growth_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_ch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;growth_rate&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;link&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;dv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dv&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;link&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grmul&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;link&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;ch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_link&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_ch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;growth_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ch&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;link&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;2^n 개의 layer들이 이런 식으로 processed되면 layer [1 : 2^n -1]는 메모리에서 flush된다.&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;어떻게 flush 된다는 건지 잘 이해가 되지 않음.&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Power-of-two-th harmonic waves가 만들어짐. 그래서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Harmonic&lt;/code&gt; 이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;461&quot; alt=&quot;스크린샷 2020-09-11 오후 9 52 09&quot; src=&quot;https://user-images.githubusercontent.com/48315997/92927733-0cbaa480-f479-11ea-8194-3567f8ae83f1.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;이 방식은 concatenation cost를 눈에 띄게 감소시킨다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;layers with an index divided by a larger power of two are more influential&lt;/strong&gt; than those that divided by a smaller power of two.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;많이 connection되니까 당연히 influential 하다.&lt;/li&gt;
      &lt;li&gt;In this model, they amplify these key layers by increasing their channels, &lt;strong&gt;which can balance the channel ratio between the input and output of a layer to avoid a low MoC.&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;이런 key layer들을 &lt;strong&gt;amplify&lt;/strong&gt; 했음(channel 수를 늘리면서)&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;layer &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l&lt;/code&gt; has an initial growth rate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt;, we let its channel number to be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k * m^n&lt;/code&gt; , where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; is the max number satisfying that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l&lt;/code&gt; is divided by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2^n&lt;/code&gt;
&lt;img width=&quot;442&quot; alt=&quot;스크린샷 2020-09-11 오후 10 11 01&quot; src=&quot;https://user-images.githubusercontent.com/48315997/92929542-af742280-f47b-11ea-89b9-22e4e5e9331d.png&quot; /&gt;&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt;  은 low-dimensional compression factor 역할을 한다.&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt; 을 2보다 작게하면 input channel을 output channel보다 작게 할 수 있다.
            &lt;ul&gt;
              &lt;li&gt;Empirically, settin &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt; between 1.6 and 1.9&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;transition-and-bottleneck-layers&quot;&gt;Transition and Bottleneck Layers&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HDB(Harmonic Dense Block)&lt;/code&gt; : the proposed connection pattern forms a group of layers
    &lt;ul&gt;
      &lt;li&gt;is followed by a Conv1x1 layer as a transition&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;HDB의 depth는 2의 제곱수로 설정
    &lt;ul&gt;
      &lt;li&gt;HDB의 마지막 레이어가 가장 큰 채널수를 가지도록 하기 위해서&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DenseNet -&amp;gt; gradient할 때 모든 레이어를 다 pass함&lt;/li&gt;
  &lt;li&gt;논문의 HBD with depth L -&amp;gt; pass through at most &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log L layers&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;degradation을 완화시키기위해, depth-L HDB를 layer L과 all its preceeding &lt;strong&gt;odd numbered layers&lt;/strong&gt;  를 concatenation시킨다.&lt;/li&gt;
      &lt;li&gt;2~L-2의 all even layer들의 아웃풋은 HDB가 한번 끝날때마다 버려진다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Bottleneck layer
    &lt;ul&gt;
      &lt;li&gt;DenseNet에서는 param. efficiency를 위해 매 Conv3x3 layer전에 bottleneck을 두었다.&lt;/li&gt;
      &lt;li&gt;하지만 HarDnet에서는 위에서 &lt;strong&gt;이미 channel ratio(매 레이어마다 input&amp;amp;output 사이의)의 균형을 잡았으므로 bottleneck layer는 쓸모없어진다.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;그래서 HBD에서는 Bottleneck layer없이 &lt;strong&gt;Conv3x3 for all layers&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transition layer
    &lt;ul&gt;
      &lt;li&gt;&lt;img width=&quot;492&quot; alt=&quot;스크린샷 2020-09-11 오후 10 26 18&quot; src=&quot;https://user-images.githubusercontent.com/48315997/92931026-d16ea480-f47d-11ea-96df-eff3717796b9.png&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;inverted trainsition module
        &lt;ul&gt;
          &lt;li&gt;maps input tensor to an additional max pooling function along with the original average pooling, followed by concatenation and Conv1x1.&lt;/li&gt;
          &lt;li&gt;50% of CIO를 감소시킴&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;&lt;img width=&quot;483&quot; alt=&quot;스크린샷 2020-09-11 오후 10 27 05&quot; src=&quot;https://user-images.githubusercontent.com/48315997/92931109-ed724600-f47d-11ea-81a6-d3903463743b.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;CamVid Dataset&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;replace all the blocks in a FC-DenseNet with HDBs&lt;/li&gt;
      &lt;li&gt;the architecture of FC-DenseNet with an encoder-decoder structure and block level shortcuts to create models for sematic segmentation.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;blockquote&gt;
      &lt;p&gt;We propose FC-HarDNet84 as specified in Table 3 for comparing with FC-DenseNet103. &lt;strong&gt;The new network achieves CIO reduction by 41% and GPU inference time reduction by 35%.&lt;/strong&gt; A smaller version, FC-HarDNet68, also outperforms FC-DenseNet56 by a 65% less CIO and 52% less GPU inference time.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;1009&quot; alt=&quot;스크린샷 2020-09-11 오후 10 34 16&quot; src=&quot;https://user-images.githubusercontent.com/48315997/92931776-ee57a780-f47e-11ea-9201-93de47f4a12a.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ImageNet Datasets&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;1018&quot; alt=&quot;스크린샷 2020-09-11 오후 10 34 41&quot; src=&quot;https://user-images.githubusercontent.com/48315997/92931811-fca5c380-f47e-11ea-9c47-4b9738f47a92.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Object Detection
    &lt;ul&gt;
      &lt;li&gt;HarDNet-68 as &lt;strong&gt;a backbone model for a Single Shot Detector (SSD) and train it with PASCAL VOC 2007 and MS COCO datasets&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img width=&quot;479&quot; alt=&quot;스크린샷 2020-09-11 오후 10 35 26&quot; src=&quot;https://user-images.githubusercontent.com/48315997/92931875-1810ce80-f47f-11ea-9883-8d1d9ac87947.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;There is an assumption with the &lt;strong&gt;CIO&lt;/strong&gt;, which is &lt;strong&gt;a CNN model that is processed layer by layer without a fusion.&lt;/strong&gt; In contrast, &lt;strong&gt;fused-layer computation for multiple convolutional layers has been proposed.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;CIO still failed&lt;/strong&gt; to predict the actual inference time &lt;strong&gt;such as comparing two network models with significantly differnent architectures&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;In some of the layers CIO may dominate, but for the other layers, MACs can still be the key factor if its computational density is relatively higher. To precisely predict the inference latency of a network, we need to breakdown to each of the layers and investigate its MoC to predict the inference latency of the layer.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;어쨌거나 &lt;strong&gt;DRAM traffic의 중요성&lt;/strong&gt;을 강조하고 싶어함.&lt;/li&gt;
  &lt;li&gt;traffic reduction을 위한 가장 좋은 방법은 &lt;strong&gt;MoC를 증가시키는 것&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;which might be counter-intuitive to the widely-accepted knowledge of that using more Conv1x1 achieves a higher efficiency.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Traffic" /><category term="Computer Vision" /><category term="Semantic Segmentation" /><category term="Network" /><summary type="html">HarDNet : A Low Memory Traffic network 를 읽고 개인적으로 정리한 글입니다.</summary></entry><entry><title type="html">Neural Architecture Search With Reinforcement Learning</title><link href="http://localhost:4000/paper%20review/2020/08/29/NEURAL-ARCHITECTURE-SEARCH-WITH-REINFORCEMENT-LEARNING/" rel="alternate" type="text/html" title="Neural Architecture Search With Reinforcement Learning" /><published>2020-08-29T00:00:00+09:00</published><updated>2020-08-29T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/2020/08/29/NEURAL%20ARCHITECTURE%20SEARCH%20WITH%20REINFORCEMENT%20LEARNING</id><content type="html" xml:base="http://localhost:4000/paper%20review/2020/08/29/NEURAL-ARCHITECTURE-SEARCH-WITH-REINFORCEMENT-LEARNING/">&lt;p&gt;Neural Architecture Search With Reinforcement Learning 논문을 읽고 정리한 글입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;(You should include contents of summary and introduction.)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;we use a re- current network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;RNN을 이용해서 neural network의 model description(하이퍼 파라미터: # of filters, stride length …)을 문자열로 생성한다.
강화학습을 통해 expected accuracy를 최대로 만든다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/658/1*KgICs1DPpGbqY2WWPn1kwg.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Controller에서 p의 확률로 A라는 Architecture를 생성한다.&lt;/li&gt;
  &lt;li&gt;자식 네트워크에서는 A 아키텍쳐를 훈련시켜 정확도 R을 구한다.&lt;/li&gt;
  &lt;li&gt;정확도를 리워드의 신호로 사용한다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;policy gradient&lt;/code&gt;를 계산해서 컨트롤러를 업데이트한다.&lt;/li&gt;
  &lt;li&gt;반복하다보면 더 높은 확률로 더 높은 정확도를 보이는 아키텍쳐를 찾을 수 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;related-work-basic-concepts&quot;&gt;Related work (Basic concepts)&lt;/h2&gt;

&lt;p&gt;NAS 부분의 거의 최초라고 볼 수 있음.&lt;/p&gt;

&lt;p&gt;이전 연구들 : Hyperparameter optimization&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;it is difficult to ask them to &lt;strong&gt;generate a variable-length configuration&lt;/strong&gt; that specifies the structure and connectivity of a network&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;유전자 알고리즘&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;search-based 방식이라 탐색속도가 느림.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;컨트롤러에서의 Neural Architecture 방식은 이전 예측값들을 input으로 받아 하이퍼 파라미터를 한 번에 하나씩 예측하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto-regressive&lt;/code&gt;한 방식이다.&lt;/p&gt;

&lt;h2 id=&quot;methods&quot;&gt;Methods&lt;/h2&gt;
&lt;p&gt;(Explain one of the methods that the thesis used.)&lt;/p&gt;

&lt;p&gt;이 논문의 Key point : &lt;strong&gt;skip connection 예측하여 모델의 복잡도를 높인 것&lt;/strong&gt;, &lt;strong&gt;파라미터 접근방식을 사용해서 훈련 속도를 높인 것&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Generate Model with a Controller Recurrent Neural Network&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*9dgifjZ6BKyPqzIxR-EGwg.png&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It predicts filter height, filter width, stride height, stride width, and number of filters for one layer and repeats. Every prediction is carried out by a softmax classifier and then fed into the next time step as input.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;컨트롤러를 이용하여 CNN 모델에 사용하는 하이퍼파라미터들을 생성함.&lt;/p&gt;

&lt;p&gt;레이어마다 사용할 필터, Stride 값을 예측하고 반복함.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;하이퍼 파라미터 예측시에 softmax classifier를 거친값이 다음 스텝의 input으로 들어감.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;컨트롤러 RNN이 아키텍쳐를 생성하면 생성된 아키텍쳐의 뉴럴 네트워크를 훈련시킴.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The parameters of the controller RNN, θc, are then optimized in order to maximize the expected validation accuracy of the proposed architectures.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Validation set으로 네트워크의 정확도를 측정하고, 컨트롤러 RNN의 파라미터 세타C는 정확도의 기대값을 최대화하기 위해 최적화됨.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Training with Reinforce&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/456/1*T03ptXjYcHkOBLFfoEs89A.png&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;controller to maximize its expected reward&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;컨트롤러 token list a[1]:a[T] : Architecture predicted by the controller RNN viewed as a sequence of actions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;자식 네트워크는 생성된 구조의 정확도 R을 출력하고, 이 R을 강화학습의 리워드로 사용해서 컨트롤러를 강화학습 훈련시킴.&lt;/li&gt;
  &lt;li&gt;Layer 하나짜리 CNN에서의 T=3임.
    &lt;ul&gt;
      &lt;li&gt;a1 : filter height&lt;/li&gt;
      &lt;li&gt;a2 : filter width&lt;/li&gt;
      &lt;li&gt;a3 : # of filters&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/91626958-815c0080-e9ee-11ea-89cb-43ee634d253e.png&quot; alt=&quot;스크린샷 2020-08-29 오전 11 55 12&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/91627195-bc5f3380-e9f0-11ea-9bd2-3d2b8653075c.png&quot; alt=&quot;스크린샷 2020-08-29 오후 12 11 09&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/91627201-c84af580-e9f0-11ea-8824-88e3e1f87255.png&quot; alt=&quot;스크린샷 2020-08-29 오후 12 11 30&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In this work, we use the REINFORCE rule from Williams (1992)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Standard REINFORCE Update Rule&lt;/li&gt;
  &lt;li&gt;R은 미분 불가능함. =&amp;gt; policy gradient를 써서 세타 C를 업데이트한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;accelerate-training-with-parallelism-and-asynchronous-updates&quot;&gt;Accelerate Training with Parallelism and Asynchronous Updates&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;자식 네트워크 : 하나의 모델을 뜻함&lt;/li&gt;
  &lt;li&gt;여러 컨트롤러 * 여러 자식 네트워크 =&amp;gt; 많은 네트워크를 만들어냄
    &lt;ul&gt;
      &lt;li&gt;훈련 속도를 높이기 위해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;파라미터-서버&lt;/code&gt; 구조 사용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*9UQdtOqDpyef44nhKYsrcA.png&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;S개의 파라미터 서버가 있고 이 서버와 연결된 K개의 복제된 &lt;strong&gt;컨트롤러에 공유된 파라미터 값이 저장됨.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;각각의 컨트롤러는 m개의 자식 네트워크를 복제해서 &lt;strong&gt;병렬&lt;/strong&gt;로 훈련시킴.&lt;/p&gt;

&lt;p&gt;이 때 자식 네트워크의 정확도는 파라미터 서버에 보낼 세타 C에 대한 gradient를 계산하기 위해 기록됨.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Increase Architecture Complexity with Skip Connection and Other Layer Types&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/91627249-22e45180-e9f1-11ea-9774-73701c1e18c4.png&quot; alt=&quot;스크린샷 2020-08-29 오후 12 14 03&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Skip connection을 추가해서 탐생 공간을 넓힌다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;레이어마다 anchor point를 더해서 이전 레이어들 중 어떤 레이어를 현재 레이어의 input으로 할지 결정함.&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Generate Recurrent Cell Architectures&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;지금까지 CNN을 위한 Neural Architecture, 지금은 RNN&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*MFYHqx5BOad936u6QfhmrQ.png&quot; alt=&quot;4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RNN, LSTM은 x(t), h(t-1)을 input으로 하고 h(t)를 output으로 하는 트리구조로 나타낼 수 있음(맨 오른쪽)&lt;/p&gt;

&lt;p&gt;RNN 컨트롤러에서는 트리 노드들의 결합방석(addition, elementwise multiplication)과 활성화함수(sigmoid,tanh)를 선택할 수 있음.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;그림 (b)의 Cell indices 의 왼쪽 1부분이 의미하는 것은 다음 메모리구조 C_t와 연결되는것은 Tree index 1 이며 오른쪽 0부분은 h_t 를 구할때 사용되는 것이 Tree index 0 이라는 것입니다. 그림 (b)의 Tree index 2 는 Tree0과 Tree1의 결합방식을 나타내는 것으로 그림에선 elementwise multiplication와 sigmoid의 결합이 됩니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;기존 SOTA 모델과 비교했을 때 약간의 성능 감소는 있었지만 &lt;strong&gt;더 작은 파라미터로 구현이 되었음,&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CNN (CIFAR-10 dataset)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/91627442-4fe53400-e9f2-11ea-98be-cb4432fd5b75.png&quot; alt=&quot;스크린샷 2020-08-29 오후 12 22 27&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RNN (Penn Treebank dataset)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/91627448-64c1c780-e9f2-11ea-8047-2596427ffe02.png&quot; alt=&quot;스크린샷 2020-08-29 오후 12 23 03&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Transfer Learning on Neural Machine Translation
    &lt;ul&gt;
      &lt;li&gt;LSTM을 빼고 NAS를 통해 만든 cell을 넣었음.&lt;/li&gt;
      &lt;li&gt;LSTM에 특화된 하이퍼파라미터들을 튜닝하지 않음&lt;/li&gt;
      &lt;li&gt;BELU score 0.5 오름&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;additional-studies&quot;&gt;Additional studies&lt;/h2&gt;
&lt;p&gt;(If you have some parts that cannot understand, you have to do additional studies for them. It’s optional.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Understanding Deep Learning Requires Rethinking Generalization&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Designing Neural Network Architectures Using RL&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;(References for your additional studies)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=XP3vyVrrt3Q&quot;&gt;https://www.youtube.com/watch?v=XP3vyVrrt3Q&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/@sunwoopark/slow-paper-neural-architecture-search-with-reinforcement-learning-6de601560522&quot;&gt;https://medium.com/@sunwoopark/slow-paper-neural-architecture-search-with-reinforcement-learning-6de601560522&lt;/a&gt;&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Reinforcement Learning" /><category term="Neural Architecture Search" /><summary type="html">Neural Architecture Search With Reinforcement Learning 논문을 읽고 정리한 글입니다.</summary></entry><entry><title type="html">딥러닝 모델을 이용한 수화 교육 웹 어플리케이션-Handlang(2)</title><link href="http://localhost:4000/project/2020/08/26/Handlang2/" rel="alternate" type="text/html" title="딥러닝 모델을 이용한 수화 교육 웹 어플리케이션-Handlang(2)" /><published>2020-08-26T00:00:00+09:00</published><updated>2020-08-26T00:00:00+09:00</updated><id>http://localhost:4000/project/2020/08/26/Handlang2</id><content type="html" xml:base="http://localhost:4000/project/2020/08/26/Handlang2/">&lt;p&gt;DSC EWHA에서 2019.9~2020.8 까지 진행한 팀프로젝트로, 딥러닝 모델을 이용한 수화 학습 웹 어플리케이션입니다.
이 포스팅에서는 웹에 관련된 것을 다룹니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;handlang---aslamerican-sign-language-education-by-using-deep-learning-model&quot;&gt;Handlang - ASL(American Sign Language) Education by using deep learning model&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;딥러닝으로 학습된 수화 인식 모델을 바탕으로 알파벳, 숫자에 해당되는 수화를 학습 및 연습 할 수 있는 웹 어플리케이션입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;wireframe---figma&quot;&gt;Wireframe - Figma&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fn8Hg1%2FbtqCPBw0VEs%2F6fRBkKw5iK71fcigASJIC0%2Fimg.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93708908-f5fd0780-fb74-11ea-950e-adbd44260edd.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figma를 사용하여 팀원들과 홈페이지 와이어프레임을 구상하였습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;flask&quot;&gt;Flask&lt;/h2&gt;

&lt;p&gt;웹 개발 초보자에게 비교적 쉬운 Flask를 사용하여 구현하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;model-deploy&quot;&gt;Model Deploy&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;학습시킨 모델을 불러오는 법&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_model&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'model/handlang_model_4.h5'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 지문자 모델
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'model/su_adamax.h5'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 숫자 모델
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;ajax&quot;&gt;Ajax&lt;/h2&gt;

&lt;p&gt;웹캠으로 받은 이미지를 실시간으로 Detect해야하기 때문에 페이지를 새로 고치지 않아도 데이터를 로드할 수 있는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ajax&lt;/code&gt;를 사용하였습니다.&lt;/p&gt;

&lt;h2 id=&quot;translation&quot;&gt;Translation&lt;/h2&gt;

&lt;p&gt;한글/영어 버전의 웹페이지를 구현하기 위해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flask_babel&lt;/code&gt;을 사용했습니다.&lt;/p&gt;

&lt;h2 id=&quot;study--quiz&quot;&gt;Study &amp;amp; Quiz&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93709264-fa76ef80-fb77-11ea-99c2-33eccceb6f65.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93709269-05ca1b00-fb78-11ea-85c1-ad523fd94f95.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93709270-0cf12900-fb78-11ea-9d04-4dbf3a648ea3.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/48315997/93709273-12e70a00-fb78-11ea-999f-71a7f8876db6.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;team-handlang&quot;&gt;Team Handlang&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/yskim0/Handlang&quot;&gt;Project Github Link&lt;/a&gt;&lt;/p&gt;</content><author><name>Yeonsoo Kim</name></author><category term="Deep Learning" /><category term="Object Detection" /><category term="Flask" /><category term="ASL Education Application" /><category term="YOLO" /><category term="Fast R-CNN" /><category term="Inception v3" /><summary type="html">DSC EWHA에서 2019.9~2020.8 까지 진행한 팀프로젝트로, 딥러닝 모델을 이용한 수화 학습 웹 어플리케이션입니다. 이 포스팅에서는 웹에 관련된 것을 다룹니다.</summary></entry></feed>