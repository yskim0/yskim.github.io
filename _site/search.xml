<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìžë™ìœ¼ë¡œ ë°ì´í„° ì‹œê°í™” Plot ê·¸ë¦¬ê¸°]]></title>
      <url>/project/2021/05/23/Capstone_Start7_review/</url>
      <content type="text"><![CDATA[ì´í™”ì—¬ëŒ€ 2021-1í•™ê¸° ìº¡ìŠ¤í†¤ë””ìžì¸í”„ë¡œì íŠ¸B ìŠ¤íƒ€íŠ¸7íŒ€ Ewha Visualization Recommendation Program(ERP) ê¸°ìˆ  íŠœí† ë¦¬ì–¼ì— ê´€í•œ ê¸€ìž…ë‹ˆë‹¤.ë³¸ í¬ìŠ¤íŒ…ì€ ì‹œê°í™” ì¶”ì²œ í”„ë¡œê·¸ëž¨(Visualization Recommendation Program)ì„ ê°œë°œí•˜ëŠ” ê³¼ì • ì¤‘, Data2Visë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì— ì í•©í•œ ì‹œê°í™” Plotì„ ìžë™ìœ¼ë¡œ ê·¸ë ¤ì£¼ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµ ì‹œí‚¤ëŠ” ê³¼ì •ì— ëŒ€í•˜ì—¬ ìž‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.  Contents  About Our Project  Data2Vis          Model Archi.        Our Model          Dataset      Model config &amp; Training      Tensorboard - Loss        Web Demo  ConclusionAbout Our Projectâ€¦(and goal)ì•ˆë…•í•˜ì„¸ìš”. ë…¼ë¬¸ ë¦¬ë·° ê¸€ì´ ì•„ë‹Œ í”„ë¡œì íŠ¸ ê´€ë ¨ ê¸€ì€ ì˜¤ëžœë§Œì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.ìš”ì¦˜ ìºê¸€, ë°ì´ì½˜ ë“± ë°ì´í„° ë¶„ì„ ê´€ë ¨ Competitionë“¤ì´ êµ‰ìž¥ížˆ ë§Žë‹¤ëŠ” ê±¸ ê´€ì‹¬ìžˆìœ¼ì‹  ë¶„ë“¤ì€ ì•„ì‹¤ ê²ë‹ˆë‹¤.ì €ë„ ë°ì´ì½˜ ëŒ€íšŒì— ìˆ˜ìƒí•œ ê²½í—˜ì´ ìžˆê³ , ì§€ê¸ˆì€ ì˜ˆì „ë§Œí¼ì€ ëª»í•˜ì§€ë§Œ í•œì°½ Data Science ê³µë¶€ë¥¼ ë§Žì´ í•  ë•Œê°€ ìžˆì—ˆìŠµë‹ˆë‹¤.ì²˜ìŒ ë°ì´í„° ë¶„ì„ì„ ê³µë¶€í•˜ì‹œëŠ” ë¶„ë“¤ë¶€í„° ì „ë¬¸ì ìœ¼ë¡œ ë‹¤ë£¨ì‹œëŠ” ë¶„ë“¤ê¹Œì§€, ë°ì´í„° ë¶„ì„ ë¶„ì•¼ì—ì„œ ì§€ë‚˜ì¹  ìˆ˜ ì—†ëŠ” ë¶€ë¶„ì€ ë‹¨ì—°ì½” Visualization about DATA ì¼ ê²ƒìž…ë‹ˆë‹¤. ìºê¸€ì—ì„œ ëŒ€íšŒ í•˜ë‚˜ê°€ ì—´ë¦¬ìžë§ˆìž í•œì‹œê°„ ë‚´ë¡œ ì•„ë¦„ë‹¤ìš´ EDA plotë“¤ì´ ê³ ìˆ˜ë‹˜ë“¤ì˜ ì†ì„ ê±°ì³ ê·¸ë ¤ì§€ëŠ” ê²ƒì„ ë³´ì‹¤ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. (Kaggleì—ì„œëŠ” Notebook ë¶„ì•¼ì˜ ê·¸ëžœë“œ ë§ˆìŠ¤í„°ë¶„ë“¤ì´ ì£¼ë¡œ ì´ ì—­í• ì„ ë§¡ì£ .)ë˜í•œ ì´ëŸ¬í•œ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ëŒ€íšŒê°€ ì•„ë‹ˆë”ë¼ë„,  ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ì— ìžˆì–´ì„œ í˜„ìž¬ ìžì‚¬/ê°œì¸ì´ ê°–ê³  ìžˆëŠ” ë°ì´í„°ê°€ ì–´ë–¤ í˜•íƒœì´ê³  ì–´ë–¤ ì˜ë¯¸ì¸ì§€ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•œ ì¼ìž…ë‹ˆë‹¤.ì´ëŸ¬í•œ ìƒí™©(=ë°ì´í„°ì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•œ ìƒí™©)ì— ë°œë§žì¶”ì–´ Data Visualizationì— ëŒ€í•œ ì—°êµ¬ ë˜í•œ ë°œì „ë˜ê³  ìžˆìŠµë‹ˆë‹¤.ì €í¬ íŒ€ì€ ì´ Data Visualization ì—°êµ¬ì—ì„œë„ cold-start ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìžˆëŠ” (ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ) Data Visualization Recommendationì„ í”„ë¡œì íŠ¸ ì£¼ì œë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤. ìžì„¸ížˆëŠ” ì €í¬ê°€ ì œê³µí•˜ëŠ” ìƒ˜í”Œ ë°ì´í„° ë˜ëŠ” ì‚¬ìš©ìžê°€ ì›í•˜ëŠ” ë°ì´í„°ì…‹ì„ upload í–ˆì„ ë•Œ, ì €í¬ì˜ Visualization Recommendation ëª¨ë¸ì´ ë°ì´í„°ì…‹ì„ í•´ì„í•˜ì—¬ ì—¬ëŸ¬ ê°œì˜ plotì„ ê·¸ë ¤ì£¼ì–´ ì‚¬ìš©ìžì—ê²Œ ì¶”ì²œí•´ì£¼ëŠ” ê²ƒìž…ë‹ˆë‹¤.ì¦‰ ìš”ì•½í•˜ìžë©´,  ë°ì´í„°ì…‹(e.g., csv, tsv, json)ì„ inputìœ¼ë¡œ í•˜ì—¬  ì €í¬ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ í•´ë‹¹ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì ì ˆí•œ chart(visualization) recommendationì„ kê°œ ë¦¬ìŠ¤íŠ¸ì—…í•˜ì—¬ ë³´ì—¬ì£¼ê³   ì‚¬ìš©ìžê°€ chartë¥¼ ì„ íƒ          ì¶”ê°€ì ìœ¼ë¡œëŠ” ì°¨íŠ¸ë¥¼ íŽ¸ì§‘í•˜ê³  ì €ìž¥í•  ìˆ˜ ìžˆëŠ” ê¸°ëŠ¥      ì˜ ê¸°ëŠ¥ì„ êµ¬í˜„í•œ ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬í˜„í•˜ëŠ” ê²ƒì´ ëª©í‘œìž…ë‹ˆë‹¤.ë³¸ í¬ìŠ¤íŒ…ì—ì„œëŠ” â€˜ì‚¬ìš©ìžê°€ chartë¥¼ ì„ íƒí•˜ê¸° ì „â€™ê¹Œì§€ì˜ ê³¼ì •, ì¦‰ ë°ì´í„°ì…‹ì„ ë°›ì•„ ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ chart recommendationì„ ëª‡ ê°€ì§€ ë³´ì—¬ì£¼ëŠ” ë‹¨ê³„ê¹Œì§€ ìž‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.Data2Visì•žì„œ Data Visualizationì— ëŒ€í•œ ì—°êµ¬ê°€ í™œì„±í™”ë˜ê³  ìžˆë‹¤ í•˜ì˜€ëŠ”ë°, ì´ì— ë”°ë¼ ë‹¹ì—°ížˆ Visualizaiton Recommendation ì—°êµ¬ë„ ìƒë‹¹ížˆ ë°œì „í•˜ì˜€ìŠµë‹ˆë‹¤. ê´€ë ¨í•˜ì—¬ ì—¬ëŸ¬ ê°œì˜ ë…¼ë¬¸ì„ ì½ì–´ë³´ì•˜ê³  github ì½”ë“œ ë“±ì„ ë‹¤ ì‚´íŽ´ë³¸ í›„ ë‚´ë¦° ê²°ê³¼, ì €í¬ëŠ” Data2Vis : Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural Networks ë…¼ë¬¸ì— ì œì‹œëœ ëª¨ë¸ì„ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤. (ë¬¼ë¡  ì €í¬ê°€ ì‚¬ìš©í•  ë°ì´í„°ì…‹ì— ë” optimizeí•  ì˜ˆì •ì´ë¯€ë¡œ ëª¨ë¸ì— ëŒ€í•œ ì—¬ëŸ¬ ê°€ì§€ ì‹¤í—˜ë“¤ì„ ë°©í•™ë™ì•ˆ ê±°ì¹  ì˜ˆì •ìž…ë‹ˆë‹¤.)Data2VisëŠ” visualization generation ë¬¸ì œë¥¼ í•˜ë‚˜ì˜ language translation problemìœ¼ë¡œ ë³´ì•˜ê³ , ì´ ë¬¸ì œë¥¼ attention ë² ì´ìŠ¤ì˜ LSTM encoder-decoder ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤.ë˜í•œ ê·¸ëž˜í”„ë¥¼ ê·¸ë ¤ë‚´ëŠ” grammarë¡œ JSON í¬ë§·ì˜ Vega-Liteë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.  ë”°ë¼ì„œ, í•™ìŠµìš© ë°ì´í„°ì…‹ë„ Vega-Lite ì½”ë“œ!  ë°ì´í„°ì…‹ì— ëŒ€í•œ ì´ì•¼ê¸°ëŠ” ì•„ëž˜ ì„¹ì…˜ì—ì„œ ë” ìžì„¸í•˜ê²Œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.Data2Vis ëª¨ë¸ì— ëŒ€í•´ ê°„ëžµí•˜ê²Œ ì •ë¦¬í•´ë³´ê³  í•´ë‹¹ ì„¹ì…˜ì€ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤. (ì´ì „ ê²Œì‹œê¸€ì— ë” ìžì„¸ížˆ ë¦¬ë·°ë˜ì–´ ìžˆìŠµë‹ˆë‹¤. ì•„ëž˜ ë¶€ë¶„ì€ í•´ë‹¹ ê¸€ì„ ë°œì·Œí•˜ì˜€ìŠµë‹ˆë‹¤.)Model Architecture  the data visualization problem as a Seq2Seq translation problem    input : dataset (fields, values in JSON format)output : valid Vega-Lite visualization specification        encoder-decoder archi.          where the encoder reads and encodes a source sequence into a fixed length vector, and a decoder outputs a translation based on this vector.        Attention Mechanism          Attention mechanisms allow a model to focus on aspects of an input sequence while generating output tokens.        Beam Search algorithm          The beam search algorithm used in sequence-to-sequence neural translation models keeps track of k most probable output tokens at each step of decoding, where k is known as the beamwidth. This enables the generation of k most likely output sequences for a given input sequence.            THREE techniques : bidirectional encoding, differential weighing of context via an attention mechanism, and beam search    character-based sequence modelOur Model (Dataset, Training-Settingsâ€¦)ðŸ“© input : 1ê°œì˜ datasetðŸ“¬ output : visualization recommendation plotë³¸ê²©ì ìœ¼ë¡œ ì €í¬ í”„ë¡œì íŠ¸ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ë§Œë“¤ê² ìŠµë‹ˆë‹¤. ì‚¬ì‹¤ data2vis ê¹ƒí—ˆë¸Œì—ì„œ ì œê³µí•˜ëŠ” pretrained modelì´ ìžˆì§€ë§Œ, ì–´ì°¨í”¼ ë°©í•™ì— ì—¬ëŸ¬ ì‹¤í—˜ì„ í• ê±°ë‹ˆê¹Œ ë¯¸ë¦¬ ì—°ìŠµí•´ë³¸ë‹¤ëŠ” ìƒê°ìœ¼ë¡œ ì²˜ìŒë¶€í„° ìƒˆë¡œ í•™ìŠµì‹œì¼°ìŠµë‹ˆë‹¤.DatasetVega-Lite ê·¸ëž˜ë¨¸ ê¸°ë°˜ì˜ ì–‘ì§ˆì˜ plot ë°ì´í„°ì…‹ì„ í¬ë¡¤ë§í•˜ëŠ” ë“± ë”°ë¡œ ìˆ˜ì§‘í•´ì˜¤ê¸° ì–´ë µë‹¤ê³  íŒë‹¨í•œ ê²°ê³¼, data2visê°€ í•™ìŠµí•  ë•Œ ì‚¬ìš©í–ˆë˜ ë°ì´í„°ì…‹ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. (ë³¸ í¬ìŠ¤íŒ…ì—ì„œ ì‚¬ìš©ëœ ë°ì´í„°ì…‹ì€ data2vis githubì— ìžˆìŠµë‹ˆë‹¤.)ì›ë³¸ ë°ì´í„° ì˜ˆì‹œ  barley.jsonplotì„ ê·¸ë¦¬ê³ ìž í•˜ëŠ” ì›ë³¸ ë°ì´í„°ê°€ ìœ„ì™€ ê°™ì€ í˜•íƒœë¥¼ ê°€ì§„ json íƒ€ìž…ì˜ ë°ì´í„°ë¼ ê°€ì •í•˜ê² ìŠµë‹ˆë‹¤.ì´ëŠ” ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë•Œ ì‚¬ìš©ë˜ëŠ” training data ì¤‘ í•˜ë‚˜ë¡œ, ìœ„ì˜ barely.json ë°ì´í„°ì— ëŒ€í•œ plotì„ ê·¸ë¦¬ëŠ” Vega-lite ê·¸ëž˜ë¨¸ ê¸°ë°˜ì˜ ë°ì´í„°ìž…ë‹ˆë‹¤.ì¦‰, Vega-lite ê·¸ëž˜ë¨¸ ê¸°ë°˜ì˜ plotì„ ìƒì„±í•˜ê¸° ìœ„í•˜ì—¬ ì´ëŸ¬í•œ í˜•ì‹ì˜ ë°ì´í„°ì…‹ì´ í•™ìŠµìš©ìœ¼ë¡œ í•„ìš”í•œ ê²ƒìž…ë‹ˆë‹¤.barley.json ë°ì´í„°ì˜ ì¼ë¶€ì™€ ìœ„ì˜ Vega-lite ê·¸ëž˜ë¨¸ ê¸°ë°˜ ì½”ë“œë¥¼ Vega Editorë¥¼ í†µí•´ ê·¸ë¦° plotìž…ë‹ˆë‹¤."mark" : "bar"ë¡œ ì½”ë”©í–ˆë“¯ì´ bar ì°¨íŠ¸ì´ê³ , encoding íŒŒíŠ¸ë¥¼ ë´¤ì„ ë•Œ xì¶•ì€ yield, yì¶•ì€ yield ë²”ìœ„ì— ë”°ë¥¸ count aggregation functionì´ ì ìš©ë˜ì–´ ê·¸ë¦¬ê³ ìž í•œ ì°¨íŠ¸ê°€ ìž˜ ê·¸ë ¤ì¡ŒìŒì„ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.ë”°ë¼ì„œ, ì €í¬ê°€ í•™ìŠµì‹œí‚¤ê³ ìž í•˜ëŠ” ëª¨ë¸ì€  barley.jsonê³¼ ê°™ì€ ë°ì´í„°ê°€ inputìœ¼ë¡œ ë“¤ì–´ì™”ì„ ë•Œ  ëª¨ë¸ ì•ˆì—ì„œ Vega-lite ê·¸ëž˜ë¨¸ ê¸°ë°˜ì˜ ì ì ˆí•œ plotì„ ê·¸ë¦´ ìˆ˜ ìžˆëŠ” ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ëŠ” language translation ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²ƒìž…ë‹ˆë‹¤.          ì´ë¥¼ ìœ„í•´ì„œ ì‹¤ì œ ë°ì´í„°ì˜ í•„ë“œë¥¼ str, numìœ¼ë¡œ ë°”ê¾¸ì–´ ëª¨ë¸ì— ë„£ì€ í›„ Vega-lite Specë¥¼ ì•„ì›ƒí’‹ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.        ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜¨ Vega-lite specì— ì›ë³¸ ë°ì´í„° í•„ë“œë¥¼ ë‹¤ì‹œ mapping ì‹œì¼œ ìµœì¢… Vega-lite specì„ ë§Œë“  í›„ ê·¸ëŒ€ë¡œ ì›¹ì— ì˜¬ë ¤ì£¼ë©´ ë©ë‹ˆë‹¤.Training data11ê°œì˜ ë°ì´í„°ì…‹ì— ëŒ€í•œ 4300ì—¬ê°œì˜ Vega-Lite codeë¥¼ í•™ìŠµìš© ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.ì°¸ê³ ë¡œ ë°ì´í„°ì…‹ ë¶„í¬ëŠ” Training : Eval : Test = 0.8 : 0.1 : 0.1 ìž…ë‹ˆë‹¤.Model config &amp; TrainingConfigí•™ìŠµì‹œí‚¤ëŠ” ëª¨ë¸ì˜ configìž…ë‹ˆë‹¤. ê¸°ì¡´ëŒ€ë¡œ ëª¨ë¸ì€ AttentionSeq2Seqì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë‚˜ ë‹¤ë¥¸ ë¶€ë¶„ë“¤ì„ ëª‡ ê°€ì§€ ìˆ˜ì •í•´ë³´ì•˜ìŠµë‹ˆë‹¤.  LSTM cell -&gt; GRU cell ë¡œ ë³€ê²½  Dropout ìˆ˜ì¹˜ë¥¼ 0.5 -&gt; 0.8ë¡œ ë³€ê²½  epoch ìˆ˜ë¥¼ 20000 -&gt; 15000ìœ¼ë¡œ ë³€ê²½ (3000ë§ˆë‹¤ save)Trainingì´ì œ í„°ë¯¸ë„ì— ê°€ì„œ ê°€ìƒí™˜ê²½ ìž˜ ì„¤ì •í•´ë‘ê³  ì•„ëž˜ì™€ ê°™ì€ ëª…ë ¹ì–´ë¥¼ ì¹œ í›„ í•™ìŠµì´ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ë©´ ë©ë‹ˆë‹¤.python3 -m bin.train --config_paths=\"example_configs/nmt_medium-Copy1.yml,example_configs/train_seq2seq.yml,example_configs/text_metrics_bpe.yml" [ê²°ê³¼]ëŒ€ëžµ 2ì¼ì •ë„ ì†Œìš”ë˜ì—ˆìŠµë‹ˆë‹¤.Tensorboard - Losstensorboardë¥¼ í™œìš©í•´ train ê³¼ì •ì´ ì–´ë• ëŠ”ì§€ ëª¨ë‹ˆí„°ë§ í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.ë‹¤ë“¤ ìž˜ ì•„ì‹¤ ê²ƒ ê°™ìŠµë‹ˆë‹¤ë§Œ, summary ë°ì´í„° íŒŒì¼ì´ ìžˆëŠ” ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•˜ì—¬ ì•„ëž˜ì™€ ê°™ì€ ëª…ë ¹ì–´ë¥¼ ì¹©ë‹ˆë‹¤.tensorboard --logdir .í•™ìŠµì´ ë§ˆë¬´ë¦¬ë  ì¦ˆìŒì—ëŠ” lossê°€ 0.03 ì •ë„ì˜ ê°’ì„ ê°€ì§€ê³  ìžˆì—ˆìŠµë‹ˆë‹¤.ì•„ì‰½ê²Œë„ accuracyê°™ì€ ì •ëŸ‰ì ì¸ metricì„ ì‚¬ìš©í•˜ê¸°ëŠ” ì–´ë µê¸° ë•Œë¬¸ì— ì§ì ‘ ê²°ê³¼ë¥¼ ë³´ê³  í‰ê°€í•´ë³´ê² ìŠµë‹ˆë‹¤.Web Demoì§ì ‘ í•™ìŠµì‹œí‚¨ ëª¨ë¸(model.ckpt-15000)ì„ web demoì™€ ì—°ë™ì‹œì¼°ìŠµë‹ˆë‹¤.ëª¨ë¸ì— randomí•œ test dataë¥¼ ë¶ˆëŸ¬ì™€ì„œ Inferenceí•œ í›„ Vega-lite specì„ ì›¹ì— ê·¸ë¦° ê²°ê³¼ìž…ë‹ˆë‹¤.ê·¸ëŸ´ë“¯í•œ plotì´ ê½¤ ë‚˜ì˜¤ëŠ” ê²ƒì„ ë³´ì‹¤ ìˆ˜ ìžˆìŠµë‹ˆë‹¤!Conclusioní˜„ìž¬ ì €í¬ëŠ” í”„ë¡œì íŠ¸ì— ë§žëŠ” ì›¹ íŽ˜ì´ì§€ë¥¼ êµ¬í˜„í•˜ëŠ” ì¤‘ì´ê³ , ëª¨ë¸ ê°œì„ ê³¼ ê´€ë ¨í•˜ì—¬ íŒ€ ë‚´ì—ì„œ ë…¼ì˜í•˜ê³  ìžˆìŠµë‹ˆë‹¤.ëª¨ë¸ ìª½ì€ ì €ì˜ ì¼ì´ê¸° ë•Œë¬¸ì—, ì œê°€ ìƒê°í•´ë‘” ë°”ë¡œëŠ”  plotì˜ ì¢…ë¥˜ë¥¼ ë‹¤ì–‘í™”í•œë‹¤.  plotì˜ í…Œë§ˆë¥¼ ë‹¤ë¥´ê²Œ ë§Œë“¤ ìˆ˜ ìžˆë„ë¡ í•œë‹¤.  ë” ë§Žì€ aggregation ê¸°ëŠ¥ì„ ì¶”ê°€í•œë‹¤.  (ë³¸ì§ˆì ìœ¼ë¡œëŠ” Vega-lite ë§ê³  plotlyë¡œ ê°€ê³  ì‹¶ë‹¤ëŠ” ìƒê°ì´ ìžˆìŒâ€¦) : ë‹¤ì–‘í™”ì‹œí‚¤ê¸° ìˆ˜ì›”í•´ë³´ì—¬ì„œë“±ì´ ìžˆìŠµë‹ˆë‹¤.]]></content>
      <categories>
        
          <category> Project </category>
        
      </categories>
      <tags>
        
          <tag> Capstone </tag>
        
          <tag> Visualization </tag>
        
          <tag> Recommendation </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[sudo ê¶Œí•œì´ ì—†ëŠ” ì„œë²„ì—ì„œ git-lfs ì‚¬ìš©í•˜ê¸°]]></title>
      <url>/troubleshooting/2021/05/19/git_lfs/</url>
      <content type="text"><![CDATA[How to Use git-lfs without sudo (sudo ê¶Œí•œì´ ì—†ëŠ” ì„œë²„ì—ì„œ git-lfs ì‚¬ìš©í•˜ê¸°)(base) ~$ wget https://github.com/git-lfs/git-lfs/releases/download/v2.10.0/git-lfs-linux-386-v2.10.0.tar.gz(base) ~$ tar -xvf git-lfs-linux-386-v2.10.0.tar.gzREADME.mdCHANGELOG.mdgit-lfsinstall.sh(base) ~$ chmod 755 install.sh(base) ~$ vi ./install.sh # --&gt; prefix ë³€ìˆ˜ë¥¼ í˜„ìž¬ ê¶Œí•œë°›ì€ ê²½ë¡œë¡œ ë³€ê²½(base) ~$ bash ./install.shGit LFS initialized.(base) ~$ git lfs install(base) ~$ git lfs pullFinish]]></content>
      <categories>
        
          <category> Troubleshooting </category>
        
      </categories>
      <tags>
        
          <tag> git </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Mac OS BigSur 11.2.3 "bundle exec jekyll serve" Error]]></title>
      <url>/troubleshooting/2021/05/12/BigSur_Jekyll_TroubleShooting/</url>
      <content type="text"><![CDATA[How to Solve â€œMac OS BigSur 11.2.3 bundle exec jekyll serve Errorâ€Problem1. Problem Recognitioní‰ì†Œì™€ ê°™ì´ github ë¸”ë¡œê·¸ ê¸€ì„ ì“°ê³  bundle exec jekyll serve ë¥¼ í„°ë¯¸ë„ì— ìž…ë ¥í–ˆëŠ”ë° ì•„ëž˜ì™€ ê°™ì€ ì—ëŸ¬ê°€ ëœ¸  Could not find commonmarker-0.17.13 in any of the sources.Run bundle install to install missing gems.2. To solve #1., bundle install or install commonmarker directlyâ€¦ But :(I didnâ€™t screenshot these commandsâ€™ error messageâ€¦ but if you get error message like  (Gem::FilePermissionError)    You donâ€™t have write permissions for the /Library/Ruby/Gems/2.X.0 directory.Then, it must be installed rbenv ruby-build.Solution1. brew install rbenv ruby-buildIf you installed sucessfully, then go to the number 3.But if you got another problem like meâ€¦  Error: Your CLT does not support macOS 11. It is either outdated or was modified. Please update your CLT or delete it if no updates are available. Update them from Software Update in System Preferences or run:â€¦It is probably the effect of the Big Sur update.So I solved this problem by â€¦  sudo rm -rf /Library/Developer/CommandLineToolssudo xcode-select â€“installref)  https://apple.stackexchange.com/questions/401899/-homebrew-your-clt-does-not-support-macos-11-0  https://flaviocopes.com/how-to-fix-clt-support-macos-11/          ì—¬ê¸°ì—ìžˆëŠ” ì‚¬ì§„ë“¤ì´ ì •ìƒì ìœ¼ë¡œ ë‚˜ì˜¤ë©´ OKì¸ë“¯.      2. Follow this blogâ€™s steps(~source ~/.zshrc)3. bundle installFinally, I installed commonmarker successfully.4. bundle exec jekyll serveFinish!  ì–´ëŠ ë‹¨ê³„ì¸ì§€ ê¸°ì–µì€ ì•ˆë‚˜ëŠ”ë° Gemfileì— gem 'commonmarker'ë¥¼ ì¶”ê°€í•˜ê¸°ëŠ” í–ˆìŒ. (ì´ê²Œ í•„ìˆ˜ì¸ì§€ëŠ”â€¦ ìˆœì„œê°€ ë’¤ì£½ë°•ì£½ì´ë¼ ëª¨ë¥´ê² ìŒ, ê·¼ë° bundle instasllë¡œ ë‹¤ ë˜ì§€ ì•Šì•˜ì„ê¹Œ ì‹¶ìŒ. Bundle exec Jekyll serve ë¥¼ í–ˆì„ ë•Œ 1ë²ˆê³¼ ê°™ì€ ì—ëŸ¬ê°€ ëœ¬ë‹¤ë©´ ê·¸ ë•Œ Gemfileì— ì¨ë„ ëŠ¦ì§€ ì•Šì„ ë“¯.)]]></content>
      <categories>
        
          <category> Troubleshooting </category>
        
      </categories>
      <tags>
        
          <tag> jekyll </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[VizML - A Machine Learning Approach to Visualization Recommendation]]></title>
      <url>/paper%20review/2021/05/06/VizML/</url>
      <content type="text"><![CDATA[VizML : A Machine Learning Approach to Visualization Recommendation ì„ ì½ê³  ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.VizML : A Machine Learning Approach to Visualization RecommendationAbstract  ML approach to visualization recommendation  learns visualization design choices from a large corpus of datasets and associated visualization          identify five key design choices (viz. type, encoding type, â€¦)      train models to predict these design choices using 1M dataset-viz. pairs        NN predicts wellProblem Formulation  representation          are specified using encodings that map from data to the retinal properties(position, length, color) of graphical marks(points, lines, rectangles)        That is, to create basic visualizations in many grammars or tools, an analyst specifes higher-level design choices, which we defne as statements that compactly and uniquely specify a bundle of lower-level encodings. Equivalently, each gram- mar or tool affords a design space of visualizations, which a user constrains by making choices.Trained with a corpus of datasets ${d}$ and corresponding design choices ${C}$, ML-based recommender systems treat recommendation as an optimization problem,such that predicted $ C_{rec} âˆ¼ C_{max}. $Related WorkRule-based  encode visualization guidelines as collection of â€œif-thenâ€ statements or rules.  sometimes effective, but high costML-based  learn the relationship between data and visualizations by training models on analyst interaction  DeepEye, Data2Vis, Draco-Learn          do not learn to make visualization design choices      trained with annotations on rule-generated visualizations in controlled settings -&gt; limit        DeepEye          combines rule-based visualization generation with models trained to 1) classify Good/Bad 2) rank lists of viz.      learning to rank        Data2Vis          Seq2Seq Model that maps JSON-encoded datasets to Vega-lite visualization specifications                  Vega and Vega-Lite are visualization tools implementing a grammar of graphics, similar to ggplot2.                    4300 automatically generated Vega-Lite ex.        Draco-Learn          represents 1) visualizations as logical facts 2)design guidelines as hard and soft constraints, SVM      recommends visualizations that satisfy these constraints        VizML          In terms of LEARNING TASK                  DeepEye learns to classify and rank visualizations          Data2Vis learns an end-to-end generation model          Draco-Learn learns soft constraints weights          By learning to predict design choices, VizML models are easier to quantitatively validate, provide interpretable measures of feature importance, and can be more easily integrated into visualization systems.                    In terms of DATA QUANTITY â€¦        BUT 3 ML-Based systems recommend both data queries and visual encodings, while VizML only recommends the latter.DataFeature Extractingcode  We map each dataset to 841 features, mapped from 81 single- column features and 30 pairwise-column features using 16 aggregation functions.  Each Col. -&gt; 81 single-column features across four categories  Dimension(D) feature = # of rows in col.  Types(T) feature = categorical/temporal/quantitative  Values(V) feature = the statistical &amp; structural properties of the values within a col.  Names(N) feature = column name  We distinguish between these feature categories for three reasons.      First, these categories let us organize how we create and interpret features.    Second, we can observe the contribution of diferent types of features.    Third, some categories of features may be less generalizable than others.    We order these categories (D â†’ T â†’ V â†’ N) by how biased we expect those features to be towards the Plotly corpus.    We create 841 dataset-level features by aggregating these single- and pairwise-column features using the 16 ag- gregation functionsDesign Choice Extraction  Examples of encoding-level design choices include mark type, such as scatter, line, bar; and X or Y column encoding, which specifes which column is represented on which axis; and whether or not an X or Y column is the single column represented along that axis.  By aggregating these encoding-level design choices, we can characterize visualization-level design choices of a chartMethodsFeature PreprocessingPrediction Tasks  Two visualization-level prediction tasks          Dataset-level features to predict visualization-level design      1) Visualization Type[VT]      2) Has Shared Axis [HSA]        Three encoding-level prediction tasks          use features about individual columns to predict how theay are visually encoded      consider col. indep.      1) Mark Type[MT]      2) Is Shared X-axis or Y-axis [ISA]      3) Is on X-axis or Y-axis [XY]        For the VT, MT tasks, the 2- class task predicts line vs. bar, and the 3-class predicts scatter vs. line vs. bar.Neural Network and Baseline Models  In terms of features, we constructed four diferent feature sets by incrementally adding the Dimensions (D), Types (T), Values (V), and Names (N) categories of features, in that order. We refer to these feature sets as D, D+T, D+T+V, and D+T+V+N=All. The neural network was trained and tested using all four feature sets independently. The four base- line models only used the full feature set (D+T+V+N=All).Additional Studies  DeepEye: Towards Automatic Data Visualization.  Data2Vis: Automatic Generation of Data Vi-sualizations Using Sequence to Sequence Recurrent Neural Networks.  Draco-Learn : Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco.  Vega- Lite: A Grammar of Interactive Graphics.  Polaris: a system for query, analysis, and visualization of multidimensional databases]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Machine Learning </tag>
        
          <tag> Visualization </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Data2Vis - Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural Networks]]></title>
      <url>/paper%20review/2021/05/06/Data2Vis/</url>
      <content type="text"><![CDATA[Data2Vis: Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural Networks ì„ ì½ê³  ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.Data2Vis: Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural NetworksAbstract  end-to-end trainable neural translation model  formulate visualization generation as a language translation problem, where data specifications are mapped to visualization specifications in a declarative language (Vega-Lite).          Vege-Lite -&gt; JSON format        multilayered ateention-based encoder-decoder network with LSTM  introduce 2 metrics - language syntax validity, visualization grammar syntax validityRelated WorkDeclarative Visualization Specification  One of our aims with Data2Vis is to bridge this gap between the speed and expressivity in specifying visualizations.Automated Visulaization  We pose visualization specifica- tion as a machine translation problem and intro- duce Data2Vis, a deep neural translation model trained to automatically translate data specifica- tions to visualization specifications. Data2Vis emphasizes the creation of visualizations using rules learned from examples, without resorting to a predefined enumeration or extraction of con- straints, rules, heuristics, and features.      Machine Translation Problem  DNNs for Machine Translation  Data2Vis is also a sequence- to-sequence model using the textual source and target specifications directly for translation, with- out relying on explicit syntax representations.Model  the data visualization problem as a Seq2Seq translation problem    input : dataset (fields, values in JSON format)output : valid Vega-Lite visualization specification        encoder-decoder archi.          where the encoder reads and encodes a source sequence into a fixed length vector, and a decoder outputs a translation based on this vec- tor.        Attention Mechanism          Atten- tion mechanisms allow a model to focus on aspects of an input sequence while generating out- put tokens.        Beam Search algorithm          The beam search algorithm used in sequence-to-sequence neural translation models keeps track of k most probable output tokens at each step of decoding, where k is known as the beamwidth. This enables the generation of k most likely output sequences for a given input sequence.            THREE techniques : bidirectional encoding, differential weighing of context via an attention mechanism, and beam search    character-based sequence modelData and Preprocessing  the model must select a subset of fields to focus on when creating visual- izations (most datasets have multiple fields that cannot all be simultaneously visualized)  the model must learn differences in data types across the data fields (numeric, string, temporal, ordinal, categorical, etc.), which in turn guides how each field is specified in the generation of a visualiza- tion specification.  the model must learn the appropriate transformations to apply to a field given its data type (e.g., aggregate transform does not apply to string fields).  view-level transforms : aggregate, bin, calculate, filter, timeUnit  field-level transforms : aggregate, bin, sort, timeUnitEvaluation Metrics  language syntax validity(lsv)          measure of how well a model learns the syntax of the underlying language used to specify the visualization.        grammar syntax validity(gsv)          a measure of how well a model learns the syntax of the grammar for visualization specification.      ExperimentsResultsLimitations  Field Selection and Transformation  Training DataFuture Work  Training Data and Training Strategy  Extending Data2Vis to Generate Multiple Plausible Visualizations  Targeting Additional Grammars  Natural Language and Visualization Specification]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Visualization </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Stanford CS231n Lec 02. Image Classification]]></title>
      <url>/cs231n/2021/03/21/cs231n_lec02/</url>
      <content type="text"><![CDATA[Stanford CS231n 2017 ê°•ì˜ë¥¼ ë“£ê³  ê°œì¸ì ìœ¼ë¡œ ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.Lecture 2 : Image Classification pipelineImage Classification  Computer Vision Task  Problem is â€¦          Semantic Gap : between image and pixels (what the computer sees)                  Computer understands the image as a big grid of numbers (800,600,3)                      Challenges (algorithm should be robust to these challenges)          Viewpoint variation                  all pixels change when the viewpoint is changed                    Illumination                  different light condition                    Deformation                  Example : catâ€¦                    Occlusion                  The image shows just â€œpartâ€ of a cat                    Background Cluttuer      I track as Variation      Image Classifierdef classify_image(image):	# some magic!	return class_label  Attempts have been made          find edges and corners        Data-Driven Approach          Collect a dataset of images and labels      Use ML to train a classifier      Evaluate the classifier on new images        First Classifier : Nearest Neighbor              train : memorize all training data          O(1)        Predict : predict the label of most similar training image          O(N)        But we want classifier that are fast at prediction; slow for training is OK.**  K-Nearest Nighbors          Take majority vote from K closest points          Distance Metric (to compare images)          L1 distance(Manhattan distance)                  Calculate the difference of image          Depends on choice of coordinate system          Use when individual vector is meaningful                    L2 distance(Euclidean distance)                  Use when generic vector in some space                      kNN on images never used          Very slow at test time      Distance metrics on pixels are not informative                  couldnâ€™t reflected â€œperceptional distanceâ€                    Curse of dimensionality                    If dimensions are increased in image, data points must densely cover to these dimensions          Training examples are exponentially needed.                    Hyperparameters &amp; Pipeline  Problem-dependent          try them all out and see what works best â€¦        Split data into train, val, and test          underlying : the same probability distribution        Cross-Validation          Split data into folds      We can know which hyperparameters are going to perform more robustly      Useful to small datasets -&gt; not used too frequently in deep learning      Linear Classification  Parametric Approach : summarize knowledge of training examples &amp; stick all that knowledge into W.  Image -&gt; f(x,W) -&gt; N numbers giving class scores          W : parameters or weights        f(x,W) = Wx + b (# of classes = 10, input dimension = 3072)          f(x,W) : 10x1      W should be 10 x 3072      x : 3072x1      b : 10x1      Wx gives classesâ€™ scores        Bias          constant vector      Not interact with training set      data independent, preferences for some classes            Overview    Interpretation of linear classifiers as template matching          1 class : 1 template (driven from training data)        Hard cases for a linear classifier  Question          how can we tell whether this W is good or bad?      ]]></content>
      <categories>
        
          <category> cs231n </category>
        
      </categories>
      <tags>
        
          <tag> cs231n </tag>
        
          <tag> Deep Learning </tag>
        
          <tag> lecture note </tag>
        
          <tag> Computer Vision </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[ì§€ë‚œ í•œ ë‹¬ì„ ëŒì•„ë³´ë©°, 2021ë…„ 2ì›” íšŒê³ ]]></title>
      <url>/logs/2021/03/14/2021_February/</url>
      <content type="text"><![CDATA[2021ë…„ 2ì›” íšŒê³  ê¸€ìž…ë‹ˆë‹¤.ëª©ì°¨  ETRI ì¸í„´ ì¢…ë£Œ  ì´í™”ë³´ì´ìŠ¤ ì¸í„°ë·°  AI/ë°ì´í„°ë¶„ì„ ë™ì•„ë¦¬ ê°œì„¤  3ì›”ì˜ ê°ì˜¤ETRI ì¸í„´ ì¢…ë£Œë²Œì¨ ë‘ ë‹¬ì´ ì§€ë‚˜ ì—°êµ¬ì—°ìˆ˜ìƒ ê¸°ê°„ì´ ëë‚¬ë‹¤. ì‹œê°„ì€ ë‚´ ìƒê°ë³´ë‹¤ë„ ë” ìœì‚´ê°™ì´ í˜ëŸ¬ê°„ë‹¤ëŠ” ê±¸ ëª¸ì†Œ ëŠë‚„ ìˆ˜ ìžˆì—ˆë‹¤.ê²°ë¡ ì ìœ¼ë¡œ ë§í•˜ë©´ ETRI ì¸í„´ì„ í•˜ê¸¸ ì°¸ ìž˜í–ˆë‹¤.ì¢‹ì€ ë™ê¸°ë“¤, ë©‹ìžˆëŠ” ë°•ì‚¬ë‹˜ë“¤ê³¼ ì„ ë°°ë‹˜ë“¤, ì´  ëª¨ë‘ì™€ í•¨ê»˜ í•  ìˆ˜ ìžˆì–´ ì˜ê´‘ì´ì—ˆë‹¤.ë˜ ê·¸ë™ì•ˆ ìž˜ ëª°ëžê¸°ì— ê´€ì‹¬ ì—†ë˜ ë¶„ì•¼ì— ëŒ€í•´ ìƒˆë¡­ê²Œ ì•Œê²Œ ë˜ëŠ” ì¾Œê°ì„ ëŠë‚„ ìˆ˜ ìžˆì—ˆë‹¤.íŠ¹í—ˆëŠ” ìž‘ì„±í–ˆê³  ë…¼ë¬¸ì€ ìž‘ì„±í• ì§€ ì•ˆí• ì§€ ë¯¸ì§€ìˆ˜ì¸ ìƒíƒœë¡œ ë– ë‚¬ë‹¤. ë…¼ë¬¸ì„ ë‹¹ì—°ížˆ ì¨ì•¼ëœë‹¤ê³  ìƒê°í–ˆì§€ë§Œ ì´ê²ƒì €ê²ƒ ìš°ë ¤ë˜ëŠ” ë¶€ë¶„ë“¤ì´ ìžˆì–´ì„œ ì§€ê¸ˆë„ ê³ ë¯¼ ì¤‘ì´ë‹¤.ë™ê¸°ë“¤í•œí…Œ íŠ¹ížˆë‚˜ ê³ ë§ˆì› ë‹¤. ë¯¸ìˆ™í•˜ê³  íˆ´íˆ´ê±°ë¦¬ëŠ” ë‚´ ì„±ê²©ì„ ìž¬ë°Œê²Œ ìž˜ ë°›ì•„ì£¼ê³  ë˜ ì„œë¡œ ì—´ì •ì ì´ë¼ ëˆ„êµ¬ í•˜ë‚˜ ë‚™ì˜¤ë˜ì§€ ì•Šê³  ì •ë§ ì—´ì‹¬ížˆ ì¼í•  ìˆ˜ ìžˆì—ˆë‹¤. ë‚´ê°€ ì•„ëŠ” ETRI ì¸í„´ë“¤, ë‹¤ë¥¸ ëž© ì—°êµ¬ì—°ìˆ˜ìƒë“¤ ì¤‘ì— ìš°ë¦¬ê°€ ê°€ìž¥ ë°”ì˜ê²Œ ì¼í•˜ì§€ ì•Šì•˜ì„ê¹Œ ì‹¶ë‹¤. ì•¼ê·¼ê³¼ ì£¼ë§ ì¶œê·¼ì„ ìžì£¼ í–ˆìœ¼ë‹ˆâ€¦ (ìš°ë¦¬ë¼ë¦¬ ë‹¤ìŒ ê¸°ìˆ˜ ì¸í„´ë“¤ì´ ë¶ˆìŒí•˜ë‹¤ê³  ê³„ì† ë§í–ˆìŒã…‹ã…‹)íŠ¹í—ˆë‚˜ ë…¼ë¬¸ì„ ë‚´ê¸°ì— ë‘ ë‹¬ì´ë¼ëŠ” ì‹œê°„ì€ ë§¤ìš° ì§§ì€ ì‹œê°„ì´ë¼ê³  ì—¬ê²¨ì¡Œë‹¤. ê·¸ëž˜ì„œ ì‚¬ì‹¤ â€˜ì•„, ëª»í•´ë„ ê·¸ë§Œì´ì§€! ë°°ìš´ ê²Œ ë§Žìœ¼ë‹ˆê¹Œâ€™ ì´ëŸ° ë¥˜ì˜ ìƒê°ì„ í–ˆì—ˆëŠ”ë° ìƒê°ë³´ë‹¤ í•  ë§Œí•˜ë‹¤. ë°•ì‚¬ë‹˜ë“¤ê»˜ì„œ ì •ë§ ë§Žì´ ë„ì™€ì£¼ì‹œê³  ì¸í”„ë¼ë„ ë¹µë¹µí•˜ë‹ˆ ë³¸ì¸ì˜ ì˜ì§€ë§Œ ìžˆë‹¤ë©´ ëª» í•  ê¸°ê°„ì´ ì ˆëŒ€ ì•„ë‹ˆì—ˆë‹¤.ETRI ì¸í„´ì— ëŒ€í•œ í›„ê¸°ëŠ” ì•„ì˜ˆ ë‹¤ë¥¸ ê¸€ë¡œ íŒŒì„œ ì“°ë ¤ê³  í•œë‹¤. ì–´ì°Œëë“  ë‚˜ëŠ” ETRI ì¸í„´ì„ ì ê·¹ ì¶”ì²œí•˜ëŠ” ìž…ìž¥ì´ë‹¤. ì´ ì •ë„ì˜ í™˜ê²½ê³¼ ì‚¬ëžŒì„ ì¶©ì¡±ì‹œí‚¤ëŠ” íšŒì‚¬ê°€ ë§Žì§€ ì•Šì„ ê±°ë¼ ìƒê°ë˜ê¸° ë•Œë¬¸ì´ë‹¤. ì‚¬ëžŒë„ ì¢‹ê³  ì¼ë„ ì¢‹ë‹¤ë©´ ë§ˆë‹¤í•  ì´ìœ ê°€ ìžˆì„ê¹Œ?ì´í™”ë³´ì´ìŠ¤ ì¸í„°ë·°ìš°ì—°ì°®ì€ ê¸°íšŒë¡œ ì´í™”ë³´ì´ìŠ¤ì— ì¸í„°ë·°ë¥¼ í•˜ê²Œ ë˜ì—ˆë‹¤.í˜„ìž¬ ë‚˜ëŠ” ì•„ëŠ” ë™ìƒì´ëž‘ ì´í™”ì—¬ëŒ€ AI/ë°ì´í„°ë¶„ì„ ë¶„ì•¼ ì˜¤ì¹´ë°©ê³¼ ë””ìŠ¤ì½”ë“œë¥¼ ê°™ì´ ìš´ì˜í•˜ê³  ìžˆëŠ”ë° ì—¬ê¸°ì— ê³„ì‹  ì´í™”ë³´ì´ìŠ¤ ê¸°ìžë‹˜ê»˜ì„œ ì½”ë¡œë‚˜ì‹œëŒ€ ë„¤íŠ¸ì›Œí‚¹ì— ëŒ€í•œ ì£¼ì œë¡œ ì¸í„°ë·°ë¥¼ ìš”ì²­í•´ì£¼ì…¨ë‹¤.êµ‰ìž¥ížˆ ì˜ê´‘ì´ì—ˆê³  ì‚¬ì‹¤ ë‚˜ëŠ” ë”±ížˆ í•œ ì¼ì´ ì—†ëŠ”ë° ì–´ë–¤ ë‹µë³€ì„ ë“œë ¤ì•¼ í• ê¹Œ ë§Žì€ ê³ ë¯¼ì„ í–ˆë‹¤.ë˜ ì´í™”ë³´ì´ìŠ¤ëŠ” í•™êµ ê³µì‹ ì˜ìž ì‹ ë¬¸ì¸ì§€ë¼ ê½¤ ë¶€ë‹´ì´ ëœ ê²ƒë„ ì‚¬ì‹¤ì´ì—ˆë‹¤.í•˜ì§€ë§Œ ê¸°ìžë²—ê»˜ì„œ êµ‰ìž¥ížˆ ë¶„ìœ„ê¸°ë¥¼ ìž˜ í’€ì–´ì£¼ì…¨ê³ , ìš°ë¦¬ê°€ ë‹µë³€í•  ìˆ˜ ìžˆì„ ì •ë„ì˜ ì¢‹ì€ ì§ˆë¬¸ë“¤ì„ ì£¼ì…”ì„œ ìž¬ë°ŒëŠ” ì¸í„°ë·°ê°€ ê°€ëŠ¥í–ˆë‹¤.ìš°ë¦¬ í•™êµ ë„¤íŠ¸ì›Œí‚¹ì˜ ìž¥ì„ ì¡°ê¸ˆì´ë¼ë„ ë„“ížˆê³  ì‹¶ì—ˆë˜ ë‚˜ì˜ ìž‘ì€ ë°”ëžŒì´ ì ì°¨ ë§Žì€ ì´í™”ì¸ì—ê²Œ ë‹¿ëŠ” ê²ƒ ê°™ì•„ ë¿Œë“¯í–ˆë‹¤. ë˜ ë” ì—´ì‹¬ížˆ (ê³µë¶€)í•´ì•¼ ê² ë‹¤ëŠ” ë™ê¸°ë¶€ì—¬ë„ í™•ì‹¤ížˆ ëë‹¤.ì´ ë„¤íŠ¸ì›Œí‚¹ì˜ ìž¥ì´ ë³„ íƒˆì—†ì´, ë¶€ë”” ì˜¤ëž˜ ê°ˆ ìˆ˜ ìžˆìœ¼ë©´ ì¢‹ê² ë‹¤.ê¸°ì‚¬ì˜ ì „ë¬¸ì€ ì—¬ê¸°ì„œ ë³¼ ìˆ˜ ìžˆë‹¤.AI/ë°ì´í„°ë¶„ì„ ë™ì•„ë¦¬ ê°œì„¤ìž‘ë…„ë¶€í„° ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ë¶„ì•¼ í•™íšŒë¥¼ í•˜ë‚˜ ë§Œë“¤ê¹Œ ê³ ë¯¼í•˜ê¸´ í–ˆìœ¼ë‚˜, ì¡¸í”„ ë“± ì‚¬ì • ìƒ ë¶€ë‹´ì´ ë§Žì´ ë˜ëŠ” ì¼ì€ ì‚¬ì‹¤ì¸ì§€ë¼ ê°€ëŠ¥í•˜ë©´ í”¼í•˜ë ¤ê³  í–ˆë‹¤. í•˜ì§€ë§Œ ê²°êµ­ ë§Œë“¤ê²Œ ë˜ì—ˆë‹¤â€¦ ã…‹ã…‹ã…‹ã…‹ ì‹¬ì§€ì–´ ì¡¸í”„ ë©”ì´íŠ¸ê¹Œì§€ ê¼¬ì…”ì„œ ê°™ì´~ìš°ë¦¬ í•™êµ ì‚¬ëžŒë“¤ì´ ì´ ë¶„ì•¼ì— ë” ë§Žì´ ì§„ì¶œí•˜ê³ , ë” ë§Žì´ ê¿ˆì„ ê¿¨ìœ¼ë©´ ì¢‹ê² ë‹¤ëŠ” ìƒê°ì€ í•­ìƒ ê°–ê³  ìžˆì—ˆê¸° ë•Œë¬¸ì— ì´ ë™ì•„ë¦¬ë¥¼ ë°˜ë“œì‹œ ìž˜ í‚¤ìš°ê³  ì‹¶ë‹¤. ì‚¬ì‹¤ ì–´ë ¤ìš¸ ê²ƒì´ ì—†ëŠ” ê²Œ ë‚˜í•œí…ŒëŠ” ë“ ë“ í•œ ìž„ì›ì§„ë“¤ë„ ìžˆê³ , ë˜ ì—´ì • ìžˆëŠ” ë¶€ì›ë“¤ê¹Œì§€ ìžˆë‹¤. ë‚´ê°€ ë°©í–¥ì„±ë§Œ ìžƒì§€ ì•Šìœ¼ë©´ ì¶©ë¶„ížˆ ëœë‹¤.ë‚´ê°€ ì–´ì—¿í•œ ì‚¬íšŒì¸ì´ ë˜ì–´ í›„ë°°ë‹˜ë“¤ì„ ë„ì™€ì¤„ ìˆ˜ ìžˆëŠ” ìœ„ì¹˜ì— ê°ˆ ë•Œê¹Œì§€ ì´ ë™ì•„ë¦¬ê°€ ì­‰ ì´ì–´ì ¸ë‚˜ê°”ìœ¼ë©´ ì¢‹ê² ë‹¤.êµ‰ìž¥ížˆ ë¹¡ì„¼ ì»¤ë¦¬í˜ëŸ¼ìœ¼ë¡œ ì§„í–‰í•˜ëŠ”ë°, íƒˆì£¼ìž ì—†ì´ ëª¨ë‘ê°€ ì„±ìž¥í•  ìˆ˜ ìžˆëŠ” í•œ í•´ê°€ ë˜ê¸¸!3ì›”ì˜ ê°ì˜¤ì‚¬ì‹¤ ì´ ê¸€ì„ ì“°ëŠ” ì‹œì ì´ 3/14ì¼ì´ê¸´ í•œë°â€¦ 3ì›”ì˜ í•  ì¼ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.  ì¡¸ì—… í”„ë¡œì íŠ¸ ì£¼ì œ í™•ì •  ê°•ì˜ ì—´ì‹¬ížˆ ë“£ê¸°, ê³¼ì œ ìž˜ í•˜ê¸°  ë™ì•„ë¦¬ ê³µë¶€ ìž˜ í•˜ê¸°  ì´êµìˆ˜ë‹˜ ê°•ì˜, ë…¼ë¬¸ ë”°ë¼ ê°€ê¸°  ë…¼ë¬¸ ìŠ¤í„°ë””  ê±´ê°• ì±™ê¸°ê¸°  (ETRI ë…¼ë¬¸ ìž‘ì„±?) -&gt; ì™„ì „ ë¯¸ì •.]]></content>
      <categories>
        
          <category> Logs </category>
        
      </categories>
      <tags>
        
          <tag> ì›”ê°„ íšŒê³  </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[ì§€ë‚œ í•œ ë‹¬ì„ ëŒì•„ë³´ë©°, 2021ë…„ 1ì›” íšŒê³ ]]></title>
      <url>/logs/2021/02/14/2021_January/</url>
      <content type="text"><![CDATA[2021ë…„ 1ì›” íšŒê³  ê¸€ìž…ë‹ˆë‹¤.ëª©ì°¨  ETRI 1ê°œì›” ì°¨  ë°ì´í„° ë¶„ì„ ìŠ¤í„°ë”” ì¢…ë£Œ  ê¸°íƒ€  í•œ ë‹¬ ë™ì•ˆ í•œ ê³µë¶€  ë‚¨ì€ ë°©í•™ ë™ì•ˆ í•  ì¼1ï¸âƒ£ ETRI 1ê°œì›” ì°¨ì–´ëŠìƒˆ ETRI ì¸í„´ í•œ ë‹¬ì´ ì§€ë‚¬ë‹¤. (ì§€ê¸ˆì€ ì•½ 2ì£¼ì •ë„ë°–ì— ì•ˆë‚¨ì•˜ì§€ë§Œâ€¦)í•œ ë‹¬ê°„ í•œ ì¼ì„ ì •ë¦¬í•´ë³´ìžë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  Active Learning ê°œë… ê³µë¶€ ë° ë…¼ë¬¸ ë¦¬ë”©  Active Learning - Uncertainty Sampling Implementation  ì‹¤í—˜ì— í•„ìš”í•œ ë°ì´í„°ë“¤ ë¼ë²¨ë§ì´ì „ê¹Œì§€ ë‚˜ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ ìžì²´ì— ê´€ì‹¬ì´ ìžˆì—ˆë˜ì§€ë¼ Active Learningì´ëž‘ Human in the Loop ê°œë…ì„ ì´ë²ˆì— ìƒˆë¡œ ì•Œì•˜ë‹¤.Active Learningì€ ìš”ì•½í•˜ìžë©´ ì ì€ ë°ì´í„° ìˆ˜ë¡œ ë†’ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìžˆëŠ” ê²ƒì´ ëª©ì ì¸ë°, ì´ ë•Œ ê·¸ ì ì€ ë°ì´í„°ë“¤ì„ ë¬´ìŠ¨ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìƒ˜í”Œë§í•˜ëŠ”ê°€ì— ê´€í•œ ì´ì•¼ê¸°ì´ë‹¤.ì±…ìž„ë‹˜ê»˜ì„œ ë ˆí¼ëŸ°ìŠ¤í•  ë§Œí•œ ë…¼ë¬¸, ì•„í‹°í´ì„ ì£¼ì…”ì„œ ì‹¤í—˜ì˜ ê°ˆí”¼ë¥¼ ë¹ ë¥´ê²Œ ìž¡ì„ ìˆ˜ ìžˆì—ˆë‹¤.ë˜ ë°ì´í„°ë¥¼ ë°›ê¸°ê¹Œì§€ ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë ¤ì„œ ê·¸ ë™ì•ˆ Uncertainty Samplingì„ êµ¬í˜„í•˜ê³  ì˜¤í”ˆ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‹¤í—˜í•´ë³´ì•˜ë‹¤.1ì›”ì´ ê°€ìž¥ íž˜ë“¤ì—ˆë˜ ì´ìœ ëŠ” ë¼ë²¨ë§^_^â€¦ ì¸ë°â€¦ë‚˜ëŠ” ì´ìª½ì˜ ëª¨ë“  ì¼ì„ ì°¸ì—¬í•˜ê¸° ì „ì— ë¼ë²¨ë§ ì¼ì„ í•˜ëŠ”ì§€ ì•ˆí•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ë¼ë²¨ë§ ì—…ë¬´ê°€ ë‚€ë‹¤ë©´ ê·¸ëƒ¥ ì•ˆí•œë‹¤^^â€¦ê·¸ëž˜ì„œ ETRI ì§€ì›í•  ë•Œë„ ë¼ë²¨ë§í•´ì•¼í•  ê²ƒ ê°™ì€ ë¶€ì„œëŠ” ë‹¤ ì œì™¸í•˜ê³  ìƒê°í–ˆëŠ”ë° ã… _ã…  ë‚´ê°€ ì§€ì›í•œ ê³³ì—ì„œ ê°‘ìžê¸° ë¼ë²¨ë§ì„ ì‹œí‚¤ì‹¤ ì¤„ì€ ëª°ëžë‹¤..!(ì–˜ê¸° ì—†ì—ˆìž–ì•„ìš”!!)ì•„ë¬´íŠ¼ 2ì£¼ê°„ ë¼ë²¨ë§ì„ í•˜ê³ ,, ë˜ 112 ë°ì´í„°ë¼ ê·¸ëŸ°ì§€ ì •ì‹  í”¼íí•´ì§ˆ ë§Œí•œ ë‚´ìš©ì´ ë§Žì•˜ë‹¤ã…‹ã…‹ã…‹ ë‚´ê°€ ë‘”í•˜ê³  ë©˜íƒˆì´ ê´œì°®ì€ íŽ¸ì´ë¼ ë‹¤í–‰ì´ì—ˆì§€, ê°ì •ì´ìž… ìž˜í•˜ëŠ” ì‚¬ëžŒì´ ë´¤ìœ¼ë©´ íž˜ë“¤ì—ˆì„ ê²ƒ ê°™ë‹¤.2ì›”ë¶€í„°ëŠ” ì‹¤í—˜ì„ ì‹œìž‘í–ˆëŠ”ë°, ê·¸ê±´ 2ì›” íšŒê³ ì— ê¸°ë¡í•  ê²ƒ.2ï¸âƒ£ë°ì´í„° ë¶„ì„ ìŠ¤í„°ë”” ì¢…ë£Œ10ì›”ì— ëª¨ì§‘í•˜ì—¬ ì§„í–‰í–ˆë˜ ë°ì´í„° ë¶„ì„ ìŠ¤í„°ë””ê°€ ë“œë””ì–´ ë§ˆë¬´ë¦¬ ëë‹¤!í›„ë°˜ë¶€ì—ëŠ” ë‚´ê°€ ì •ì‹ ì´ ì—†ì–´ì„œ ê³µë¶€ë¥¼ ë§Žì´ ëª» í–ˆì§€ë§Œã… ã…  ê·¸ëž˜ë„ ì¢‹ì€ ë²—ë“¤ì„ ë§Œë‚˜ì„œ ë„ˆë¬´ ê¸°ë¶„ì´ ì¢‹ì•˜ë‹¤ :)â€˜íŒŒì´ì¬ ë¨¸ì‹ ëŸ¬ë‹ ì™„ë²½ ê°€ì´ë“œâ€™ ë¼ëŠ” ì±…ì€ ë‚´ê°€ ì´ì „ì— í•œ ë²ˆ ì½ì€ ì±…ì´ê¸´ í•˜ì§€ë§Œ, ì´ë ‡ê²Œ ë²—ë“¤ì´ëž‘ ë‹¤ì‹œ ì„¸ë¯¸ë‚˜ í˜•íƒœë¡œ ê³µë¶€í•˜ë‹ˆê¹Œ ë˜ ìƒˆë¡­ê²Œ ì™€ë‹¿ì•˜ë‹¤. ê³µë¶€ëŠ” ì—­ì‹œ í•  ë•Œë§ˆë‹¤ ìƒˆë¡œìš´ ê±´ê°€?ì´ì „ì—ëŠ” ì•™ìƒë¸”, ë¶€ìŠ¤íŒ… ê¸°ë²•ì„ ê¹Šê²Œ ìƒê°í•˜ì§€ ì•Šê³  ê³µë¶€í–ˆëŠ”ë° ì‹¤ì œ ìºê¸€ì´ë‚˜ ë°ì´ì½˜ê³¼ ê°™ì€ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì»´í”¼í‹°ì…˜ì—ì„œëŠ” ë§¤ìš° ìž˜ ì“°ì´ëŠ” ê¸°ë²•ë“¤ì´ë‹¤. ì•„ë¬´íŠ¼ ì´ëŸ¬í•œ ë‚´ìš©ë“¤ì„ ì„¸ë¯¸ë‚˜ë¥¼ ì¤€ë¹„í•˜ë©´ì„œ ê³µë¶€í•  ìˆ˜ ìžˆì–´ì„œ ì¢‹ì•˜ë‹¤.ìš°ë¦¬ í•™êµ ë²—ë“¤ì´ ë°ì´í„° ë¶„ì„ê³¼ ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ì— ë§Žì´ ë§Žì´ ì§„ìž…í–ˆìœ¼ë©´ ì¢‹ê² ë‹¤ :)3ï¸âƒ£ ê¸°íƒ€ì²« ìžì·¨ ìƒí™œë¡œ ì¸í•´ ëª¸ê³¼ ë§ˆìŒì´ ì¡°ê¸ˆ íž˜ë“¤ì—ˆì—ˆë‹¤ ã… ã… ê·¸ëž˜ì„œ 1ì›”ì—ë§Œ í˜¸ìº‰ìŠ¤ë¥¼ ë‘ ë²ˆ í–ˆë‹¤!ì²« ë²ˆì§¸ í˜¸ìº‰ìŠ¤ëŠ” ì›Œì»¤ížì´ë‹¤. ì•„ë¹ ê°€ ì›Œì»¤íž ìª½ì—ì„œ ë°”ìš°ì²˜ëž‘ ì¹´ë“œë“¤ì„ ë°›ì•„ì„œ ì›Œì»¤íž ìŠ¤ìœ„íŠ¸ë£¸ì—ì„œ ë¬µì„ ìˆ˜ ìžˆì—ˆë‹¤! ì—„ë§ˆì•„ë¹  // ë‚˜ ì´ë ‡ê²Œ í•´ì„œ ê°ê° ë°© 1ê°œì”©ì„ ì¼ë‹¤. ë‚˜ í˜¼ìž ìŠ¤ìœ„íŠ¸ë£¸ ì“´ ì ì€ ì²˜ìŒì´ë‹¤ ã…Žã…Žã…Ží•œê°•ì´ ë°”ë¡œ ë³´ì´ëŠ” ê²ƒë„ ë§ˆìŒì— ë“¤ì—ˆê³ , 4ì¸µì— ìŠ¤ì¹´ì´ì•¼ë“œë„ êµ‰ìž¥ížˆ ì¢‹ì•˜ë‹¤. ì˜¤í”ˆ ì‹œê°„ì— ë°”ë¡œ ê°„ ê±°ë¼ ì¡±ìš•ë„ í–ˆë‹¤!ë‘ ë²ˆì§¸ í˜¸ìº‰ìŠ¤ëŠ” ì—„ë§ˆê°€ ëŒ€ì „ì— ì™”ì„ ë•Œ íœ´ê°€ë‚´ê³  ê°„ â€˜ëŒ€ì „ ë¡¯ë°ì‹œí‹°í˜¸í…”â€™ì´ë‹¤. ì´ ë•Œ ì •ë§ ì„œìš¸ ê³µí™”êµ­ì´ë¼ëŠ” ê²ƒì„ ëŠê¼ˆëŠ”ë°, ëŒ€ì „ì€ ì œì¼ ì¢‹ì€ í˜¸í…”ì´ 4ì„±ê¸‰ì´ë‹¤ã…œã…œ ì•„ë¬´íŠ¼ ì´ ë•Œì¯¤ì— íž˜ë“¤ì—ˆë˜ ì¼ë“¤ì´ ë§Žì•˜ì–´ì„œ íœ´ê°€ë¥¼ ë‚´ê³  í˜¸ìº‰ìŠ¤ë¥¼ ê°”ë‹¤. ê²°ê³¼ì ìœ¼ë¡œëŠ” ë°¤ì— ì¼ì„ í•˜ê¸´ í–ˆì§€ë§Œ â€¦^^ì—¬ê¸°ëŠ” í•œê°•ë·°ëŠ” ì•„ë‹ˆê³  ê°‘ì²œ?ë·° ã…‹ã…‹ã…‹ã…‹ ë˜ê²Œ ê¹”ë”í–ˆê³ , ê°€ì„±ë¹„ ìžˆëŠ” í˜¸í…”ì´ì—ˆë‹¤!ê·¸ë¦¬ê³  ë°”ë¡œ ë§žì€ íŽ¸ì— ì„±ì‹¬ë‹¹ì´ ìžˆì–´ì„œ ë°”ë¡œ ë‹¬ë ¤ê°”ë‹¤. ë‚˜ëž‘ ìš°ë¦¬ ì—„ë§ˆëŠ” ë¹µì„ ë§¤ìš° ì¢‹ì•„í•˜ê¸° ë•Œë¬¸ì—â€¦ ì‚¬ê³  ì‹¶ì€ ë¹µë“¤ì´ ë¬´ì²™ì´ë‚˜ ë§Žì•˜ì§€ë§Œ ë‚˜ë¦„ ì ˆì œí•˜ë©° ê³¨ëžë‹¤ ã…‹ã…‹ ì´ˆì½” íŠ€ê¹€ ì†Œë³´ë¡œë¥¼ ë¨¹ì—ˆëŠ”ë° ìƒê°ë³´ë‹¤ íŒ¥ë§›ì´ ë§Žì´ ë‚˜ì„œ ì•„ì‰¬ì› ë‹¤ã… ã…  ëª…ë¬¼ì´ë¼ëŠ” ëª…ëž€ ë°”ê²ŒíŠ¸ëŠ” ì €ë…ì´ëž‘ ì•„ì¹¨ ëª¨ë‘ sold outâ€¦íœ´ê°€ ë‚ ì—ëŠ” ëŒ€ì „ í˜„ëŒ€í”„ë¦¬ë¯¸ì—„ì•„ìš¸ë ›ë„ ê°”ë‹¤! ìƒê¸´ì§€ ì–¼ë§ˆ ì•ˆëœ ê³³ì´ë¼ êµ‰ìž¥ížˆ ê¹”ë”í–ˆë‹¤. ë˜ í‰ì¼ ë‚®ì´ë¼ ê·¸ëŸ°ê°€ ì‚¬ëžŒë“¤ì´ ë³„ë¡œ ì—†ì–´ì„œ ë§¤ìš° ì¢‹ì•˜ë‹¤!ì•„ë¬´íŠ¼ 1ì›” ì¤‘ìˆœë¶€í„°ëŠ” ê°€ì¡±ë“¤ì´ ë‚´ ê¸°ë¶„ì„ í’€ì–´ì£¼ë ¤ê³  ì •ë§ ë§Žì´ ë„ì™€ì¤¬ë‹¤. ì—­ì‹œ ê°€ì¡±ì´ ìµœê³ ì•¼ &lt;3(ì¹œêµ¬ë“¤ë„ ìµœê³ ì•¼)4ï¸âƒ£í•œ ë‹¬ ë™ì•ˆ í•œ ê³µë¶€1ì›” ë™ì•ˆ í•œ ê³µë¶€ë¥¼ ì •ë¦¬í•´ë³´ìžë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  Active Learning (ì¸í„´ ì—…ë¬´)  ë…¼ë¬¸ ìŠ¤í„°ë””(Transformer, GAN, BERT, StarGAN, GPT-1)  â€¦ë”±ížˆ ê³µë¶€ë¥¼ í•œ ê±´ ë³„ë¡œ ì—†êµ¬ë‚˜. ë°˜ì„±â€¦ì§ìž¥ì„ ë‹¤ë‹ˆë©´ì„œ ê³µë¶€í•˜ëŠ” ì‚¬ëžŒë“¤ì´ ì •ë§ ëŒ€ë‹¨í•˜ê²Œ ë³´ì¸ë‹¤. ë‚˜ëŠ” ì¸í„´ì¼ ë¿ì¸ë°ë„ í‡´ê·¼í•˜ê³  ì˜¤ë©´ ë»—ê¸° ì¼ìˆ˜ì´ë‹¤ ã… ã…  ì²´ë ¥ì„ ê¸¸ëŸ¬ì•¼ í•  ê²ƒ ê°™ë‹¤5ï¸âƒ£ë‚¨ì€ ë°©í•™ ë™ì•ˆ í•  ì¼  AI/ë°ì´í„°ë¶„ì„ ë™ì•„ë¦¬ ëª¨ì§‘ ë° ì„ ë°œì›ëž˜ ë§Œë“¤ê¹Œ ë§ê¹Œ ê³ ë¯¼í–ˆëŠ”ë° ì–´ì°Œì €ì°Œ ë§Œë“¤ê²Œ ëë‹¤(í˜„ìž¬ 2ì›” ìƒí™©)ì´ë¯¸ í•˜ê²Œ ëœ ê±°, ì±…ìž„ê° ìžˆê³  ì´í™”ì˜ ì—­ì‚¬ì— í•œ ì¤„ì„ ê¸‹ëŠ” ë™ì•„ë¦¬ê°€ ë˜ë„ë¡ ì—´ì‹¬ížˆ ë…¸ë ¥í•  ê²ƒì´ë‹¤.  ë…¼ë¬¸ ìŠ¤í„°ë””  íŠ¹í—ˆ ìž‘ì„±  ë…¼ë¬¸ ìž‘ì„±  ì—¬ìœ ê°€ ëœë‹¤ë©´, Pix2Pix êµ¬í˜„  ê±´ê°•í•œ ìŒì‹ ë¨¹ê¸°(ì‹ë‹¨ ê´€ë¦¬) &amp; ìžì£¼ ê±·ê¸°íŠ¹í—ˆ ìž‘ì„±ê³¼ ë…¼ë¬¸ ìž‘ì„±ì´ ê°€ìž¥ ì£¼ê°€ ë  ë“¯ í•˜ë‹¤. ê³¼ì—° 2ì›”ì•ˆì— ë‹¤ í•  ìˆ˜ ìžˆì„ê¹Œ? í•´ì•¼ë§Œ í•œë‹¤ã… ]]></content>
      <categories>
        
          <category> Logs </category>
        
      </categories>
      <tags>
        
          <tag> ì›”ê°„ íšŒê³  </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Ubuntu 18.04 clean, ***/*** files, ***/*** blocks ë¬¸ì œí•´ê²°]]></title>
      <url>/troubleshooting/2021/01/26/Ubuntu-clean-files-blocks-trouble/</url>
      <content type="text"><![CDATA[Ubuntu 18.04ì—ì„œ clean, ***/*** files, ***/*** blocks ì—ëŸ¬ê°€ ë–´ì„ ë•Œ ê³ ì¹˜ëŠ” ë°©ë²•ì„ ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.ìž¡ì†Œë¦¬ì˜¤ëŠ˜ ì—°êµ¬ì‹¤ì—ì„œ ì •ë§ ë‹¹í™©í•œ ì—ëŸ¬ë¥¼ ë§ˆì£¼ì³¤ìŠµë‹ˆë‹¤.ì‚¬ê±´ì˜ ì—­ì‹œë‚˜ CUDAì—ì„œ ì‹œìž‘â€¦ ì €ë²ˆì£¼ê¹Œì§€ë§Œ í•´ë„ ìž˜ ëë˜ gpuì‚¬ìš©ì´ ì˜¤ëŠ˜ í•˜ë ¤ë‹ˆ ê°‘ìžê¸° ì°¾ì„ ìˆ˜ ì—†ë‹¤(?)ëŠ” ì—ëŸ¬ê°€ ë–´ìŠµë‹ˆë‹¤.ì¹¨ì°©í•˜ê²Œ ê·¸ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ êµ¬ê¸€ë§í•´ì„œ ìŠ¤íƒì˜¤ë²„í”Œë¡œìš°ì—ì„œ í•˜ë¼ëŠ” ëŒ€ë¡œ í•˜ê³  ìž¬ë¶€íŒ…í–ˆëŠ”ë° ì•„ëž˜ì™€ ê°™ì€ ì—ëŸ¬ê°€ ëœ¨ê³  ë¬´í•œ ë¸”ëž™ìŠ¤í¬ë¦°ì´ì—ˆìŠµë‹ˆë‹¤ ã…œã…œ. ì´ë¯¸ì§€ ì¶œì²˜ì§„ì‹¬ ì €ê±° ë´¤ì„ ë•Œ ëˆˆë¬¼ì´ ì°”ë” ë‚˜ì˜¬ ë»” í–ˆëŠ”ë°ìš” ..ã…‹ã…‹ã…‹ ìŠ¤íƒì˜¤ë²„í”Œë¡œìš°ë³´ë‹ˆê¹Œ ì•„ì˜ˆ ìš°ë¶„íˆ¬ë¥¼ ìž¬ì„¤ì¹˜ í•´ì•¼ í•  ìˆ˜ë„ ìžˆë‹¤ëŠ” ê¸€ì„ ë³´ê³ â€¦ ì§„ì§œ ì˜¨ëª¸ì´ ì˜¤ì‹¹í•´ì¡ŒìŠµë‹ˆë‹¤.Solution      ìš°ì„  ë‹¤ì‹œ ë¶€íŒ…í•˜ê³  recovery mode (safe mode) ë¡œ ë“¤ì–´ê°€ì•¼ í•©ë‹ˆë‹¤. ë¶€íŒ…ë  ë•Œ ë¬´í•œ Shift ëˆ„ë¥´ì‹­ì‹œì˜¤. (ìš°ë¶„íˆ¬ 18.04 ê¸°ì¤€)        Advanced options for Ubuntu ë¥¼ í´ë¦­í•˜ê³ , recovery modeë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤.ì œê°€ ìº¡ì³í•  ìƒí™©ì€ ì•„ë‹ˆì—ˆë˜ì§€ë¼ ë‹¤ ì¸í„°ë„·ì—ì„œ ë“¤ê³  ì˜¤ëŠ” ì  ì´í•´ ë¶€íƒë“œë¦½ë‹ˆë‹¤^^; ì¶œì²˜    ì´ì œ ë¦¬ì»¤ë²„ë¦¬ ëª¨ë“œì— ë“¤ì–´ì™”ëŠ”ë°ìš”. ì—¬ê¸°ì„œ ë°©í–¥í‚¤ë¥¼ ì‚¬ìš©í•´ rootë¡œ ì´ë™í•˜ê³  Enterí•©ë‹ˆë‹¤. ì´ì œ í„°ë¯¸ë„ì„ ì“¸ ìˆ˜ ìžˆìŠµë‹ˆë‹¤! ì´ì œ í„°ë¯¸ë„ì—ì„œsudo apt-get purge nvidia*ë¥¼ ì¹˜ë©´ ë©ë‹ˆë‹¤ë§Œ, ì—¬ê¸°ì„œ ì €ëŠ”dpkg was interrupted, you must manually run 'sudo dpkg --configure -a' to correct the problemë¼ëŠ” ì—ëŸ¬ê°€ ë˜ ëœ¹ë‹ˆë‹¤.ì—¬ê¸°ì„œ ì œ ë¶€íŒ… ì‹¤íŒ¨ì˜ ì›ì¸ì„ ìœ ì¶”í•´ë³¼ ìˆ˜ ìžˆì—ˆìŠµë‹ˆë‹¤. ìž¬ë¶€íŒ… ì „ì— í–ˆë˜ ë™ìž‘ë“¤ì—ì„œ ë­”ê°€ë¥¼ ê±´ë“œë ¸ê³  ê·¸ ê²°ê³¼ dpkg íŒ¨í‚¤ì§€ê°€ ì œëŒ€ë¡œ êµ¬ì„±ë˜ì§€ ì•Šì•˜ë˜ ê²ë‹ˆë‹¤.(maybeâ€¦.)ì•„ë¬´íŠ¼ ì´ ì—ëŸ¬ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì‰½ê²Œ í•´ê²°í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.ì—ëŸ¬ ë©”ì‹œì§€ëŒ€ë¡œ sudo dpkg --configure -a ë¥¼ í•´ì£¼ë©´ ë©ë‹ˆë‹¤.ì´ì œ ë‹¤ì‹œsudo apt-get purge nvidia*ë¥¼ í•˜ê²Œ ë˜ë©´ ìž˜ ì„¤ì¹˜ ë©ë‹ˆë‹¤.  reboot ë¥¼ ì¨ì„œ ìž¬ë¶€íŒ…í•©ë‹ˆë‹¤ -&gt; ì„±ê³µ!ì´ ì¼ì€ ì €ì—ê²Œ ìžˆì–´ ì •ë§ ë”ì°í–ˆë˜ ìˆœê°„ì´ì—ˆìŠµë‹ˆë‹¤. í‡´ì‚¬í•´ì•¼í•˜ë‚˜ ìƒê°ë„ í–ˆì–´ìš” ã…‹ã…‹ã…‹ ì•„ë¬´íŠ¼ ë¹ ë¥´ê²Œ ê³ ì¹˜ê³  í‡´ê·¼í–ˆìŠµë‹ˆë‹¤^^.. í˜¹ì‹œ êµ¬ê¸€ë§í•˜ë‹¤ ì €ì™€ ê°™ì€ ì—ëŸ¬ë¥¼ ë°œê²¬í•˜ì‹œê²Œ ëœë‹¤ë©´, ìœ„ ê³¼ì •ì„ í†µí•´ ê³ ì¹˜ê¸¸ ë°”ë¼ê² ìŠµë‹ˆë‹¤.]]></content>
      <categories>
        
          <category> Troubleshooting </category>
        
      </categories>
      <tags>
        
          <tag> Ubuntu </tag>
        
          <tag> Troubleshooting </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Pre-training of Deep Bidirectional Transformers for Language Understanding(BERT)]]></title>
      <url>/paper%20review/2021/01/25/BERT/</url>
      <content type="text"><![CDATA[Pre-training of Deep Bidirectional Transformers for Language Understanding(BERT) ì„ ì½ê³  ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.AbstractBERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.  bidirectionalì´ë¼ëŠ” ì ì´ ì´ì „ ì—°êµ¬ë“¤ê³¼ì˜ ê°€ìž¥ í° ì°¨ì´ì ì´ë‹¤.As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create SOTA ~...  fine-tuningì´ ë§¤ìš° ìš©ì´í•œ ëª¨ë¸ì´ë‹¤ì—¬ëŸ¬ ê°€ì§€ taskì—ì„œ SOTAë¥¼ ê¸°ë¡í–ˆìŒ.IntroductionApplying pre-trained language representations to downstream tasks  feature-based          ELMo        fine-tuning          GPT      Limitation of Unidirectional approachë‘ ì ‘ê·¼ë²• ëª¨ë‘ unidirectional ë°©ë²•ì„ ì‚¬ìš©í•¨.  But, unidirectional ë°©ë²•ì€ í•œê³„ì ì´ ëšœë ·í•¨.  GPT(1)ì²˜ëŸ¼ left-to-right architectureë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´, ëª¨ë“  tokenë“¤ì€ ì´ì „ì˜ tokenë“¤ì—ë§Œ self-attentioní•  ìˆ˜ ìžˆê²Œ ëœë‹¤.          ì´ëŠ” token-level task (ì˜ˆë¥¼ ë“¤ì–´ QA)ë“¤ì— ë¶€ì í•©í•˜ë‹¤. ì´ëŸ¬í•œ íƒœìŠ¤í¬ë“¤ì€ ì–‘ ë°©í–¥ì„ ë´ì„œ contextë¥¼ ì½ì–´ì•¼í•˜ê¸° ë•Œë¬¸.      Masked Langauge Model(MLM)BERTì—ì„œëŠ” pre-training í•  ë•Œ, ìœ„ì—ì„œ ì–¸ê¸‰í•œ unidirectionality constraintë¥¼ Masked Language Model(MLM) ìœ¼ë¡œ ì™„í™”(?)í–ˆìŒ.  MLMì€ input í† í°ë“¤ë¡œë¶€í„° randomly maskí•¨.          ì´ê²ƒì˜ ëª©ì ì€, ì˜¤ì§ contextë§Œ ê°€ì§€ê³  maskëœ original vocab.ì„ ë§žì¶”ëŠ” ê²ƒ.        Left-to-right LM pre-trainingê³¼ ë‹¬ë¦¬, MLMì˜ ëª©ì  í•¨ìˆ˜ëŠ” ì–‘ ë°©í–¥ì˜ representationsë¥¼ ì´í•´í•  ìˆ˜ ìžˆë‹¤.      which allows us to pre-train a deep bidirectional Transformer      Next Sentence Predictionjointly pre-train text-pair representationBERTPre-trainingDuring pre-training, the model is trained on unlabeled data over different pre-training tasks.Pre-training ë‹¨ê³„ì—ì„œ ëª¨ë¸ì€ unlabeled dataë¥¼ ê°€ì§€ê³  í•™ìŠµëœë‹¤.TWO Unsupervised taskë¥¼ ì‚¬ìš©í•¨.Task #1. Masked LMdeep bidirectional representationì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´, input tokenì—ì„œ ëžœë¤ìœ¼ë¡œ ëª‡ %ì •ë„ë¥¼ maskingí•˜ê³  ì´ masked tokenë“¤ì„ predictí•˜ë„ë¡ í•™ìŠµì‹œí‚´.            [MASK]      Random      Unchanged                  80%      10%      10%      Task #2. Next Sentence Prediction (NSP)ë‘ ë¬¸ìž¥ ì‚¬ì´ì˜ ê´€ê³„(relationship)ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ í•™ìŠµí•˜ëŠ” taskì´ë‹¤.IsNextì¸ì§€ ì•„ë‹Œì§€(NotNext) ë¶„ë¥˜í•˜ëŠ” binary classification ë¬¸ì œìž„.Fine-tuning BERTì ì ˆí•œ Inputê³¼ Outputì„ ë„£ìœ¼ë©´, single textë‚˜ text pair ëª¨ë‘ ê°€ëŠ¥.BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences.]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> BERT </tag>
        
          <tag> Attention </tag>
        
          <tag> Transformer </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[2020ë…„ì„ ëŒì•„ë³´ë©°, 2020 YEAR in REVIEW]]></title>
      <url>/logs/2020/12/31/2020_YEAR_in_REVIEW/</url>
      <content type="text"><![CDATA[2020ë…„ ì €ì—ê²Œ ìžˆì—ˆë˜ ì¼ì„ ì§§ê²Œ íšŒê³ í•˜ëŠ” ê¸€ìž…ë‹ˆë‹¤.ëª©ì°¨  ìˆ˜í™”ì¸ì‹ ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜ í”„ë¡œì íŠ¸ ì™„ì„±  ë°ì´í„° ë¶„ì„ ê³µë¶€  SKT AI Fellowship 2ê¸° ì§€ì›ê³¼ íƒˆë½  ë…¼ë¬¸ ìŠ¤í„°ë””  HAICON 2020 ìž…ì„   1,2í•™ê¸° ëª¨ë‘ 4ì ëŒ€ë¡œ ë§ˆë¬´ë¦¬  2021ë…„ ëª©í‘œ1ï¸âƒ£ ìˆ˜í™”ì¸ì‹ ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜ í”„ë¡œì íŠ¸ ì™„ì„±DSC Ewhaì—ì„œ ìš°ë¦¬íŒ€ì´ ì§„í–‰í•˜ë˜ í”„ë¡œì íŠ¸ëŠ” ìˆ˜í™” ì¸ì‹ê³¼ ê´€ë ¨ëœ í”„ë¡œì íŠ¸ì˜€ë‹¤.ì •í™•ížˆëŠ” â€œë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì´ìš©í•œ ìˆ˜í™” êµìœ¡ ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜â€.ì‰½ì§€ ì•Šì€ í”„ë¡œì íŠ¸ì˜€ê¸°ì— êµ‰ìž¥ížˆ ë‹¤ì‚¬ë‹¤ë‚œí–ˆì—ˆë‹¤.ë”¥ëŸ¬ë‹ì„ ì•„ì§ ìžì£¼ ë‹¤ë£¨ì§€ ì•Šì€ íŒ€ì›ë“¤ì´ ë§Žì•˜ê¸°ì—(ë‚˜ í¬í•¨) ì²˜ìŒë¶€í„° ì´ë¡  ê³µë¶€ë„ í•´ë³´ê³ , ìˆ˜í™” ì¸ì‹ ê´€ë ¨ ë…¼ë¬¸ë„ ì½ì–´ë³´ê³ , ê´€ë ¨ í”„ë¡œì íŠ¸ë“¤ë„ ë§Žì´ ì¡°ì‚¬í–ˆì—ˆë‹¤.í”„ë¡œì íŠ¸ ì´ˆê¸°ì— ì´ë ‡ê²Œ ë§Žì´ ê³ ìƒí–ˆê¸° ë•Œë¬¸ì— ìƒëŒ€ì ìœ¼ë¡œ ìš°ë¦¬ íŒ€ì€ ì¤‘í•˜ë°˜ë¶€í„°ëŠ” í•  ì¼ì´ ì—†ì—ˆë‹¤ :)íŒ€ì›ë“¤ì´ ì—´ì •, ëˆê¸°, ì˜ì§€, ì‹¤ë ¥ê¹Œì§€ ìžˆì–´ì„œ ë•ë¶„ì— ì´ í”„ë¡œì íŠ¸ë¥¼ ìˆ˜ì›”í•˜ê²Œ(?) ì™„ìˆ˜í•  ìˆ˜ ìžˆì—ˆë˜ ê²ƒ ê°™ë‹¤.êµ‰ìž¥ížˆ ë‹¤ì‚¬ë‹¤ë‚œí•œ ì¼ì´ ë§Žì•˜ì§€ë§Œâ€¦^^ ê·¸ ì´ì•¼ê¸°ë“¤ì€ ì„œë¡œì˜ ê°€ìŠ´ ì†ì— ë¬»ì–´ë‘ê¸°ë¡œ í•˜ê³  ã…‹ã…‹ã…‹ìš°ë¦¬ë§Œì˜ ìˆ˜í™” ì¸ì‹ ëª¨ë¸ì„ ë§Œë“œëŠë¼ ê°€ìž¥ ë§Žì€ ì‹œê°„ì„ ìŸì•„ ë¶€ì—ˆì—ˆë‹¤. í•˜ì§€ë§Œ ì´ë•ë¶„ì— ë‚˜ëŠ” Object Detectionì— ëŒ€í•œ ê³µë¶€ì™€ ê´€ë ¨ ê²½í—˜ì„ ìŒ“ì„ ìˆ˜ ìžˆì—ˆë‹¤.YOLO ë“± ì—¬ëŸ¬ ëª¨ë¸ë“¤ì„ ì§ì ‘ ëŒë ¤ë³´ê¸°ë„ í•˜ê³ , ì§ì ‘ ë§Œë“¤ì–´ë³´ê¸°ë„ í•˜ì˜€ë‹¤. ê²°ê³¼ì ìœ¼ë¡œëŠ” ìš°ë¦¬ íŒ€ì˜ ã…ˆã…‡ì–¸ë‹ˆê°€ ë§Œë“  ëª¨ë¸ì„ ì¼ì§€ë§Œ ðŸ˜†ì•„ë¬´íŠ¼ ì´ í”„ë¡œì íŠ¸ë¡œ ì¸í•´ ë‚˜ëŠ” ì¢‹ì€ ì‚¬ëžŒë“¤ë„ ì–»ì—ˆê³ , ë”¥ëŸ¬ë‹ì— ëŒ€í•œ í˜¸ê°ê³¼ í¥ë¯¸, ìžì‹ ê° ë“±ì„ ëª¨ë‘ ì–»ì„ ìˆ˜ ìžˆì—ˆë‹¤.í™•ì‹¤ížˆ í”„ë¡œì íŠ¸ë¥¼ í•œë²ˆ í•˜ê³  ë‚˜ë©´ ë§Žì€ ì„±ìž¥ì„ ì´ë£¨ëŠ” ê²ƒ ê°™ë‹¤. ë‚´ë…„ì˜ ë‚˜ëŠ” ì¡¸ì—… í”„ë¡œì íŠ¸ë¥¼ í†µí•´ í›¨ì”¬ ë” ì„±ìž¥í•  ìˆ˜ ìžˆê¸°ë¥¼.2ï¸âƒ£ ë°ì´í„° ë¶„ì„ ê³µë¶€ë°ì´í„° ë¶„ì„ ê³µë¶€ë¥¼ í•˜ê²Œ ëœ ê³„ê¸°ëŠ” ì¢€ ì†ë¬¼ì ì´ë‹¤.AI ë¦¬ì„œì³/ì—”ì§€ë‹ˆì–´ ì§êµ°ì„ ëª¨ì§‘í•  ë•Œ ìš°ëŒ€ì‚¬í•­ì— ìºê¸€ê³¼ ê°™ì€ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì»´í”¼í‹°ì…˜ê³¼ ê´€ë ¨ëœ ìˆ˜ìƒ ê²½í—˜ì„ ì ì–´ë†¨ê¸° ë•Œë¬¸ ^^;;ìºê¸€ê°™ì€ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ëŒ€íšŒ ìƒê¸ˆë„ ì–´ë§ˆì–´ë§ˆí•˜ê³  ë§ì´ë‹¤ã…‹ã…‹ã…‹.ìºê¸€ëŸ¬ë“¤ì˜ ì¶”ì²œì„ ë°›ì•„ íŒŒì´ì¬ ë¨¸ì‹ ëŸ¬ë‹ ì™„ë²½ ê°€ì´ë“œ (ë¶€ì œ: ê³µë£¡ì±…)ì„ ê³µë¶€í•˜ê¸° ì‹œìž‘í–ˆë‹¤.ì¸í”„ëŸ°ì—ì„œ ê°•ì˜ ë“¤ìœ¼ë©´ì„œ í—ˆê²ì§€ê² ì§„ë„ë¥¼ ëºë˜ ê¸°ì–µì´ ë‚œë‹¤.ì´ ë•Œ ì½”ë¡œë‚˜ê°€ ë§‰ ìœ í–‰í•˜ê¸° ì‹œìž‘ì¸ ë•Œë¼ ì¹œêµ¬ëž‘ ê³µë¶€ë‚˜ í•˜ìžëŠ” ìƒê°ìœ¼ë¡œ ì‹œìž‘í–ˆì—ˆë‹¤.ì†ë¬¼ì ìœ¼ë¡œ ì‹œìž‘í•œ ê³µë¶€ì´ì§€ë§Œ, ë‚˜ëŠ” ë”¥ëŸ¬ë‹ì„ í¬í•¨í•œ ì¸ê³µì§€ëŠ¥ì— ê´€ì‹¬ì´ ë§Žì•˜ê¸° ë•Œë¬¸ì— ì‹¤ìƒí™œ ë°ì´í„°ì— ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©ì‹œí‚¤ëŠ” ê²ƒì´ ë‚˜ì—ê²Œ í° í¥ë¯¸ë¡œ ë‹¤ê°€ì™”ë‹¤.ì§€ê¸ˆì€ ë¬¼ë¡  ë°ì´í„° ë¶„ì„ì´ ì´ëŠ” ì „ë¶€ëŠ” ì•„ë‹ˆë¼ëŠ” ê²ƒì€ ì•Œê³  ìžˆë‹¤.ì–´ì¨Œë“  ì´ ëŠë‚Œ? ì´ ìƒê° ë•ë¶„ì— ê³µë¶€ë¥¼ ìˆ˜ì›”í•˜ê²Œ í•  ìˆ˜ ìžˆì—ˆê³ , ìºê¸€ í•„ì‚¬ ì»¤ë¦¬í˜ëŸ¼ì„ ë”°ë¼ ê°€ë©´ì„œ(ì™„ì „ ì¡°ê¸ˆ ë”°ë¼ê°”ë‹¤ ã…‹ã…‹ã…‹ ê·¸ë•Œê°€ ê°œê°•í•  ë•Œ ì¯¤ ë˜ì–´ì„œ..) ë‚˜ì¤‘ì— ê¼­ ìºê¸€ì— ì°¸ê°€í•´ì„œ ìƒì„ ë°›ê³  ì‹¶ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤.ì´ëŠ” ì¢€ ì´ë”° ì„œìˆ í•  HAICONì— ìž…ìƒí•  ìˆ˜ ìžˆê²Œ ëœ ê³„ê¸°ê°€ ëœë‹¤ã…Žã…Ž3ï¸âƒ£ SKT AI Fellowship 2ê¸° ì§€ì›ê³¼ íƒˆë½ì„œë¥˜ í•©ê²© ðŸ˜†ë©´ì ‘ íƒˆë½ ðŸ˜‚======ì–´ì©Œë‹¤ ì•Œê²Œ ëœ SKT AI Fellowship.SKTì—ì„œ ê³µëŒ€ ëŒ€í•™(ì›)ìƒì—ê²Œ ì¸ê³µì§€ëŠ¥ê³¼ ê´€ë ¨ëœ ëª‡ëª‡ í”„ë¡œì íŠ¸ë¥¼ ì œì‹œí•˜ê³ , í° ê·œëª¨ë¡œ ì§€ì›í•´ì£¼ëŠ” í”„ë¡œê·¸ëž¨ì´ì—ˆë‹¤.ìˆ˜í™”ì¸ì‹ í”„ë¡œì íŠ¸ë¥¼ ê°™ì´í•œ íŒ€ì›ë“¤ì—ê²Œ ë¬¼ì–´ë³¸ í›„, ê°™ì´ ì¤€ë¹„í•˜ê¸°ë¡œ í–ˆë‹¤.ì£¼ì œëŠ” ë‚´ê°€ ì¢‹ì•„í•˜ëŠ”(ã…‹ã…‹ã…‹) ë¦¬ê·¸ì˜¤ë¸Œë ˆì „ë“œì˜ í•˜ì´ë¼ì´íŠ¸ ìžë™ ì¶”ì¶œ ì—°êµ¬ í”„ë¡œì íŠ¸ì˜€ë‹¤.ë¦¬ê·¸ì˜¤ë¸Œë ˆì „ë“œë¼ ë”± ì§€ì¹­í•˜ì§€ëŠ” ì•Šê³ , ìŠ¤í¬ì¸ ë¼ê³  ë§í–ˆì§€ë§Œ ë‚´ê°€ ì›Œë‚™ ë¡¤ì„ ì¦ê²¨ë³´ê¸° ë•Œë¬¸ì— ì´ ì£¼ì œë¡œ ì–´í•„ì„ ê°•í•˜ê²Œ í–ˆë‹¤.ê´€ë ¨ ì—°êµ¬ ë…¼ë¬¸ë“¤, ë‹¤ë¥¸ ìŠ¤í¬ì¸ ë“¤(ex.ì•¼êµ¬)ì— ì ìš©ëœ í•˜ì´ë¼ì´íŠ¸AI ê¸°ìˆ  ë“±ì„ ì¡°ì‚¬í•˜ì—¬ ìš°ë¦¬ë„ ì•„ì´ë””ì–´ë¥¼ ëƒˆë‹¤.ì´ë•ŒëŠ” ì •ë§ ë³„ ê¸°ëŒ€ ì•ˆí–ˆëŠ”ë°, ë©´ì ‘ê¹Œì§€ ì˜¬ë¼ê°”ë‹¤! ë©´ì ‘ì€ ê° ì£¼ì œë‹¹ ìµœì¢… 3íŒ€(ìš°ë¦¬ ì£¼ì œëŠ” ê·¸ëž¬ëŠ”ë° í™•ì‹¤í•˜ì§„ ì•Šë‹¤)ì´ ì˜¬ë¼ê°€ë©°, ì´ ì¤‘ 1íŒ€ì´ ìµœì¢… íŒ€ìœ¼ë¡œ ì„ ì •ëœë‹¤.ìƒê°ì§€ë„ ëª»í•œ ê²°ê³¼ì— ìš°ë¦¬ëŠ” ë¹ ë¥´ê²Œ ë©´ì ‘ ëŒ€ë¹„ë¥¼ í–ˆë‹¤. ìŠ¤í”„ë ˆë“œì‹œíŠ¸ì— ì˜ˆìƒ ì§ˆë¬¸ê³¼ ê·¸ì— ëŒ€í•œ ë‹µë³€ë“¤ì„ ëª¨ë‘ ì¤€ë¹„í–ˆë‹¤.ë˜ í”„ë¡œì íŠ¸ë¥¼ ì–´ë–»ê²Œ ì§„í–‰í•  ê²ƒì¸ì§€ ë°œí‘œí•´ì•¼ í–ˆëŠ”ë° ì´ëŠ” ë‚´ê°€ ë§¡ì•˜ì—ˆë‹¤.ì½”ë¡œë‚˜ë¡œ ì¸í•´ ì‹¤ì œ ë©´ì ‘ì€ Zoomìœ¼ë¡œ ì§„í–‰ë˜ì—ˆë‹¤. ë©´ì ‘ê´€ì€ 3ëª…ì´ì—ˆê³  ì¹œì ˆí•˜ê³  íŽ¸í•˜ê²Œ í•´ì£¼ì…¨ë‹¤. ë¬¼ë¡  ê¸°ìˆ ì ì¸ ì–˜ê¸°ì— ëŒ€í•´ì„œëŠ” ë‚ ì¹´ë¡­ê³  ì˜ˆë¦¬í•œ ì§ˆë¬¸, ë¹„íŒë“¤ì„ í•´ì£¼ì…¨ë‹¤.ì‚¬ì‹¤ ë©´ì ‘ ë¶„ìœ„ê¸°ê°€ êµ‰ìž¥ížˆ ì¢‹ì•˜ê³ , ìš°ë¦¬ê°€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìž˜ í–ˆê¸°ì— ê¸°ëŒ€ë¥¼ í•˜ê³  ìžˆë˜ ìƒíƒœì˜€ë‹¤ã… ã… í•˜ì§€ë§Œ ì•„ì‰½ê²Œë„ íƒˆë½ í†µë³´ë¥¼ ë°›ì•˜ë‹¤.ì§€ê¸ˆ ì™€ì„œ ê¸ì •ì ìœ¼ë¡œ ìƒê°í•´ë³´ìžë©´, ìš°ë¦¬ê°€ ê·¸ ê¸°ìˆ ì„ ì •ë§ êµ¬í˜„í–ˆì„ ìˆ˜ ìžˆëŠ” ê¸°ìˆ  ìˆ˜ì¤€ì´ì—ˆëŠ”ì§€ëŠ” ëª¨ë¥´ê² ë‹¤. ê½¤ ì–´ë ¤ìš´ ë‚œì´ë„ë¥¼ ê°€ì§„ ì—°êµ¬ë¶„ì•¼/ê¸°ìˆ ì´ê¸° ë•Œë¬¸ì´ë‹¤.ë¬¼ë¡  ì§„ì§œ ëë‹¤ë©´ ì •ë§ ì—´ì‹¬ížˆ ê³µë¶€í•˜ê³  ë…¸ë ¥í•´ì„œ ì™„ì„±í•˜ë„ë¡ ë§Œë“¤ì—ˆê² ì§€ë§Œâ€¦ ê¸ì • íšŒë¡œë¥¼ ëŒë¦°ë‹¤ë©´ ã…‹ã…‹ã…‹ ì•„ì§ ë‚´ê°€ ë¶€ì¡±í•˜ë‹¤ëŠ” ì´ì•¼ê¸°ë‹ˆê¹Œâ€¦ê·¸ëž˜ë„ ì´ ë•ë¶„ì—, ì—­ì‹œ ë‚´ íŒ€ì›ë“¤ì´ ì •ë§ ì¢‹ì€ ì‚¬ëžŒì´ìž ìš°ìˆ˜í•œ ì‚¬ëžŒë“¤ì´ì—ˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìžˆì—ˆë‹¤.ë‚´ê°€ ì•„ì§ ë§Žì´ ë¶€ì¡±í•´ì„œ ì´ë ‡ê²Œ ëœ ê²°ê³¼ë¼ëŠ” ê²ƒì„ ì¸ì •í•˜ê³ , ì•žìœ¼ë¡œ ë” ì—´ì‹¬ížˆ ê³µë¶€í•˜ê³  ë…¸ë ¥í•´ì•¼ê² ë‹¤ëŠ” ë§ˆìŒê°€ì§ì„ ë‹¤ì‹œê¸ˆ ê°€ì§ˆ ìˆ˜ ìžˆì—ˆë‹¤.4ï¸âƒ£ ë…¼ë¬¸ ìŠ¤í„°ë””ì˜¬í•´ ê°€ìž¥ í° ìˆ˜í™• ì¤‘ í•˜ë‚˜ëŠ” ì¡¸í”„ ë©”ì´íŠ¸ë¥¼ ì°¾ì€ ê²ƒì´ë‹¤.ê³¼ì— ì•„ëŠ” ì‚¬ëžŒë“¤ì´ ë§Žì§€ ì•Šê¸°ë„ í•´ì„œ í˜¼ìž í•´ì•¼ í•˜ë‚˜ ë§Žì´ ê±±ì •í–ˆëŠ”ë° ë¨¼ì € ì œì•ˆí•´ì¤€ ê²ƒì´ êµ‰ìž¥ížˆ ê³ ë§ˆì› ë‹¤ ã…Žã…Žë¹„ìŠ·í•œ ëª©í‘œë¥¼ ê°€ì§€ê³  ìžˆê³  ì„œë¡œ ì—´ì •, ì˜ì§€ë„ ê°€ë“í•˜ê¸° ë•Œë¬¸ì— ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚¼ ìˆ˜ ìžˆìœ¼ë¦¬ë¼ í™•ì‹ í•œë‹¤.ì•„ë¬´íŠ¼, ì´ ì¹œêµ¬ì™€ 7ì›”ë§ë¶€í„° ë…¼ë¬¸ ì½ê¸° ìŠ¤í„°ë””ë¥¼ ì‹œìž‘í–ˆë‹¤.ì²˜ìŒ ì‹œìž‘í•  ë•ŒëŠ” ë…¼ë¬¸ì´ë¼ëŠ” ê²ƒì´ êµ‰ìž¥ížˆ ë¶€ë‹´ìŠ¤ëŸ½ê²Œ ë‹¤ê°€ì™”ëŠ”ë°, ì´ì œëŠ” ì•„~ì£¼ ì¡°ê¸ˆì€ ìµìˆ™í•´ì ¸ì„œ í¬ê²Œ ê±°ë¶€ê°ì´ë‚˜ ë¶€ë‹´ê°ì€ ë“¤ì§€ ì•Šì•„ì¡Œë‹¤.ì´ê²ƒë§Œ í•´ë„ êµ‰ìž¥í•œ ë°œì „ ì•„ë‹Œê°€? ã…‹ã…‹ã…‹ì´ ë…¼ë¬¸ ìŠ¤í„°ë””ë¥¼ í†µí•´ ë‚´ê°€ ì–»ê²Œ ëœ ê²ƒì´ ìžˆë‹¤ë©´, â€˜ë‚´ê°€ ì´ ë¶„ì•¼ë¥¼ ì˜¤ëž«ë™ì•ˆ ì¢‹ì•„í•  ìˆ˜ ìžˆê² êµ¬ë‚˜â€™ë¼ëŠ” ìƒê°ì´ì—ˆë‹¤.ê³µë¶€ê°€ ì§€ê²¹ì§€ê°€ ì•Šê³ , ë‚´ê°€ í•´ì•¼ í•  ê²ƒì´ ì‚°ë”ë¯¸ë¼ëŠ” ê²ƒì„ ì•Œì§€ë§Œ ê·¸ê²ƒì´ ë‘ë µê±°ë‚˜ ë¶ˆì•ˆí•˜ì§€ ì•Šì•˜ë‹¤.ê·¸ëž˜ë„ ë‚˜ëŠ” ì•„ì§ ë§Žì´ ë¶€ì¡±í•˜ë‹¤! í•˜ë£¨ì— ë…¼ë¬¸ í•œíŽ¸ì”© ë§¤ì¼ë§ˆë‹¤ ë¿Œì…”ë„ ì•„ì§ ëª¨ìžë¼ë‹¤.ì§€ê¸ˆì€ ìš°ë¦¬ ë‘˜ ë‹¤ Domain Adaptation, GAN ìª½ ì—°êµ¬ì— ê´€ì‹¬ì„ ê°€ì§€ê²Œ ë˜ì–´ ê·¸ ìª½ ë…¼ë¬¸ë“¤ì„ ì½ì–´ë³´ê³  êµ¬í˜„í•˜ê³  ìžˆë‹¤. (ì•„ì§ ë§Žì´ëŠ” ëª»í–ˆì§€ë§Œ ã…‹ã…‹)5ï¸âƒ£ HAICON 2020 ìž…ì„ ì•„ê¹Œ ì˜¬í•´ ì´ˆì— ë°ì´í„°ë¶„ì„ ê³µë¶€ë¥¼ í–ˆë‹¤ê³  í–ˆëŠ”ë°, ì´ ë•Œ ê°™ì´ í•œ ì¹œêµ¬ì™€ ëŒ€íšŒë¥¼ ë‚˜ê°€ê¸°ë¡œ í–ˆë‹¤.êµ­ë‚´ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì»´í”¼í‹°ì…˜ì„ ëª¨ì•„ë†“ì€ í”Œëž«í¼ì¸ ë°ì´ì½˜ì—ì„œ í˜„ìž¬ ì§„í–‰ ì¤‘ì¸ ëŒ€íšŒì—ì„œ ê°€ìž¥ ë§Žì€ ìƒê¸ˆì„ ë¿Œë¦¬ëŠ” ëŒ€íšŒë¥¼ ê³¨ëžë‹¤ ã…‹ã…‹ã…‹.ê¸ˆì•¡ë„ ê·¸ë ‡ì§€ë§Œ, êµ­ê°€ì •ë³´ì›ê³¼ êµ­ê°€ë³´ì•ˆê¸°ìˆ ì—°êµ¬ì†Œê°€ ì§„í–‰í•œë‹¤ëŠ” ì ì—ì„œë„ í° í¥ë¯¸?ë¥¼ ëŒì—ˆë‹¤.ì²« ì°¸ê°€í•˜ëŠ” ëŒ€íšŒì´ë‹ˆ ë§Œí¼ ì •ë§ ë§Žì€ ì‹œë„ë“¤ì„ í•´ë³´ì•˜ë‹¤. ì‚¬ì‹¤ 1ë“±ê¹Œì§€ëŠ” í¬ê²Œ ë°”ë¼ì§€ëŠ” ì•Šì•˜ì§€ë§Œ ìˆ˜ìƒ ìš•ì‹¬ì€ ë¶„ëª…ížˆ ìžˆì—ˆë‹¤.ëŒ€íšŒ ì´ˆì—ëŠ” ìˆ˜ìƒê¶Œì— ìžˆì—ˆì§€ë§Œ ì¤‘í›„ë°˜ë¶€ë¡œ ë“¤ë©´ì„œ ê³ ìˆ˜ë“¤ì´ ë§Žì´ ì°¸ê°€í–ˆëŠ”ì§€ ì¡°ê¸ˆì”© ì¡°ê¸ˆì”© ë°€ë ¤ë‚¬ê³ , ê²°ê³¼ì ìœ¼ë¡œ ëŒ€íšŒ ë§ˆì§€ë§‰ ì¦ˆìŒì—” ìˆ˜ìƒê¶Œì—ì„œ ê½¤ ë©€ì–´ì¡Œì—ˆë‹¤ã… ã…  (Public score ê¸°ì¤€)ê·¼ë° ì´ê²Œ ë¬´ìŠ¨ ì¼ì¸ì§€, ì „ì²´ í…ŒìŠ¤íŠ¸ ì…‹ìœ¼ë¡œ ìµœì¢… ê²°ê³¼ë¥¼ ëŒë¦° Private scoreì—ì„œ 12ìœ„ê°€ ë‚˜ì˜¨ ê²ƒì´ë‹¤! 10ìœ„ê¹Œì§€ê°€ ìˆ˜ìƒê¶Œì´ë¼ ì•„ ì´ê±° ì§„ì§œ ì•„ê¹ë‹¤, ì¡°ê¸ˆë§Œ ë” ë…¸ë ¥í•  ê±¸ ì°°ë‚˜ í›„íšŒí•˜ê¸´ í–ˆë‹¤.ê·¸ëž˜ë„ ë‚˜ëž‘ ë‚´ ì¹œêµ¬ëŠ” ì •ë§ ìµœëŒ€í•œì˜ ë…¸ë ¥ì„ í–ˆê³ , ì •ë§ ë§Žì€ ì‹¤í—˜ì„ ê±°ì³¤ê¸° ë•Œë¬¸ì— í° ìƒìŠ¹ì€ ì—†ì—ˆì„ì§€ë„ ëª¨ë¥¸ë‹¤. (ì²˜ìŒì´ë‹¤ ë³´ë‹ˆ ì–‘ìœ¼ë¡œ ìŠ¹ë¶€í–ˆì—ˆë‹¤)ê·¸ëž˜ë„ ì¡°ê¸ˆ ê¸°ëŒ€ë¥¼ í’ˆê³ , ë³´ê³ ì„œì™€ ìµœì¢… ì œì¶œë³¸ì„ ì œì¶œí•œ ê²°ê³¼ ë‚´ ìœ„ì— 2ëª…ì´ ê·œì¹™ ìœ„ë°˜ í˜¹ì€ ë­ê°€ ë¶€ì¡±í–ˆëŠ”ì§€ ìš°ë¦¬ê°€ í„±ê±¸ì´ë¡œ ìž…ì„ ì„ í•˜ê²Œ ëœ ê²ƒì´ë‹¤!! ì§„ì§œ ê¸°ë»¤ë‹¤ ì´ë•Œ ã…Žã…Ží›„ê¸°ë¥¼ ìžì„¸ížˆ í’€ê³  ì‹¶ì§€ë§Œ ì™¸ë¶€ì— ë°œì„¤í•˜ì§€ ë§ë¼ëŠ” ì‚¬í•­ì´ ë§Žì•˜ì–´ì„œ ì´ì— ëŒ€í•œ í›„ê¸°ëŠ” ì´ê²Œ ë!! ì •ë§ ë©‹ì§„ ì‚¬ëžŒë“¤ë„ ë§Žì´ ë´¤ê³ , í•™ë¶€ìƒì€ ë‚˜ëž‘ í•œ íŒ€ë°–ì— ì—†ì—ˆë˜ ê²ƒ ê°™ì€ë° êµ‰ìž¥ížˆ ì˜ê´‘ì´ì—ˆë‹¤.ë‚˜ì¤‘ì—ëŠ” ë  ìˆ˜ ìžˆìœ¼ë©´ ìºê¸€ ëŒ€íšŒì— ìž…ìƒí•˜ê³  ì‹¶ë‹¤ ã…Žã…Ž6ï¸âƒ£ 1,2í•™ê¸° ëª¨ë‘ 4ì ëŒ€ë¡œ ë§ˆë¬´ë¦¬ì˜¬í•´ 1í•™ê¸°ëŠ” ì •ë§ íž˜ë“¤ì—ˆë‹¤. ê½¤ ë¹¡ì„¼ ê³¼ëª©ë“¤ì„ ë“£ê¸°ë„ í–ˆê³ , ì½”ë¡œë‚˜ê°€ í„°ì§€ê³  ì²« ë¹„ëŒ€ë©´ ìˆ˜ì—…ì´ë¼ ì´ë¦¬ì €ë¦¬ ì •ì‹ ì—†ì—ˆë˜ ê²ƒ ê°™ë‹¤.ë•Œë¬¸ì— 1í•™ê¸°ì—ëŠ” ì •ë§ í•™ê¸° ê³µë¶€ ì™¸ì—ëŠ” ë‹¤ë¥¸ ê³µë¶€ë¥¼ í•  ìˆ˜ê°€ ì—†ì—ˆë‹¤ ã… ã… ë‹¤ë¥¸ í•™êµë‚˜ íƒ€ê³¼ë“¤ì€ í•™ì  ìž˜ ë¿Œë ¤ì£¼ë˜ë° ã…Žã…Žâ€¦ ë‚´ê°€ ë“¤ì€ ìˆ˜ì—…ë“¤ì€ ë‹¤ ì½”ë¡œë‚˜ ì´ì „ì´ëž‘ ë¹„ìŠ·í•˜ê±°ë‚˜ ì˜¤ížˆë ¤ ë” ì§œê¸°ë„ ^^..1í•™ê¸°ëŠ” 6ì „ê³µ 1êµì–‘ 21í•™ì  ì´ìˆ˜, 4.09/4.3ì´ì—ˆê³ 2í•™ê¸°ëŠ” 6ì „ê³µ 1êµì–‘ 19í•™ì  ì´ìˆ˜, 4.2/4.3ì´ì—ˆë‹¤.2í•™ê¸°ì— ë“¤ì€ êµì–‘ìˆ˜ì—…ì€ 2017ë…„ì— ë“¤ì—ˆë˜ ê³¼ëª© í•˜ë‚˜ë¥¼ ìž¬ìˆ˜ê°•í•œ ê²ƒì´ë¼ 1-2ë“± ì ìˆ˜ì˜€ì„í…ë° ìž¬ìˆ˜ê°• ìµœëŒ€ í•™ì ì¸ A-ë¥¼ ë°›ì•„ì„œ ê°œì¸ì ìœ¼ë¡œ ì•„ì‰¬ì› ë‹¤.ê·¸ë¦¬ê³  2í•™ê¸°ëŠ” ê±°ì˜ ëª¨ë“  ê³¼ëª©ì„ 1ë“±, 2ë“± ì•„ë‹ˆë©´ ëª»í•´ë„ 5ë“± ì •ë„ í–ˆê¸° ë•Œë¬¸ì— ê°œì¸ì ìœ¼ë¡œ ë§˜ì— ë“œëŠ” í•™ê¸°ì˜€ë‹¤.ì‚¬ì‹¤ 2í•™ê¸°ì— ë“¤ì€ ê°•ì˜ë“¤ì´ ì‰½ë‹¤ê³  ëŠê¼ˆì„ ë•Œê°€ ì¢…ì¢… ìžˆì—ˆë‹¤. ë‚´ê°€ ì˜¬í•´ ë§Žì€ ì„±ìž¥ì„ í–ˆë‹¤ëŠ” ê²Œ ëŠê»´ì ¸ì„œ ë¿Œë“¯í–ˆì—ˆë‹¤.í•™ê¸°ëŠ” ì•„ì§ ê½¤ ë‚¨ì•˜ì§€ë§Œ, ë‚¨ì€ ì´ìˆ˜ í•™ì ì€ ê·¸ë ‡ê²Œ ë§Žì§€ ì•Šë‹¤. ë‚¨ì€ í•™ì ë“¤ë„ ëª¨ë‘ A+ì„ ë°›ì•„ì„œ ìµœìš°ë“±ì¡¸ì—…ì„ í•  ìˆ˜ ìžˆê²Œ ë˜ë©´ ì¢‹ê² ë‹¤!7ï¸âƒ£ 2021ë…„ ëª©í‘œì˜¬ í•œ í•´ë¥¼ ë˜ëŒì•„ë³´ë‹ˆ ì •ë§ ë§Žì€ ì¼ì´ ìžˆì—ˆë‹¤.ì½”ë¡œë‚˜ë•Œë¬¸ì— ì‚¬ëžŒë“¤ì„ ë§Žì´ ëª» ë§Œë‚¬ì§€ë§Œ, ì´ê²Œ ë‚˜ì—ê²Œ ê¸ì •ì ìœ¼ë¡œ ìž‘ìš©ë  ìˆ˜ ìžˆì—ˆë˜ ì ì€ ë¶„ëª…ížˆ ìžˆì—ˆë‹¤.í˜¼ìž ê³µë¶€í•  ì‹œê°„ì´ ëŠ˜ì–´ë‚˜ ì „ë³´ë‹¤ ê½¤ ë§Žì´ ì„±ìž¥í–ˆë‹¤ëŠ” ì ì´ ë°”ë¡œ ê·¸ê²ƒì´ë‹¤.12ì›” ì´ˆì— ì¢‹ì€ ì†Œì‹ìœ¼ë¡œëŠ” ETRI (í•œêµ­ì „ìží†µì‹ ì—°êµ¬ì›) ë™ê³„ ì—°êµ¬ì—°ìˆ˜ìƒì— í•©ê²©í–ˆë‹¤ëŠ” ê²ƒì´ë‹¤!ë‹¤ìŒì£¼ 1/2ì¼ë¶€í„° 2/26ê¹Œì§€ ëŒ€ì „ì—ì„œ ì¼ì„ í•˜ê²Œ ë˜ì—ˆëŠ”ë° êµ‰ìž¥ížˆ ì„¤ë Œë‹¤.ì¸í„´ê¸‰ì´ê¸° ë•Œë¬¸ì— ë§Žì€ ì¼ì„ ì‹œí‚¤ì§€ ì•Šì„ ê²ƒ ê°™ì§€ë§Œ, ë‚˜ ìŠ¤ìŠ¤ë¡œ ë§Žì´ ë°°ìš°ë ¤ ë…¸ë ¥í•˜ê³  ë˜ í•œ ë‹¨ê³„ ì„±ìž¥í•˜ëŠ” ê³„ê¸°ê°€ ë˜ì—ˆìœ¼ë©´ ì¢‹ê² ë‹¤.ì¼ë‹¨ 2021ë…„ ëª©í‘œëŠ” í¬ê²Œ 2ê°€ì§€ì´ë‹¤.ì¡¸ì—… í”„ë¡œì íŠ¸ ì„±ê³µì  == í•´ì™¸ íƒ‘ ì»¨í¼ëŸ°ìŠ¤ì— ë…¼ë¬¸ Accept ë˜ê¸°ë‚˜ì—ê²Œ ìžˆì–´ 2021ë…„ì€ ê°€ìž¥ ì¤‘ìš”í•œ í•œ í•´ì´ë‹¤.ì‚¬ì‹¤ìƒ ì´ë¥¼ ìœ„í•´ ì§€ê¸ˆê¹Œì§€ ì¤€ë¹„ë¥¼ í–ˆë‹¤ê³  ë´ë„ ê³¼ì–¸ì´ ì•„ë‹ˆë‹¤. ì¡¸í”„ ì‹œìž‘ì„ í•œ í•™ê¸° ë¯¸ë£¬ ê²ƒë„ ì´ë¥¼ ìœ„í•´ ê³µë¶€í•˜ê¸° ìœ„í•´ì„œì˜€ë‹¤.ë§ˆì¹¨ ì¢‹ì€ ë©”ì´íŠ¸ê°€ ìžˆê³ , ë‚˜ì™€ ë¹„ìŠ·í•˜ê²Œ ì•¼ë§ë„ í¬ê¸°ì— ìš°ë¦¬ ëª©í‘œëŠ” ìƒë‹¹ížˆ ë†’ê²Œ ìž¡ì•˜ë‹¤.ì‚¬ì‹¤ ì €ê²ƒë³´ë‹¤ í¬ë‹¤. Best Student Paper ìƒì„ ë°›ê³  ì‹¶ë‹¤!!!!!ëˆ„êµ¬ë³´ë‹¤ ì—´ì‹¬ížˆ ê³µë¶€í•˜ê³ , ëˆ„êµ¬ë³´ë‹¤ ìž˜ í•´ì•¼ ëœë‹¤ëŠ” ê²ƒì„ ì•ˆë‹¤. ë‚˜ëŠ” ë¬´ì¡°ê±´ í›„íšŒì—†ëŠ” 1ë…„ì„ ë³´ë‚¼ ê²ƒì´ë‹¤.ê¼­ ì´ ëª©í‘œê°€ ì„±ê³µí•  ìˆ˜ ìžˆë„ë¡, ìµœì„ ì„ ë‹¤í•˜ê² ë‹¤.All A+ì•žì„  ëª©í‘œì— ë¹„í•˜ë©´ ì†Œì†Œí•˜ë‹¤.ì¡¸í”„ ë•Œë¬¸ì— í•™ì ì€ 15~16 í•™ì  ì •ë„? ë§Žì§€ ì•Šê²Œ, ë¶€ë‹´ì´ ë˜ì§€ ì•ŠëŠ” ì„ ê¹Œì§€ë§Œ ë“¤ì„ ê²ƒì´ë‹¤.ëŒ€ì‹ ì— ë¬´ì¡°ê±´ ì˜¬ì—ì´ì !]]></content>
      <categories>
        
          <category> Logs </category>
        
      </categories>
      <tags>
        
          <tag> íšŒê³  </tag>
        
          <tag> Year Review </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks]]></title>
      <url>/paper%20review/2020/12/30/CycleGAN/</url>
      <content type="text"><![CDATA[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (CycleGAN) ì„ ì½ê³  ê°œì¸ì ìœ¼ë¡œ ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.ì•žì„œ ì‚´íŽ´ë³¸ DCGANì—°êµ¬ëŠ” í° ë°œì „ì„ ì´ë£¨ì—ˆì§€ë§Œ, ì´ ì—­ì‹œ unstableí•˜ì—¬ mode collapseê°€ ì¼ì–´ë‚  ìˆ˜ ìžˆëŠ” í•œê³„ì ì´ ëšœë ·í–ˆìŠµë‹ˆë‹¤.]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> GAN </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)]]></title>
      <url>/paper%20review/2020/12/25/DCGAN/</url>
      <content type="text"><![CDATA[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)ì„ ì½ê³  ê°œì¸ì ìœ¼ë¡œ ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)GANì€ Representation Learningì— íš¨ê³¼ì ì´ë¼ê³  í•©ë‹ˆë‹¤. (ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ê²ƒì´ GANì˜ learning processì™€ ê´€ë ¨ì´ ìžˆê³ , lack of heuristic cost function ë•ë¶„ì´ë¼ê³  ë§í•©ë‹ˆë‹¤.)ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ì²˜ìŒ ë‚˜ì˜¨ GANì€ í•™ìŠµí•˜ëŠ” ê²ƒì— ìžˆì–´ ë§¤ìš° unstableí•˜ë‹¤ëŠ” ë¬¸ì œì ì„ ê°€ì§€ê³  ìžˆì—ˆìŠµë‹ˆë‹¤.ë³¸ ë…¼ë¬¸ì˜ DCGANì€ í•´ë‹¹ ë¬¸ì œì  í•´ê²°ì„ í¬í•¨í•˜ì—¬ ì´ 4ê°€ì§€ì˜ Contributionsë¡œ ì •ë¦¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.  DCGANì€ stable trainingì´ ê°€ëŠ¥í•˜ë‹¤  í•™ìŠµëœ DiscriminatorëŠ” image classification íƒœìŠ¤í¬ì— ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë‹¤. (ë‹¤ë¥¸ unsupervised ì•Œê³ ë¦¬ì¦˜ë“¤ê³¼ ë¹„êµí•  ê²ƒìž„.)  GANì´ í•™ìŠµí•œ filtersë¥¼ ì‹œê°í™”í•  ìˆ˜ ìžˆê³ , íŠ¹ì • ì˜¤ë¸Œì íŠ¸ì— ëŒ€í•´ íŠ¹ì • filterë¥¼ í•™ìŠµí–ˆë‹¤ëŠ” ì ì„ ë³´ì—¬ì¤„ ìˆ˜ ìžˆë‹¤.  GeneratorëŠ” vector arithmetic propertiesë¥¼ ê°€ì§€ê³  ìžˆë‹¤.APPROACH AND MODEL ARCHITECTUREìš°ì„  DCGANì´ ê¸°ì¡´ì˜ GANê³¼ architecture ì¸¡ë©´ì—ì„œ ì–´ë–»ê²Œ ë‹¬ëžê¸°ì— í° ì„±ê³¼ë¥¼ ì´ë£° ìˆ˜ ìžˆì—ˆëŠ”ì§€ ì‚´íŽ´ë³´ê² ìŠµë‹ˆë‹¤.DCGANì´ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ì—ë„ CNNì„ ì´ìš©í•œ GANì„ ë§Œë“œëŠ” ì‹œë„ëŠ” ê³„ì† ìžˆì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ì‹œë„ë“¤ì€ ëª¨ë‘ ì„±ê³µì ì´ì§€ ëª»í–ˆì£ .DCGANì€ CNN Architectureì—ì„œì˜ ìµœì‹  ë³€í™”(?) 3ê°€ì§€ë¥¼ ì ìš©í•˜ì—¬ ì„±ê³µí•˜ì˜€ìŠµë‹ˆë‹¤.All Convolutional Net ì‚¬ìš©ìš°ì„  ê°€ìž¥ í° ë³€í™”ì ì€ Striving for Simplicity: The All Convolutional Net ì„ ì°¸ê³ í•˜ì—¬ All Convoultional Netì„ ì‚¬ìš©í–ˆë‹¤ëŠ” ì ìž…ë‹ˆë‹¤.All conv. netì€ pooling functions(ì˜ˆë¥¼ ë“¤ì–´, max pooling)ë¥¼ strided conv. ìœ¼ë¡œ ë°”ê¾¼ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ìž…ë‹ˆë‹¤. Max poolingì€ ë¯¸ë¶„ë˜ì§€ ì•ŠëŠ” ì„±ì§ˆì„ ê°€ì§„ë‹¤ê³  í•©ë‹ˆë‹¤.ì´ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë¥¼ í†µí•´ Generatorì™€ Discriminator ëª¨ë‘ ìžì‹ ë“¤ì˜ spatial downsamplingì„ í•™ìŠµí•˜ê¸°ì— ì í•©í•´ì§‘ë‹ˆë‹¤.Eliminating fully connected layers(FC layers) on top of convolutional featuresì´ ì‹œì  trendëŠ” ë§ˆì§€ë§‰ì— FC layerë¥¼ ì œê±°í•˜ê³ , global average poolingë¥¼ ì“°ëŠ” ë°©ì‹ì´ì—ˆë‹¤ê³  í•©ë‹ˆë‹¤.ë‹¤ë§Œ, ì´ global average poolingì€ ëª¨ë¸ì˜ ì•ˆì •ì„±ì€ ì˜¬ë¦¬ëŠ” ë°˜ë©´ì— convergence speedëŠ” ë–¨ì–´ëœ¨ë¦¬ëŠ” trade-off ê´€ê³„ë¥¼ ê°€ì§€ê³  ìžˆìŠµë‹ˆë‹¤.Batch Norm &amp; ReLU activationBatch NormalizationëŠ” í•™ìŠµì„ ì•ˆì •í™”ì‹œí‚¬ ìˆ˜ ìžˆëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ìž…ë‹ˆë‹¤. (normalizing the input to each unit to have zero mean and unit variance)ì´ê²ƒì€ í•™ìŠµ ë¬¸ì œì  ì¤‘ í•˜ë‚˜ì¸ initializationê³¼ deep modelì˜ gradient flowì— í° ë„ì›€ì„ ì¤„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.í•˜ì§€ë§Œ, ëª¨ë“  layerì— BNì„ ì ìš©ì‹œí‚¤ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. Generatorì˜ output layerì™€ Discriminatorì˜ input layerì—ëŠ” BNì„ ì ìš©ì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤.ë§ˆì§€ë§‰ìœ¼ë¡œ ReLUë¥¼ ì‚¬ìš©í–ˆë‹¤ëŠ” ì ì´ ì–¸ê¸‰ë˜ì–´ ìžˆëŠ”ë°, Generatorì™€ Discriminatorì— ì ìš©ë˜ëŠ” functionì´ ì•½ê°„ ë‹¤ë¦…ë‹ˆë‹¤.Generatorì—ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ReLUë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, output layerì—ëŠ” ë”°ë¡œ ReLUê°€ ì•„ë‹Œ Tanh functionì„ ì ìš©ì‹œí‚µë‹ˆë‹¤.Discriminatorì—ëŠ” Leaky ReLUë¥¼ ì ìš©í•©ë‹ˆë‹¤.(ì—¬ëŸ¬ ì‹¤í—˜ ê²°ê³¼ ì´ functionsë“¤ì´ ì¢‹ì€ ì„±ëŠ¥ì„ ëƒˆê¸° ë•Œë¬¸ì´ê² ì£ ? í™•ì‹¤í•œ ì´ìœ , ë…¼ì¦ì€ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.)DETAILS OF ADVERSARIAL TRAININGDCGANì€ LSUN, FACES, IMAGENET-1K ë°ì´í„°ì…‹ì— ëŒ€í•˜ì—¬ í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤.training setting(parameters, â€¦)ì— ëŒ€í•´ì„œëŠ” ë…¼ë¬¸ì„ ì°¸ê³ í•´ì£¼ì‹œê³ , ìœ„ ë°ì´í„°ì…‹ ì¤‘ LSUNì— ëŒ€í•´ì„œë§Œ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤ :)LSUNLSUNì€ Large-scale Scene Understandingì˜ ì¤„ìž„ë§ë¡œ, bedroom ì‚¬ì§„ë“¤ì„ ëª¨ì€ ë°ì´í„° ì…‹ìž…ë‹ˆë‹¤.ì´ ë°ì´í„°ì…‹ì„ ê°€ì§€ê³  í•™ìŠµì‹œí‚¨ ëª¨ë¸ë¡œ ìƒì„±ëœ ì´ë¯¸ì§€ëŠ” qualityê°€ ìƒë‹¹ížˆ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.í•˜ì§€ë§Œ ì´ê²ƒì´ over-fittingì´ ë˜ì–´ ì´ë ‡ê²Œ ëœ ê²ƒì¸ì§€, í•™ìŠµ ë°ì´í„°ì…‹ì— ëŒ€í•˜ì—¬ ê¸°ì–µ(memorization)í•˜ì—¬ ë§Œë“¤ì–´ì§„ ê²ƒì¸ì§€ì— ëŒ€í•´ íŒë³„í•´ë´ì•¼ í•©ë‹ˆë‹¤.  overfitting?ë³¸ ë…¼ë¬¸ì˜ Fig.2 ì™€ Fig.3ì„ í†µí•´ ëª¨ë¸ì´ ì˜¤ížˆë ¤ underfitting ë˜ì–´ìžˆìŒì„ ì–˜ê¸°í•©ë‹ˆë‹¤.underfittingì´ ì´ë£¨ì–´ì¡Œë‹¤ê³  ì—¬ê¸°ëŠ” ì´ìœ ëŠ” ì•„ì§ noise textureê°€ ëˆˆì— ë³´ì´ê¸° ë•Œë¬¸ì´ë¼ê³  í•©ë‹ˆë‹¤.(ì—„ì²­ ê°€ê¹Œì´ ë³´ì§€ ì•ŠëŠ” ì´ìƒì€ ìž˜ ëŠë¼ì§€ ëª»í•˜ê² ëŠ”ë° ë§ì´ì£ .)  memorization?ì‚¬ì‹¤ ì´ê²Œ ê°€ìž¥ ì¤‘ìš”í•œ ë¶€ë¶„ì´ë¼ê³  ë³¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ìƒˆë¡œ generate í•˜ëŠ” imageê°€ ì‚¬ì‹¤ í•™ìŠµ ë°ì´í„°ì—ì„œ ê¸°ì–µ(memorize)í•˜ì—¬ ë§Œë“¤ì–´ì§„ ê²ƒì´ë¼ë©´, ì§„ì •í•œ ì˜ë¯¸ì˜ Generateê°€ ì•„ë‹ˆë‹ˆê¹ìš”.ì´ì— ëŒ€í•´ì„œ ë³¸ ë…¼ë¬¸ì˜ ì €ìžë“¤ì€ memorizeí•˜ëŠ” ê°€ëŠ¥ì„±ì„ ì¤„ì´ê¸° ìœ„í•´ image de-duplication process(ì¤‘ë³µ ì œê±° í”„ë¡œì„¸ìŠ¤)ë¥¼ ê±°ì¹©ë‹ˆë‹¤.de-duplicationì„ í•˜ê¸° ìœ„í•´ autoencoderë¥¼ í•˜ë‚˜ ë§Œë“­ë‹ˆë‹¤.  We fit a 3072-128-3072 de-noising dropout regularized RELU autoencoder on 32x32 downsampled center-crops of training examples.EMPIRICAL VALIDATION OF DCGANs CAPABILITIESë§¨ ì²˜ìŒì— ì´ ë…¼ë¬¸ì˜ contributions ì¤‘ í•˜ë‚˜ë¡œ í•™ìŠµëœ DiscriminatorëŠ” image classification íƒœìŠ¤í¬ì— ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë‹¤ ë¼ê³  ì–˜ê¸°í–ˆì—ˆì£ ?ì´ ëª©ì°¨ì—ì„œëŠ” ì •ë§ DCGANì´ feature extractorë¡œì¨ì˜ ì—­í• ì´ ê°€ëŠ¥í•œê°€, ê·¸ëž˜ì„œ CIFAR-10 ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œë„ classification taskë¥¼ ìž˜ ìˆ˜í–‰í•˜ëŠ”ê°€ë¥¼ í™•ì¸í•´ë´…ë‹ˆë‹¤.ê²°ë¡ ì ìœ¼ë¡œ, ë‹¤ë¥¸ unsupervised ì•Œê³ ë¦¬ì¦˜ì€ K-means modelë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. (Exemplar CNN ëª¨ë¸ë³´ë‹¤ëŠ” ì¢€ ëª» ë¯¸ì¹˜ì§€ë§Œìš”.)ì¶”ê°€ì ìœ¼ë¡œ SVHN Digits ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œë„ ì‹¤í—˜ì„ í•´ë³´ì•˜ìŠµë‹ˆë‹¤. test error ì¸¡ë©´ì—ì„œ SOTAë¥¼ ë‹¬ì„±í•˜ëŠ” ì¾Œê±°ë¥¼ ì´ë£¨ì—ˆìŠµë‹ˆë‹¤.INVESTIGATING AND VISUALIZING THE INTERNALS Of THE NETWORKSë˜ ë‹¤ë¥¸ contributionsìœ¼ë¡œ  GANì´ í•™ìŠµí•œ filtersë¥¼ ì‹œê°í™”í•  ìˆ˜ ìžˆê³ , íŠ¹ì • ì˜¤ë¸Œì íŠ¸ì— ëŒ€í•´ íŠ¹ì • filterë¥¼ í•™ìŠµí–ˆë‹¤ëŠ” ì ì„ ë³´ì—¬ì¤„ ìˆ˜ ìžˆë‹¤ ì™€ GeneratorëŠ” vector arithmetic propertiesë¥¼ ê°€ì§€ê³  ìžˆë‹¤ ê°€ ìžˆì—ˆìŠµë‹ˆë‹¤.ì´ ëª©ì°¨ì—ì„œëŠ” ë‘ ë¶€ë¶„ì— ëŒ€í•´ ì„¤ëª…í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.Walking in the Latent Spacelatent spaceë¥¼ ë³€ê²½í–ˆì„ ë•Œ sharp transitions(ê¸‰ìž‘ìŠ¤ëŸ¬ìš´ ë³€í™”)ê°€ ìžˆìœ¼ë©´ ì´ëŠ” memorizationì´ ì¼ì–´ë‚¬ë‹¤ëŠ” ì‹ í˜¸ì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.ë°˜ëŒ€ë¡œ ë¶€ë“œëŸ¬ìš´ ë³€í™”ê°€ ì¼ì–´ë‚˜ë©´ memorizationì´ ëœ ê²ƒì´ ì•„ë‹ˆë¼ ì œëŒ€ë¡œ í•™ìŠµì´ ë˜ì—ˆë‹¤ê³  ë³¼ ìˆ˜ ìžˆì£ .ìœ„ì˜ ì‚¬ì§„ì„ ë³´ì‹œë©´ DCGANì˜ ê²½ìš° sharp transitionì´ ì•„ë‹Œ smoothí•œ ë³€í™”ê°€ ì´ë£¨ì–´ì¡ŒìŒì„ ë³¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.Visualizing the Discriminator Featuresì´ ë‚´ìš©ì—ì„œëŠ” Guided backpropagationì„ í†µí•´ GANì´ í•™ìŠµí•œ filtersë¥¼ ì‹œê°í™”í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.discriminatorê°€ featureë“¤ì„ í•™ìŠµí•´ì„œ, íŠ¹ì • íŒŒíŠ¸(bed, windows,â€¦)ë“¤ì— ëŒ€í•˜ì—¬ active í•˜ê³  ìžˆìŒì„ ë³¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.Manipulating the Generator RepresentationForgetting to Draw Certain Objectsì´ê±´ ë§¤ìš° ìž¬ë¯¸ìžˆëŠ” ì‹¤í—˜ìž…ë‹ˆë‹¤.ê°„ë‹¨í•˜ê²Œ ìš”ì•½í•˜ìžë©´, Generatorê°€ ë¬´ìŠ¨ representationì„ í•™ìŠµí–ˆëŠ”ì§€ ì•Œì•„ë³´ê¸° ìœ„í•˜ì—¬ íŠ¹ì • filter(ì—¬ê¸°ì„œëŠ” window filter)ë¥¼ ì‚­ì œí•´ë´…ë‹ˆë‹¤.ì¦‰, Windowë¼ëŠ” objectë¥¼ Forget í•˜ê²Œ ë˜ëŠ” ê²ƒì´ì£ .(ê¸°ìˆ ì ìœ¼ë¡œëŠ” window filterë¥¼ dropout ì‹œí‚¨ë‹¤ê³  ë§í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.)ê²°ê³¼ì ìœ¼ë¡œ ì´ ì‹¤í—˜ì—ì„œëŠ” ì°½ë¬¸ì´ ì•„ë‹Œ ë‹¤ë¥¸ representations, objectsê°€ ë“¤ì–´ê°€ê²Œ ë©ë‹ˆë‹¤!ìœ„ì˜ ê²°ê³¼ë¥¼ ë³´ë©´ ì•„ì‹œë‹¤ì‹œí”¼, ì°½ë¬¸ì´ì—ˆë˜ ê²ƒì´ ë¬¸ìœ¼ë¡œ ë°”ë€ŒëŠ” ë“± ë‹¤ë¥¸ objectë¥¼ ìƒì„±í•©ë‹ˆë‹¤.Vector Arithmetic On Face Samplesë§Žì€ ë¶„ë“¤ì´ ê°€ìž¥ ìž¬ë°Œì–´í•˜ì‹¤(?) ë¶€ë¶„ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.word embedding ê´€ë ¨í•´ì„œ vector(â€œKingâ€) - vector(â€œManâ€) + vector(â€œWomanâ€)ê°€ vector(â€œQueenâ€)ì˜ ê²°ê³¼ê°€ ë‚˜ì˜¤ë“¯ì´, DCGANì—ì„œë„ ì´ì™€ ë¹„ìŠ·í•œ arithmeticí•œ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤ê³  ë°í˜”ìŠµë‹ˆë‹¤.Generatorì˜ inputì¸ Z vectorì— ëŒ€í•œ arithmetic operationì„ í•˜ëŠ”ë°, single sampleë¡œëŠ” ë¶ˆì•ˆì •í•˜ì—¬ 3ê°œì˜ Z vectorë¥¼ í‰ê· í•œ ê°’ì„ ì‚¬ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤.smiling woman - neutral woman + neutral man = smiling man ì´ë¯¸ì§€ê°€ ë§Œë“¤ì–´ì§€ëŠ” ë§ˆë²•ê°™ì€ ê¸°ìˆ ì„ ë³´ì‹¤ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.ì•žì„œ ë§í–ˆë‹¤ì‹œí”¼ 3ê°œì˜ Z vectorë¥¼ averageí•˜ì—¬ ìƒˆë¡œìš´ Yë²¡í„°ë¥¼ ë§Œë“  ê²ƒë„ í™•ì¸í•´ë³¼ ìˆ˜ ìžˆì£ .ì´ê²Œ ë‹¤ê°€ ì•„ë‹™ë‹ˆë‹¤. face pose ë˜í•œ Z spaceì— ì„ í˜•ì ìœ¼ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.ë°”ë¡œ ì´ë ‡ê²Œ ë§ì´ì£ . ì´ë¯¸ ì´ì „ë¶€í„° scale, rotation, positionì— ëŒ€í•˜ì—¬ conditional generative modelì€ í•™ìŠµí•  ìˆ˜ ìžˆë‹¤ê³  ì—°êµ¬ë˜ì–´ì™”ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ì—°êµ¬ëŠ” purely unsupervised model ì´ë¼ëŠ” ì ì—ì„œ í° ë³€í™˜ì ì´ ëœ ê²ƒì´ì£ .FUTURE WORKì‚¬ì‹¤ stablityë¥¼ ì™„ì „ížˆ í•´ê²°í•œ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤.DCGANì„ ì˜¤ëž«ë™ì•ˆ í•™ìŠµí•˜ê²Œ ë˜ë©´ collapse mode, oscillating modeê°€ ë°œìƒí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì•„ì§ë„ ë¶ˆì•ˆì •ì„±ì´ ë‚¨ì€ ê²ƒì´ì£ .ê·¸ëž˜ì„œ ì´ ë…¼ë¬¸ì—ì„œëŠ” í•´ë‹¹ ë¬¸ì œì ì„ Future workë¡œ ë‚¨ê¸°ê³  ë§ˆë¬´ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.ReferencesJaejun Yooâ€™s PlaygroundDCGANë„ ì½ì—ˆìœ¼ë‹ˆ, êµ¬í˜„ë„ í•´ë³´ê³  í›„ì† ë…¼ë¬¸ë“¤ë„ ì°¬ì°¬ížˆ ì½ì–´ë³´ë ¤ê³  í•©ë‹ˆë‹¤.ê¸´ ê¸€ ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤ :)]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> GAN </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Unsupervised Intra-domain Adaptation for Semantic Segmentation]]></title>
      <url>/paper%20review/2020/12/22/intraDA/</url>
      <content type="text"><![CDATA[Unsupervised Intra-domain Adaptation for Semantic Segmentation ì„ ì½ê³  ê°„ëžµí•˜ê²Œ ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.Unsupervised Intra-domain Adaptation for Semantic Segmentation  automatically annotated data has a problem.          synthetic data -&gt; real data      directly adapting models from the source data to the unlabeled target data (to reduce the inter-domain gap)      But result? ==&gt; bad :(                  there is the large distribution gap among the target data itself(intra-domain gap)                    Approachtwo-step self-supervised domain adaptation approach to minimize the inter-domain and intra-domain gap together.            inter-domain gap      separate the target domain into an easy &amp; hard split (using entropy-based ranking function)                  intra-domain gap      self-supervised adaption from the easy to hard split              segmentation predictions of easy split data (from G_inter) =&gt; pseudo labels ë¡œ ì‚¬ìš©      Given easy split data &amp; pseudo labels, hard split data =&gt; D_intraëŠ” easy? hard? íŒë³„      Results]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Domain Adaptation </tag>
        
          <tag> GAN </tag>
        
          <tag> Semantic Segmentation </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Multimodal Unsupervised Image-to-Image Translation (MUNIT)]]></title>
      <url>/paper%20review/2020/12/22/MUNIT/</url>
      <content type="text"><![CDATA[Multimodal Unsupervised Image-to-Image Translation (MUNIT) ì„ ì½ê³  ê°„ëžµí•˜ê²Œ ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.Multimodal Unsupervised Image-to-Image Translation (MUNIT)Keywords : GAN, Image-to-Image translation, style transfer  image representation = content code + style code          content code : domain invariant      style code : domain specific        Image Translation == Recombine its content code with a random style code          random style code : style space of the target domain      Auto-encoder Architecture]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> GAN </tag>
        
          <tag> Domain Adaptation </tag>
        
          <tag> Style Transfer </tag>
        
          <tag> Image-to-Image Translation </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[DETR:End-to-End Object Detection with Transformers]]></title>
      <url>/paper%20review/2020/11/17/DETR/</url>
      <content type="text"><![CDATA[DETR : End-to-End Object Detection with Transformers ë¥¼ ì½ê³  ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.ì˜¬í•´ ê°€ìž¥ í° ì¸ê¸°ë¥¼ ëª°ê³  ìžˆëŠ” ë…¼ë¬¸ ì¤‘ í•˜ë‚˜ì¸ DETRì„ ì½ì–´ë³´ê² ìŠµë‹ˆë‹¤.Yann Lecun êµìˆ˜ë‹˜ê»˜ì„œ ì‚¼ì„± AI í¬ëŸ¼ì—ì„œ DETR ë…¼ë¬¸ì„ ì–¸ê¸‰í•˜ë©´ì„œ Transformer + Vision ì˜ ì—°êµ¬ê°€ ì´ì œ ì£¼ ê³¼ì œì¼ ê²ƒì´ë¼ê³  ë§ì”€í•˜ì…¨ìŠµë‹ˆë‹¤. DETRì€ ë‚˜ì˜¤ìžë§ˆìž í° í™”ì œê°€ ë˜ì—ˆê¸°ì— ì‚´ì§ ì½ì–´ë³´ê³  ë¦¬ë·°ëŠ” ì•ˆí•˜ë ¤ í–ˆìœ¼ë‚˜, ViT ë…¼ë¬¸ì„ ì½ê³  ì €ë„ ì´ ë¶„ì•¼ì˜ ìž¥ëž˜ê°€ ìœ ë§í•˜ë‹¤ê³  ì¸ì‹í•˜ê²Œ ë˜ì–´ ì´ë ‡ê²Œ í•˜ë‚˜í•˜ë‚˜ ëœ¯ì–´ë´¤ìŠµë‹ˆë‹¤!í”¼ë“œë°±ì€ ì–¸ì œë‚˜ í™˜ì˜í•©ë‹ˆë‹¤! ëŒ“ê¸€ì°½ì„ ì•„ì§ ì—´ì§€ ëª»í•´ì„œ ë©”ì¼ë¡œ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ê¾¸ë²…(__)Abstractì´ ë…¼ë¬¸ì—ì„œëŠ” object detectionì„ í•˜ë‚˜ì˜ direct set prediction ë¬¸ì œë¡œ ë³¸ë‹¤.ì´ ì ‘ê·¼ë²•ì€ Non-maximum suppression, anchor generationê³¼ ê°™ì€ ìž‘ì—…ë“¤ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë°©ë²•ìœ¼ë¡œ, detection pipelineì„ ë§¤ìš° ê°„ì†Œí™”ì‹œí‚¬ ìˆ˜ ìžˆë‹¤.ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” ëª¨ë¸ì€ DEtection Transformer(ì´í•˜ DETR)ì´ë‹¤. ì´ ëª¨ë¸ì˜ ì£¼ìš” íŠ¹ì§•ìœ¼ë¡œëŠ” object detectionì„ set prediction problemìœ¼ë¡œ ë³¸ë‹¤ í•˜ì˜€ìœ¼ë¯€ë¡œ, set ê¸°ë°˜ì˜ global lossë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ì ì´ë‹¤. Transformer encoder-decoder êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ bipartite matchingì„ í†µí•´ unique predictionì„ í•œë‹¤.í•™ìŠµëœ object ì¿¼ë¦¬ë“¤ì˜ small setì´ ì£¼ì–´ì§€ë©´, DETRì€ ê°ì²´ë“¤ê³¼ ì „ì²´ ì´ë¯¸ì§€ context ê°„ì˜ ê´€ê³„ì— ëŒ€í•´ ë³‘ë ¬ì (parallel) ìœ¼ë¡œ final setì— ëŒ€í•œ ì˜ˆì¸¡ì„ directly outputìœ¼ë¡œ ë‚´ë³´ë‚¸ë‹¤.ì´ ëª¨ë¸ì€ libraryë„ í•„ìš”ì—†ê³ , ê°œë…ì ìœ¼ë¡œ ë§¤ìš° ì‹¬í”Œí•˜ë‹¤.COCO datasetì— ëŒ€í•´ì„œ Faster R-CNN baseline ê¸‰ì˜ ì •í™•ë„ì™€ ëŸ°íƒ€ìž„ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ë‹¤ê³  í•œë‹¤.ê²Œë‹¤ê°€, DETRì€ panoptic segmentationì„ ì‰½ê²Œ ì¼ë°˜í™”í•  ìˆ˜ ìžˆë‹¤!IntroductionObject detectionì˜ ëª©í‘œëŠ” ë°”ìš´ë”© ë°•ìŠ¤ setê³¼ labelì„ ë§žì¶”ëŠ” ê²ƒì´ë‹¤.ìµœê·¼ ëŒ€ë¶€ë¶„ì˜ detectorë“¤ì€ ì´ë¥¼ set of proposals, anchors, or window centersì— ëŒ€í•´ regression/classificationì„ ì‚¬ìš©í•˜ëŠ” â€˜ê°„ì ‘ì ì¸â€™ë°©ì‹ìœ¼ë¡œ í•´ê²°í•˜ë ¤ í•œë‹¤.í•˜ì§€ë§Œ ì´ëŸ° ë°©ì‹ë“¤ì€ postprocessing ë‹¨ê³„ì—ì„œ ì¤‘ë³µëœ ì˜ˆì¸¡ê°’ë“¤ì„ collapse ì‹œí‚¤ëŠ” ê³¼ì •ì´ë‚˜(NMSë¥¼ ì–˜ê¸°í•˜ëŠ” ë“¯í•˜ë‹¤), ì•µì»¤ ì…‹ì— ëŒ€í•´ ë¯¸ë¦¬ assigní•˜ëŠ” íœ´ë¦¬ìŠ¤í‹±í•œ ë°©ì‹ ë“±ì— ì˜í•´ performanceê°€ ì˜í–¥ ë°›ì„ ìˆ˜ ìžˆë‹¤.ê·¸ëž˜ì„œ ì´ ë…¼ë¬¸ì—ì„œëŠ” ìœ„ì™€ ê°™ì€ Indirect ë°©ì‹ì´ ì•„ë‹Œ Direct ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.  We propose a direct set prediction approach to bypass the surrogate tasks.ì´ì™€ ê°™ì€ end-to-end ë°©ì‹ì€ ê¸°ì¡´ì˜ object detection ì—°êµ¬ì—ì„œëŠ” ìžì£¼ ë‹¤ë¤„ì§€ì§€ ì•Šì•˜ë˜ ê²ƒì´ë‹¤.Obejct detection ë¬¸ì œë¥¼ direct set prediction problemìœ¼ë¡œ ë§Œë“¤ì–´ training pipelineì„ ë§Œë“ ë‹¤.ì´ ë•Œ, transformer ê¸°ë°˜ì˜ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œë‹¤. íŠ¸ëžœìŠ¤í¬ë¨¸ì˜ self-attention ê¸°ë²•ì€ elementsë“¤ ê°„ì˜ ëª¨ë“  pairwise interactionsë¥¼ í†µí•´ ì¤‘ë³µëœ predictionì„ ì œê±°í•  ìˆ˜ ìžˆê²Œ í•´ì¤€ë‹¤.DETRì€ ëª¨ë“  objectë“¤ì„ í•œë²ˆì— predictí•œë‹¤. ì´ëŠ” end-to-end íŠ¹ì„±ì¸ë°, ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ ì‚¬ì´ì—ì„œ set lossë¥¼ í†µí•´ bipartite matching(ì´ë¶„ ë§¤ì¹­)ì„ í›ˆë ¨í•œë‹¤. ì´ë¡œ ì¸í•´ DETRì€ hand-designed ìž‘ì—… ì—†ì´ ê°„ë‹¨í•œ detection íŒŒì´í”„ë¼ì¸ì„ ê°€ì§ˆ ìˆ˜ ìžˆëŠ” ê²ƒì´ë‹¤.ë‹¤ë¥¸ detection modelë“¤ê³¼ ë‹¤ë¥´ê²Œ DETRì€ customized layerë„ í•„ìš”í•˜ì§€ ì•Šì•„, ëª¨ë¸ ìžì²´ë„ êµ‰ìž¥ížˆ ê°„ë‹¨í•˜ë‹¤. CNNê³¼ transformerë§Œ ê°€ì§€ê³  ìžˆë‹¤ë©´ ì‰½ê²Œ reproduce í•  ìˆ˜ ìžˆë‹¤.ì´ì „ì˜ direct set prediction ì—°êµ¬ë“¤ê³¼ ë¹„êµí•  ë•Œ, DETRì˜ ì£¼ìš”ì ì€ bipartite matching(ì´ë¶„ ë§¤ì¹­)ê³¼ parallel decodingì„ ê²°í•©ì‹œí‚¨ ê²ƒì´ë‹¤. DETRì˜ Matching loss functionì€ ì˜ˆì¸¡ê°’ì„ ground truthì— uniquely í• ë‹¹í•  ìˆ˜ ìžˆê³ , predicted objectsì˜ ìˆœì—´ì´ ë³€í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë³‘ë ¬ì ìœ¼ë¡œ predictí•  ìˆ˜ ìžˆë‹¤.DETRì€ COCO datasetì— evaluate í•´ë´¤ì„ ë•Œ, Faster R-CNN baselineì— ëŒ€í•˜ì—¬ ê²½ìŸë ¥ ìžˆëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤. íŠ¹ížˆ í° objectë“¤ì— ëŒ€í•˜ì—¬ ìƒë‹¹ížˆ ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤ê³  í•œë‹¤. í•˜ì§€ë§Œ, small objectë“¤ì— ëŒ€í•´ì„œëŠ” low performanceë¥¼ ë³´ì˜€ë‹¤. ì´ëŠ” ì•„ë§ˆ Faster R-CNNì— FPNì„ ì ìš©í•˜ì—¬ ì¢‹ì€ ì„±ê³¼ë¥¼ ì´ë¤˜ë“¯ì´ future workì—ì„œë„ í•´ê²°ë²•ì´ ìƒê¸°ì§€ ì•Šì„ê¹Œ ê¸°ëŒ€í•œë‹¤.DETRì€ ë³µìž¡í•œ ë¬¸ì œë¥¼ ì‰½ê²Œ í™•ìž¥ì‹œí‚¬ ìˆ˜ ìžˆë‹¤. ì‹¤í—˜ì—ì„œëŠ” op pretrained DETRì„ ì´ìš©í•˜ì—¬ segmentation headë¥¼ í•™ìŠµì‹œì¼°ë”ë‹ˆ Panoptic Segmentationì— ê²½ìŸë ¥ì„ ê°€ì¡Œë‹¤ê³  í•œë‹¤.Related Work  Our work build on prior work in several domains: bipartite matching losses forset prediction, encoder-decoder architectures based on the transformer, paralleldecoding, and object detection methods.The DETR modelset predictionì„ ìœ„í•´ ê°€ìž¥ ì¤‘ìš”í•œ ìš”ì†ŒëŠ” 2ê°œì´ë‹¤.  a set prediction loss that forces unique matching between predicted and ground truth boxes  an architecture that predicts (in a single pass) a set of objects and models their relationObject detection set prediction lossDETRì€ fixed-size setì—ì„œ Nê°œì˜ predictionì„ í•œë‹¤. ì´ ë•Œ Nì€ ì´ë¯¸ì§€ì˜ object ê°œìˆ˜ë³´ë‹¤ ìƒë‹¹ížˆ í¬ê²Œ ì„¤ì •í•œë‹¤.ground truthì— ëŒ€í•˜ì—¬ predicted objects(class, position, size)ë¥¼ scoringí•˜ëŠ” ê²ƒì´ í•™ìŠµê³¼ì •ì—ì„œ ì–´ë ¤ì› ë‹¤ê³  í•œë‹¤.ì—¬ê¸°ì„œ lossëŠ” ì˜ˆì¸¡ê°’ê³¼ ground truth ì‚¬ì´ì˜ ìµœì ì˜ ì´ë¶„ ë§¤ì¹­ì„ ë§Œë“¤ì–´ ë‚´ê³ , ì´ë¥¼ í†µí•´ bounding box lossë¥¼ ìµœì í™”í•œë‹¤.$y$ë¥¼ ground truth, $\hat{y} = {\hat{y_i}}_{i=1}^N$ ì€ Nê°œì˜ prediction setì´ë¼ í•˜ìž.Nì€ ì´ë¯¸ì§€ë‚´ì˜ object ê°œìˆ˜ë³´ë‹¤ í¬ë‹¤ê³  ê°€ì •í•˜ë©´, $y$ë„ a set of size Nì— $\varnothing$ (no object)ê°€ padded ëë‹¤ê³  ë³¼ ìˆ˜ ìžˆë‹¤.ì´ëŸ¬í•œ y, y_hat ì‚¬ì´ì˜ bipartite matching(ì´ëŠ” ground truthì™€ prediction ì‚¬ì´ì˜ ì¼ëŒ€ì¼ ë§¤ì¹­ì´ë¼ ìƒê°í•˜ë©´ íŽ¸í•  ë“¯) ì°¾ê¸° ìœ„í•´ì„œ, matching costê°€ minimumì´ ë˜ëŠ” ë°”ìš´ë”© ë°•ìŠ¤ì˜ ìˆœì„œë“¤, ì¦‰ ìˆœì—´ì„ ì°¾ì•„ì•¼ í•œë‹¤.ì´ëŸ¬í•œ optimal assignentëŠ” í—ê°€ë¦¬ì•ˆ ì•Œê³ ë¦¬ì¦˜ì— ì˜í•´ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°ë  ìˆ˜ ìžˆìŒ.ì´ì œ ë‹¤ì‹œ matching costì— ëŒ€í•´ ì•Œì•„ë³´ìž. each element of i of the ground truth setì€ $y_i = (c_i, b_i)$ ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìžˆìŒ. ì´ ë•Œ c_iëŠ” target class label, b_iëŠ” box center coordinates(xyì˜ center, height, width) ìž„.\[L_{Hungarian}(y,\hat{y} = \mathbb{-1}_{c_i\neq\varnothing} + \mathbb{1}L_{box}(b_i, \hat{b_{\sigma(i)}})\]ì•žì˜ í•­ì— -1ì„ ê³±í•˜ëŠ” ì´ìœ ëŠ” í•´ë‹¹ í´ëž˜ìŠ¤ probabilityê°€ ë†’ì„ìˆ˜ë¡ lossë¥¼ ë‚®ê²Œ ì‚°ì •í•˜ê¸° ìœ„í•¨ì´ê³ , ë’· í•­ì€ bounding boxì˜ ë¡œìŠ¤ë¥¼ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ì™€ ê°™ì€ ê³¼ì •ì—†ì´ ê·¸ëŒ€ë¡œ ì ìš©í•œë‹¤ëŠ” ê²ƒì´ ì¡°ê¸ˆ íŠ¹ì´í•œ ì§€ì ì´ë‹¤.ì—¬ê¸°ì„œ ì´ ë…¼ë¬¸ì˜ ì°¨ë³„ì  í˜¹ì€ ì¤‘ìš”í•œ ì§€ì ì€ ì¤‘ë³µ(duplicates)ì—†ì´ direct set predictionì„ ìœ„í•œ ì¼ëŒ€ì¼ ë§¤ì¹­ì´ ëœë‹¤ëŠ” ì ì´ë‹¤. ì¦‰, ì¤‘ë³µë˜ëŠ” predictionì´ ì¡´ìž¬í•˜ì§€ ì•Šë‹¤ëŠ” ëœ». (ì¼ëŒ€ì¼ ë§¤ì¹­ì„ í•˜ê¸° ë•Œë¬¸ì— ì¤‘ë³µë˜ëŠ” ì˜ˆì¸¡ì´ ìƒê¸°ì§€ ì•ŠìŒ -&gt; NMSê°€ í•„ìš”ì—†ë‹¤!!)ë‹¤ë¥¸ ëª¨ë¸ì—ì„œëŠ” ì•µì»¤ë°•ìŠ¤ë¥¼ ë§Žì´ ë½‘ì•„ì„œ í•˜ë‚˜ ë¬¼ì²´ì— ì—¬ëŸ¬ ê°œì˜ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ê·¸ë ¤ì„œ NMS(Non-maximum suppression)ì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ì—ˆìŒ!!ì´ë ‡ê²Œ ì¼ëŒ€ì¼ ë§¤ì¹­ëœ predictionê³¼ ground truth ì‚¬ì´ì—ì„œëŠ” ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ í—ê°€ë¦¬ì•ˆ ê¸°ë°˜ì˜ lossë¥¼ ì‚¬ìš©í•´ì„œ í•™ìŠµí•¨. (ì¦‰ ë°©ê¸ˆí–ˆë˜ ê²ƒì€ matching costë¥¼ ê³„ì‚°í•˜ëŠ” lossì˜€ê³ , ì´ë²ˆì´ ì§„ì§œ ì˜ˆì¸¡ê°’ê³¼ Ground Truthì‚¬ì´ì˜ lossì¸ë“¯?)L_matchì™€ ì•½ê°„ ë¹„ìŠ·í•´ë³´ì´ì§€ë§Œ, class í™•ë¥  ê°’ì„ no-object ì¼ ë•Œë„ ê³„ì‚°í•œë‹¤ëŠ” ì , logë¥¼ ì‚¬ìš©í•´ cross entropyê°™ì€ ì„±ì§ˆì´ ì¶”ê°€ë˜ì—ˆë‹¤ëŠ” ì ì´ ë‹¤ë¥´ë‹¤.í´ëž˜ìŠ¤ê°€ no_objectì¼ ë•Œ ì•žì˜ í•­ì˜ lossë¥¼ 1/10ìœ¼ë¡œ ì¤„ì—¬ class imbalanceë¥¼ ì¡°ì ˆí–ˆë‹¤ê³  í•œë‹¤.ë§ˆì§€ë§‰ìœ¼ë¡œ bounding box lossë¥¼ ì‚´íŽ´ë³´ìž. ë‹¤ë¥¸ detectorë“¤ì€ ë¯¸ë¦¬ ì •í•´ë†“ì€ ì•µì»¤ ë°•ìŠ¤ candidateë“¤ì„ ì–¼ë§ˆë‚˜ ì›€ì§ì¼ì§€ì— ëŒ€í•œ ë¸íƒ€ë¥¼ í•™ìŠµí–ˆë‹¤. ê·¸ëŸ¬ë‚˜ DETRì—ì„œëŠ” ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ directly ì˜ˆì¸¡í•œë‹¤. L1 Lossë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, L1 lossì˜ ë‹¨ì ì€ ë°”ìš´ë”© ë°•ìŠ¤ê°€ í¬ë©´ ìž‘ì€ ê±°ì— ë¹„í•´ lossê°€ ë” í¼. (ë‹¨ìˆœížˆ ì¢Œí‘œì˜ì°¨ì´ë§Œì„ ê³ ë ¤í•˜ë‹ˆê¹Œ)ì´ ë‹¨ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ Generalized IoU lossë¥¼ L1 Lossì— linear combination í–ˆë‹¤.DETR Architecture[ìš”ì•½]  CNNìœ¼ë¡œ feature map ìƒì„±  íŠ¸ëžœìŠ¤í¬ë¨¸ ì¸ì½”ë”ì— ë“¤ì–´ê°€ê¸° ìœ„í•´ì„œ positional encoding ë”í•¨  íŠ¸ëžœìŠ¤í¬ë¨¸ ì¸ì½”ë”ì˜ ì¶œë ¥ì€ ë””ì½”ë”ì— ë“¤ì–´ê°          ì´ ê°’ë“¤ì´ key, value ì—­í• ì„ í•¨.        ë””ì½”ë”ì˜ inputìœ¼ë¡œëŠ” Nê°œì˜ object queryê°€ ë“¤ì–´ê°„ë‹¤.          ì´ ì¿¼ë¦¬ë„ í•™ìŠµí•˜ê²Œ ë¨. ì²˜ìŒì—ëŠ” ëžœë¤ê°’ìœ¼ë¡œ ì±„ìš°ì§€ë§Œ, ì ì  í•™ìŠµí•˜ëŠ” ê²ƒ      ì¼ì¢…ì˜ positional encoding ì—­í• ì„ í•¨ (ì–´ëŠ ë¶€ë¶„ì„ ê´€ì‹¬ìžˆê²Œ ë³¼ ê²ƒì¸ì§€)        ê°ê° FFNì„ í†µí•´ì„œ Nê°œë¥¼ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤.DETRì˜ ì „ì²´ì ì¸ êµ¬ì¡°ëŠ” ìœ„ Figureê³¼ ê°™ë‹¤. í¬ê²Œ 3ê°€ì§€ ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ëœë‹¤.      CNN backbone : CNNì„ ì´ìš©í•´ compact feature representationì„ ì¶”ì¶œí•œë‹¤.(feature map)        Encoder-Decoder Transformer        A simple Feed Forward Network(FFN) : ê²°ê³¼ê°’ ì¶œë ¥  DETRì˜ êµ¬ì¡°ëŠ” ë§¤ìš° ê°„ë‹¨í•´ì„œ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì„ íŒŒì´í† ì¹˜ë¡œ êµ¬í˜„í•˜ë©´ 50ì¤„ì •ë„ì´ë‹¤! (ê¸€ í•˜ë‹¨ë¶€ì— ì²¨ë¶€í•˜ê² ìŒ.)Backboneì‹¤í—˜ì—ì„œëŠ” ResNetì„ ì‚¬ìš©í•¨. H, WëŠ” 5ë²ˆ ì •ë„ì˜ down-scaleì„ í–ˆë‹¤.(H0/32, W0/32)ìµœì¢… C = 2048ë¡œ ì‚¬ìš©(ResNet ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ê°€ 2048ì´ê¸° ë•Œë¬¸)Transformer encoderìš°ì„  1x1 conv.ë¥¼ í†µí•´ ì±„ë„ì„ $d$ dimensionìœ¼ë¡œ ì¤„ì¸ë‹¤.ë˜ ì¸ì½”ë”ì— ë“¤ì–´ê°€ê¸° ìœ„í•´ì„  ë²¡í„°ë¡œ ë°”ë€Œì–´ì•¼ í•˜ë‹ˆê¹Œ 3ì°¨ì›ì„ 2ì°¨ì›ìœ¼ë¡œ ë°”ê¿”ì•¼ í•œë‹¤. H,Wë¥¼ ë‹¤ í•©ì³ì„œ spatial dimensions of d x HWë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.ì´ ë•Œ HWëŠ” ì‹œí€€ìŠ¤ì˜ ê°œìˆ˜ê°€ ë˜ê³ , ê° ì‹œí€€ìŠ¤ ë²¡í„°ì˜ ì‚¬ì´ì¦ˆê°€ dê°€ ë˜ëŠ” ê²ƒì´ë‹¤.íŠ¸ëžœìŠ¤í¬ë¨¸ëŠ” attentionê¸°ë°˜ì´ê¸° ë•Œë¬¸ì— permutation-invariantí•˜ë‹¤. ì¦‰, ìˆœì„œë¥¼ ë¬´ì‹œí•œë‹¤ëŠ” ê²ƒì´ë‹¤.ë•Œë¬¸ì—, ìˆœì„œë¥¼ ë¶€ì—¬í•˜ê¸° ìœ„í•´ fixed positional encodingì„ input of each attention layerì— ì¶”ê°€í•œë‹¤. (ì°¸ê³ ) x, yì¶• ë”°ë¡œ d/2ì”© í•´ì„œ ì¸ì½”ë”©í•´ì„œ concatí•˜ëŠ” ì‹ìœ¼ë¡œ positional encodingì„ í•´ì¤€ë‹¤.Transformer decodertransformerì˜ ìŠ¤íƒ ë‹¤ë“œ ì•„í‚¤í…ì³ë¥¼ ë”°ëžë‹¤ê³  í•œë‹¤.ë‹¤ë§Œ originalê³¼ì˜ ì°¨ì´ì ì€ ê° ë””ì½”ë” ë ˆì´ì–´ì—ì„œ Nê°œì˜ ê°ì²´ë¥¼ parallelí•˜ê²Œ ë””ì½”ë”©í•œë‹¤ëŠ” ê²ƒì´ë‹¤.ë””ì½”ë”ë¥¼ í†µê³¼í•˜ê²Œ ë˜ë©´ Nê°œì˜ ì˜¤ë¸Œì íŠ¸ê°€ parallelí•˜ê²Œ ë‚˜ì˜¤ëŠ”ë°, ì´ ë•Œ ë””ì½”ë”ì˜ìž…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” ê²ƒì€ object queriesì´ë‹¤.ë””ì½”ë” ì—­ì‹œ permutation-invariant í•˜ê¸° ë•Œë¬¸ì— ì¼ì¢…ì˜ positional encodingì´ í•„ìš”í•˜ë‹¤. (ìž…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” ê²ƒë“¤ì´ ë‹¤ ë‹¬ë¼ì•¼ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ë½‘ì•„ì¤„ ìˆ˜ ìžˆê¸° ë•Œë¬¸ì´ë‹¤)ì—¬ê¸°ì„œ object queryê°€ positional encoding ì—­í• ì„ í•œë‹¤.ì´ ì¿¼ë¦¬ëŠ” í•™ìŠµì˜ ëŒ€ìƒì´ ë˜ë©° each attention layerì˜ ì¸í’‹ìœ¼ë¡œ ì¶”ê°€ëœë‹¤.ì´ê²ƒë“¤ì€ independently FFNì„ ì§€ë‚˜ ë°•ìŠ¤ ì¢Œí‘œ, í´ëž˜ìŠ¤ ë¼ë²¨ë¡œ ë””ì½”ë“œëœë‹¤. (ê·¸ë ‡ê²Œ Nê°œì˜ final predictionì„ ì–»ê²Œ ëœë‹¤.)ê°œì¸ì ìœ¼ë¡œ ê°€ìž¥ ì¤‘ìš”í•œ ë¶€ë¶„ì´ë¼ê³  ìƒê°í•˜ëŠ”ê²Œ, ì´ëŸ¬í•œ self- and encoder-decoder attention ëª¨ë¸ì€ imageë¥¼ global ì¶”ë¡ í•  ìˆ˜ ìžˆê²Œ í•´ì¤€ë‹¤. ì¦‰ use the whole image as contextë¡œ ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ê²ƒì´ë‹¤!!Prediction feed-forward networks (FFNs)3-layer í¼ì…‰íŠ¸ë¡  with ReLU êµ¬ì¡°ì´ë‹¤.ì´ FFNì€ bounding boxì˜ normalized center coordinates, height, widthë¥¼ ì˜ˆì¸¡í•˜ë©°, softmax functionì„ í†µí•´ class labelì„ ì˜ˆì¸¡í•œë‹¤.no object í´ëž˜ìŠ¤ëŠ” background roleì„ í•˜ê¸°ë„ í•œë‹¤.Experimentsë‹¤ëŠ” ì‚´íŽ´ë³´ì§€ ì•Šê³ , ëª‡ ê°œë§Œ ì‚´íŽ´ë³¼ ê²ƒìž„.Comparison with Faster R-CNNAP_S : Small scale imagesAP_L : Large scale imagesCNNì€ local íŠ¹ì„±ì„ ìž˜ ë°˜ì˜í•˜ë¯€ë¡œ small img.ë“¤ì— DETRë³´ë‹¤ ë” íš¨ê³¼ì ì´ë‹¤.í•˜ì§€ë§Œ DETRì€ Attention ê¸°ë°˜ì´ê¸° ë•Œë¬¸ì— ëª¨ë“  positionì„ ë³¼ ìˆ˜ ìžˆê³ , ë•Œë¬¸ì— Large img.ì— ë§¤ìš° íš¨ê³¼ì ì´ë‹¤.ê·¸ëŸ¬ë‚˜ ìµœê·¼ ë‚˜ì˜¨ ë…¼ë¬¸ Deformable DETRì—ì„œ AP_Së„ Faster R-CNNë³´ë‹¤ ë†’ì€ ì„±ê³¼ë¥¼ ê±°ë‘ì—ˆë‹¤! (ë‹¤ìŒ ë…¼ë¬¸ì€ ì´ê±¸ ì½ì–´ë³´ë ¤ê³  í•œë‹¤.)(Attentionì´ globalí•œ reasonì´ ëœë‹¤ëŠ” ìž¥ì ì´ ìžˆê¸° ë•Œë¬¸ì— panoptic segmentationì— ìž˜ ë§žì§€ ì•Šì„ê¹Œí•˜ëŠ” ìƒê°ì´ ë“ ë‹¤â€¦)Number of encoder layersì¸ì½”ë”ì˜ ë¸”ëŸ­ìˆ˜ê°€ ë§Žì„ìˆ˜ë¡ APëŠ” ì˜¬ë¼ê°„ë‹¤.ë˜í•œ EncoderëŠ” disentangling(ì´ë¯¸ì§€ ë¶„í•´, í•´ì²´) ì—­í• ì— ì•„ì£¼ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìžˆì—ˆë‹¤ê³  í•œë‹¤.image disentanglement ëž€ ìœ„ì˜ ì‚¬ì§„ê³¼ ê°™ì´ ê°ì²´ë³„ë¡œ(ì¸ìŠ¤íƒ„ìŠ¤ë³„ë¡œ) ìž˜ ë¶„ë¦¬ê°€ ëìŒì„ í‘œí˜„í•˜ëŠ” ë“¯í•˜ë‹¤.ì´ë ‡ê²Œ ê°ì²´ë³„ë¡œ attentionì´ ìž˜ ë‚˜ëˆ„ì–´ì§„ë‹¤ë©´ ë””ì½”ë”ì—ì„œ ê°ì²´ì˜ bounding box, classë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì‰½ë‹¤ê³  í•œë‹¤.Number of decoder layersì´ê²ƒì€ decoder layerì˜ ê°œìˆ˜ê°€ ì–´ëŠì •ë„ ë˜ë©´, NMSë¥¼ ì“¸ ë•Œì™€ ì„±ëŠ¥ìƒ ë³„ ì°¨ì´ê°€ ì—†ë‹¤ëŠ” ê²°ê³¼ì´ë‹¤.ê°œì¸ì ìœ¼ë¡œ ì •ë§ ë†€ëžë˜ ê²°ê³¼ì´ë‹¤. ê°ì²´ì˜ head, edgeë¥¼ ì •ë§ ìž˜ attentioní•˜ê³  ìžˆìŒì„ ì•Œ ìˆ˜ ìžˆë‹¤. ì¸ì½”ë”ëŠ” global attentionì„ í†µí•´ ì¸ìŠ¤í„´ìŠ¤ë“¤ì„ ë¶„ë¦¬í•œë‹¤ë©´, ë””ì½”ë”ëŠ” í´ëž˜ìŠ¤ì™€ ê°ì²´ì˜ ë°”ìš´ë”ë¦¬(ê°€ìž¥ìžë¦¬)ë¥¼ ì¶”ì¶œí•œë‹¤ê³  í•œë‹¤.Panoptic SegmentationConclusionDETRì€ object detection ë¬¸ì œë¥¼ direct set predictionìœ¼ë¡œ ë³´ì•˜ê³ , ì´ë¥¼ Transformerì™€ bipartite matchingì„ í†µí•œ end-to-end ë°©ì‹ìœ¼ë¡œ í’€ì–´ëƒˆë‹¤. (ë§Žì€ ì—°êµ¬ìžë“¤ì´ ëª¨ë“  ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ìµœì¢… ì§€í–¥ì ì€ end-to-endë¼ê³  í•œë‹¤.)transformer íŠ¹ì„±ìƒ, DETR ë˜í•œ architectureë¥¼ flexible í•˜ê²Œ í™•ìž¥ì‹œí‚¬ ìˆ˜ ìžˆë‹¤.(Panoptic segmentationì²˜ëŸ¼)CodeAdditional studies(If you have some parts that cannot understand, you have to do additional studies for them. Itâ€™s optional.)Deformable DETRReferences(References for your additional studies)  ì¸ê°„ì§€ëŠ¥ì´ ì¸ê³µì§€ëŠ¥ì„ ê³µë¶€í•˜ëŠ” ìž¥ì†Œ  KPâ€™s blog  TF ë…¼ë¬¸ ì½ê¸° ëª¨ìž„ PR-284]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> Object Detection </tag>
        
          <tag> Encoder-Decoder </tag>
        
          <tag> Transformer </tag>
        
          <tag> Panoptic Segmentation </tag>
        
          <tag> DETR </tag>
        
          <tag> ViT </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[MMDetection ì‚¬ìš©í•˜ê¸°]]></title>
      <url>/computer%20vision/2020/11/03/mmdetection-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/</url>
      <content type="text"><![CDATA[Daconì˜ K-Fashion AI ê²½ì§„ëŒ€íšŒì˜ baseline ì„¤ëª…ì— ë”°ë¼ MMdetection toolkitì„ ì„¤ì¹˜í•´ë³´ê³  í•™ìŠµê¹Œì§€ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.ì €ëŠ” ì˜¤ëŠ˜ë¶€í„° ì‹œìž‘ëœ K-Fashion AI ê²½ì§„ëŒ€íšŒì— ì°¸ê°€í•©ë‹ˆë‹¤.ì´ ëŒ€íšŒëŠ” ì œê°€ ê´€ì‹¬ìžˆëŠ” ë¶„ì•¼ì¸ ì»´í“¨í„° ë¹„ì „ ëŒ€íšŒë¼ ê´€ì‹¬ì„ ê°€ì§€ê²Œ ë˜ì—ˆê³ , ì‹¤ì œë¡œ CV ê´€ë ¨ ëŒ€íšŒëŠ” ì²˜ìŒì´ë‹ˆ ë°°ìš´ë‹¤ëŠ” ë§ˆìŒê°€ì§ìœ¼ë¡œ ì—´ì‹¬ížˆ ê³µë¶€í•´ë³´ë ¤ í•©ë‹ˆë‹¤.MMdectection?ì´ ëŒ€íšŒì˜ ë² ì´ìŠ¤ë¼ì¸ì„ ì°¸ê³ í•˜ë©´ì„œ mmdetection ì´ë¼ëŠ” ê²ƒì„ ì²˜ìŒ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ë„ ìžˆìœ¼ë‹ˆ ì‹œê°„ ë‚¨ì„ ë•Œ ë´ì•¼ê² ë„¤ìš”ê¹ƒí—ˆë¸ŒëŠ” ì—¬ê¸°ì— ë“¤ì–´ê°€ë©´ ë˜ê³ , ë¦¬ë“œë¯¸ íŒŒì¼ì— mmdetectionì´ ì–´ë–¤ ê±´ì§€ ë‚˜ì™€ìžˆìŠµë‹ˆë‹¤.ì§§ê²Œ ìš”ì•½í•˜ìžë©´,  MMDetection is an open source object detection toolbox based on PyTorch.ì´ë¼ í•˜ë„¤ìš”.object detectionì—ì„œ ë‹¤ë£¨ëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ë“¤ì„ í•œ ê³³ì— ëª¨ì•„ë’€ë‹¤ê³  í•˜ë‹ˆ ì •ë§ ê°„íŽ¸í•´ë³´ìž…ë‹ˆë‹¤. (ì´ëŸ° ê±¸ ì´ì œ ì•Œê²Œëœ ê²Œ ì•„ì‰¬ìš¸ ì •ë„ë¡œâ€¦)ì—¬ê¸° ì— support í•˜ëŠ” ëª¨ë¸ë“¤ì´ ë‚˜ì—´ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.ë˜í•œ ë²„ì „ ì—…ë°ì´íŠ¸ ë° ìœ ì§€ë³´ìˆ˜ë„ êµ‰ìž¥ížˆ ìž˜ ë˜ê³  ìžˆë‹¤ê³  í•©ë‹ˆë‹¤!Installationë°ì´ì½˜ ë² ì´ìŠ¤ ë¼ì¸ì„ ë³´ê³  ë”°ë¼í•˜ëŠ” ê²ƒì´ë¯€ë¡œ ë²„ì „ë„ ë§žì¶°ì„œ í•´ë´…ë‹ˆë‹¤.ìš°ì„  mmdetection ê¹ƒí—ˆë¸Œ ë§í¬ì— ê°€ì„œ ë¸Œëžœì¹˜ë¥¼ v2.3.0 ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.ë°”ë¡œ ê°€ê¸°README.md ë¥¼ ì¡°ê¸ˆ ë‚´ë¦¬ë‹¤ ë³´ë©´ install.md ë¡œ ë„˜ì–´ê°ˆ ìˆ˜ ìžˆëŠ” í•˜ì´í¼ë§í¬ê°€ ìžˆìŠµë‹ˆë‹¤.ê·¸ê±¸ ëˆŒëŸ¬ install.mdë¡œ ì´ë™í•©ë‹ˆë‹¤.Requirements ìž˜ í™•ì¸í•˜ì‹œê³ , install mmdetection ìˆœì„œëŒ€ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.1. ê°€ìƒí™˜ê²½ ìƒì„±conda create -n ê°€ìƒí™˜ê²½ì´ë¦„ python=3.7 -yconda activate ê°€ìƒí™˜ê²½ì´ë¦„ ì´ë ‡ê²Œ ê°€ìƒ í™˜ê²½ ìƒì„±í•˜ê³  activate ì‹œí‚µë‹ˆë‹¤.2. pytorch, torchvision ì„¤ì¹˜ (ë²„ì „ ì£¼ì˜)ë² ì´ìŠ¤ë¼ì¸ ë”°ë¼ê°€ë ¤ë©´ (ì¦‰, mmdetection version==2.3.0) torch version == 1.5.0 ì´ì–´ì•¼ í•©ë‹ˆë‹¤.pytorchë¥¼ ì›í•˜ëŠ” ì´ì „ ë²„ì „ìœ¼ë¡œ ì„¤ì¹˜í•´ì•¼ í•  ë•Œ ê³µì‹í™ˆíŽ˜ì´ì§€ì— ì»¤ë§¨ë“œê°€ ë‹¤ ë‚˜ì™€ìžˆìŠµë‹ˆë‹¤. linkconda install pytorch==1.5.0 torchvision==0.6.0 -c pytorch3. MMdetection repo. clonemmdetection ë ˆí¬ì§€í„°ë¦¬ë¥¼ clone í•´ì•¼ í•©ë‹ˆë‹¤.ì´ ë•Œë„ ì´ ë„íë¨¼íŠ¸ì™€ ë‹¤ë¥´ê²Œ ì£¼ì˜í•´ì•¼ í•  ì ì´ ìžˆìŠµë‹ˆë‹¤. ì•žì„œ ë§í–ˆë“¯ì´ ì €ëŠ” ë² ì´ìŠ¤ë¼ì¸ ë”°ë¼ branch == v2.3.0ìœ¼ë¡œ ì´ë™í–ˆìŠµë‹ˆë‹¤. ì´ ë¸Œëžœì¹˜ì— í•´ë‹¹ë˜ëŠ” ë²„ì „ìœ¼ë¡œ git clone í•´ì•¼ í•©ë‹ˆë‹¤.git clone --branch v2.3.0 https://github.com/open-mmlab/mmdetection.git4. Requirements ì„¤ì¹˜ìš°ì„  í´ë¡ í•œ ë””ë ‰í† ë¦¬ë¡œ ê°‘ë‹ˆë‹¤.cd ./mmdetectionì €ì™€ ê°™ì´ ë‹¤ë¥¸ ì‚¬ëžŒë“¤ê³¼ ê°™ì´ ì“°ëŠ” ì„œë²„ë¥¼ ì“°ì‹œëŠ” í™˜ê²½ì´ë¼ë©´ ë§ˆìŒëŒ€ë¡œ íŒ¨í‚¤ì§€/ë¼ì´ë²„ë¦¬ë¥¼ ë‹¤ìš´ ë°›ì•˜ì„ ë•Œ ì¶©ëŒë  ê°€ëŠ¥ì„±ì´ ìžˆìŠµë‹ˆë‹¤.ê·¸ëž˜ì„œ ì´ requirements ë“¤ì„ ê°€ìƒ í™˜ê²½ ë‚´ì— ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.í˜„ìž¬ ê°€ìƒí™˜ê²½ì˜ ì ˆëŒ€ ì£¼ì†Œë¥¼ ëª¨ë¥´ì‹œë©´, í„°ë¯¸ë„ì—conda env listë¥¼ ì¹˜ì‹œë©´ ê°€ìƒí™˜ê²½ ì´ë¦„ ë’¤ì— ê²½ë¡œê°€ ë‚˜ì˜µë‹ˆë‹¤.ê·¸ pathë¥¼ copy í•´ë‘ì„¸ìš”.ê°€ìƒí™˜ê²½ì˜ ê²½ë¡œë¥¼ {PATH}ë¼ê³  ê°€ì •í•˜ê³  ì ì–´ë³´ê² ìŠµë‹ˆë‹¤.{PATH}/bin/pip install -r requirements/build.txtì´ë ‡ê²Œ í•˜ë©´ requirementsê°€ ìž˜ ì„¤ì¹˜ë  ê²ƒìž…ë‹ˆë‹¤.5. Set uppip install -v -e .  # or "python setup.py develop"ì•žì— ëª…ë ¹ì–´ í•´ë³´ê³  ì•ˆë˜ë©´ python setup.py develop í•˜ì‹œë©´ ë©ë‹ˆë‹¤.(ì € ê°™ì€ ê²½ìš°ëŠ” pip install -v -e . í–ˆì„ ë•Œ ì•ˆëì–´ì„œ python setup.py develop ëª…ë ¹ì–´ í•´ì„œ ì •ìƒ ì„¤ì¹˜ ëìŠµë‹ˆë‹¤.)Training í•˜ê¸° ì „ ì¤€ë¹„ í•´ì•¼ í•  ê²ƒë“¤1. Dataset ê´€ë ¨mmdetection/mmdet/datasets ì—ì„œ ë°ì´í„°ì…‹ì— ëŒ€í•œ ê¸°ë³¸ ì„¤ì •ì„ í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.custom.py ë¡œ ì§ì ‘ personal í•œ ì„¤ì •ì„ í•  ìˆ˜ ìžˆê² ì§€ë§Œ ì´ ëŒ€íšŒëŠ” coco.py ì—ì„œ ì„¤ì •ëœ ê²ƒì—ì„œ í¬ê²Œ ë²—ì–´ë‚˜ì§€ ì•Šì•„ ì´ëŒ€ë¡œ ì‚¬ìš©í•´ë„ ëœë‹¤ê³  í•©ë‹ˆë‹¤.ê·¸ëž˜ì„œ coco.py ì—ì„œ ë¯¸ë¦¬ ì„¤ì •ëœ CLASSES ë“¤ì„ ì´ ëŒ€íšŒì—ì„œ ì“°ì¼ í´ëž˜ìŠ¤ë¡œ ë°”ê¾¸ê² ìŠµë‹ˆë‹¤.ë˜í•œ, mmdetectionì˜ ê²½ìš° train/test í´ë”ê°€ í•œ ê³³ì— flatten í•˜ê²Œ ì¡´ìž¬í•´ì•¼ í•œë‹¤ê³  í•©ë‹ˆë‹¤./train/a/*.jpg ì´ëŸ° í˜•íƒœê°€ ì•„ë‹Œ /train/a_*.jpg ì´ëŸ° ì‹ìœ¼ë¡œìš”.ì´ê±´ íŒŒì´ì¬ ì½”ë“œë¡œ ì‰½ê²Œ ìˆ˜ì • ê°€ëŠ¥í•˜ë‹ˆ ìžŠì§€ ì•Šê³  í•˜ì‹œê¸¸ ë°”ë¦½ë‹ˆë‹¤.2. training optionsíŠ¸ë ˆì´ë‹í•  ë•Œ í•„ìš”í•œ ê¸°ë³¸ ì˜µì…˜ë“¤ì„ mmdetection/config/_base_/default_runtime.py ì—ì„œ ê°€ëŠ¥í•©ë‹ˆë‹¤.ì´ configëŠ” ìš°ì„  ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œì—ì„œ ì œê³µí•œ ê²ƒì„ ë¶™ì—¬ ì¨ì„œ ì§€ê¸ˆ ì œ í™˜ê²½ì— ë§žê²Œ ì¡°ê¸ˆì”© ê³ ì³¤ìŠµë‹ˆë‹¤. (data root, gpu ê°œìˆ˜, epoch ë“±)ìžì„¸í•œ ê±´ ì—¬ê¸°ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.train/test set ê²½ë¡œ ë“±ë“± ìž˜ ì„¤ì •í•˜ê³  ë‚˜ë©´ ë°”ë¡œ í•™ìŠµ ì‹œìž‘ ê°€ëŠ¥í•©ë‹ˆë‹¤.Training !python tools/train.py configs/_base_/default_runtime.pyì´ê±¸ ì¹˜ì‹œë©´ configë¡œ í•´ë†¨ë˜ ì„¤ì •ë“¤ì´ ì£¼ë£¨ë£© ëœ¨ë©´ì„œ í•™ìŠµì´ ì‹œìž‘ë©ë‹ˆë‹¤.TroubleshootingAttributeError: â€˜COCOâ€™ object has no attribute â€˜get_cat_idsâ€™pycocotools ê´€ë ¨ ì˜¤ë¥˜ìž…ë‹ˆë‹¤.ì €ê°™ì€ ê²½ìš°ì—ëŠ”{PATH}/bin/pip install "git+https://github.com/open-mmlab/cocoapi.git#subdirectory=pycocotools"ë¡œ í•´ê²°í–ˆìŠµë‹ˆë‹¤.ModuleNotFoundError: No module named â€˜mmcv._extâ€™mmcvê°€ ì œëŒ€ë¡œ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°ìž…ë‹ˆë‹¤.{PATH}/bin pip uninstall mmcv{PATH}/bin pip install mmcv-full==1.0.5ì €ëŠ” ì´ë ‡ê²Œ í•´ê²°í–ˆìŠµë‹ˆë‹¤.Referenceë°ì´ì½˜ ë² ì´ìŠ¤ë¼ì¸]]></content>
      <categories>
        
          <category> Computer Vision </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> MMdetection </tag>
        
          <tag> Mask R-CNN </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale(ViT)]]></title>
      <url>/paper%20review/2020/10/31/ViT/</url>
      <content type="text"><![CDATA[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT) ë¥¼ ì½ê³  ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.An Image is Worth 16x16 Words: Transformers for Image Recognition at ScaleOverview  image classification taskì— ê¸°ì¡´ì˜ transformer ëª¨ë¸ì„ ì´ìš©í•œë‹¤.          transformerì˜ ìž¥ì ë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ìžˆìŒ.      simple, computational efficiency, scalability        Vision Transformer(ViT)          ì›ë³¸ ì´ë¯¸ì§€ë¥¼ patchesë“¤ë¡œ splití•œë‹¤.                  ì´ ë•Œ ì´ë¯¸ì§€ íŒ¨ì¹˜ë“¤ì„ NLPì—ì„œì˜ token(word)ì™€ ê°™ì€ ì—­í• ìž„.                    ì´ íŒ¨ì¹˜ë“¤ì˜ sequence of linear embeddingì„ Transformerì˜ inputìœ¼ë¡œ feedí•œë‹¤.        í¬ì§€ì•Šì€ ë°ì´í„° ì…‹ì—ì„œëŠ” ResNetë³´ë‹¤ ì•½ê°„ ë‚®ì€ ì •í™•ë„          TransformerëŠ” CNNê³¼ ë‹¤ë¥´ê²Œ translation equivariance, locality ê°™ì€ inductive biases(=weight sharing)ì´ ì—†ê¸° ë•Œë¬¸ì— ë°ì´í„°ì…‹ì´ ì ìœ¼ë©´ generalizeë˜ê¸° ì–´ë ¤ì›€.        í•˜ì§€ë§Œ large scale ë°ì´í„°ì—ì„œëŠ” CNNì˜ inductive biasë¥¼ ëŠ¥ê°€í•¨.          image recognition benchmarksì—ì„œ ì—¬ëŸ¬ SOTA ë‹¬ì„±í•¨.      Related work (Basic concepts)ì œì•ˆí•œ ëª¨ë¸ì´ ìµœê·¼ ì—°êµ¬ ì¤‘ì—ì„œëŠ” iGPT ë…¼ë¬¸ê³¼ ìœ ì‚¬í•˜ë‹¤ê³  í•¨.  reducing image resolution and color spaceí•œ í›„ transformerë¥¼ ì ìš©Methods(Explain one of the methods that the thesis used.)      Model overview          Vision Transformer(ViT)                  imageë¥¼ patch ë‹¨ìœ„ë¡œ ìž˜ë¼ì„œ flattenì‹œí‚¨ í›„, ê·¸ê²ƒì„ linear projectioní•˜ì—¬ encoderì— feedí•œë‹¤.                      Equation              (Eq.1) trainable linear projectionì€ flattenëœ patchë“¤ì„ D dimensionì— mappingì‹œí‚¨ë‹¤.      (Eq.4) BERTì˜ [class] í† í°ì²˜ëŸ¼, embedded patchì˜ sequence(image representation y) ì „ì— learnable embeddingì„ ì¶”ê°€í•œë‹¤. (prepend)                  both during pre-training and fine-tuning, a classification head is attached to $\left({z}_{L}^{0} \right)$                    Position embeddings                  positional informationì„ ìœ ì§€í•˜ê¸° ìœ„í•´ patch embeddingì— ë¶™ì—¬ì§. (ìžì„¸í•œê±´ Appendix.D.3)          standard learnable 1D position embedding ì‚¬ìš©          ì´ë ‡ê²Œ position embeddingê¹Œì§€ ë”í•´ì§„ sequence of embedding vectorë“¤ì€ encoderì˜ inputì´ ë¨.                    Encoder                  alternating layers of multiheaded self-attention(MSA)          MLP blocks                          contains 2 layer with a GELU non-linearity              MLP ë¶€ë¶„ code                              class MlpBlock(nn.Module):      """Transformer MLP / feed-forward block."""  def apply(self,              inputs,              mlp_dim,              dtype=jnp.float32,              out_dim=None,              dropout_rate=0.1,              deterministic=True,              kernel_init=nn.initializers.xavier_uniform(),              bias_init=nn.initializers.normal(stddev=1e-6)):      """Applies Transformer MlpBlock module."""      actual_out_dim = inputs.shape[-1] if out_dim is None else out_dim      x = nn.Dense(          inputs,          mlp_dim,          dtype=dtype,          kernel_init=kernel_init,          bias_init=bias_init)      x = nn.gelu(x)      x = nn.dropout(x, rate=dropout_rate, deterministic=deterministic)      output = nn.Dense(          x,          actual_out_dim,          dtype=dtype,          kernel_init=kernel_init,          bias_init=bias_init)      output = nn.dropout(output, rate=dropout_rate, deterministic=deterministic)      return output                                Layernorm(LN) is applied before every block          residual connections after every block                    Hybrid Architecture                  patch embedding projection E (Eq.1)ì´ CNN feature mapìœ¼ë¡œ ëŒ€ì²´ë  ìˆ˜ ìžˆë‹¤.                          ì¦‰, ResNetê³¼ ê°™ì€ CNNêµ¬ì¡°ì˜ ëª¨ë¸ì„ ê°€ì§€ê³ , 2D feature map ì¤‘ í•˜ë‚˜ë¥¼ 1Dë¡œ flattenì‹œí‚¨ í›„ transformer dimensionì— projecting ì‹œí‚´.              ìœ„ì—ì„œ ë§Œë“¤ì–´ì§„ sequenceì— classification input embedding, position embeddingë¥¼ ì¶”ê°€ì‹œì¼œ encoderì— inputìœ¼ë¡œì¨ feed ì‹œí‚¬ ìˆ˜ ìžˆìŒ.                                          Fine-Tuning and Higher Resolution  large datasetìœ¼ë¡œ pre-trainingí•˜ê³ , smaller downstream taskì— ëŒ€í•´ fine-tune í•˜ë ¤ê³  í•¨.  ì´ë¥¼ ìœ„í•´ì„œ pre-trained prediction headë¥¼ ì§€ìš°ê³ , 0ìœ¼ë¡œ initializedgks D x K feedforward layerë¥¼ ì¶”ê°€í•¨.          K : # of downstream classes        pre-training ë³´ë‹¤ ë†’ì€ resolutionìœ¼ë¡œ fine-tuningí•˜ëŠ” ê²ƒì€ beneficialí•  ë•Œë„ ìžˆìŒ.  higher resolutionì„ feedí•˜ê²Œ ë˜ë©´, patch sizeëŠ” ë™ì¼í•˜ë¯€ë¡œ sequence lengthê°€ ê¸¸ì–´ì§  ViTëŠ” ìž„ì˜ì ì¸ sequence lengthë¥¼ ë‹¤ë£° ìˆ˜ ìžˆìŒ(ë©”ëª¨ë¦¬ ì œì•½ì— ë”°ë¼ì„œ)  í•˜ì§€ë§Œ pre-trained position embeddingì´ ì˜ë¯¸ ì—†ì–´ì§ˆ ìˆ˜ ìžˆìŒ          ì›ë³¸ ì´ë¯¸ì§€ì˜ locationì— ë”°ë¼ pre-trained position embeddingì˜ 2D interpolationì„ ìˆ˜í–‰í•¨.        ìœ„ì™€ ê°™ì€ resolution adjustmentì™€ patch extractionì€ ì´ë¯¸ì§€ì˜ 2D êµ¬ì¡°ì— ëŒ€í•´ inductive biasë¥¼ manually ViTì— ì£¼ìž…ì‹œí‚¤ëŠ” ìœ ì¼í•œ í¬ì¸íŠ¸ìž„.Code  model.py          Github      References(References for your additional studies)https://jeonsworld.github.io/vision/vit/]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> Transformer </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[StyleGAN:A Style-Based Generator Architecture for Generative Adversarial Networks]]></title>
      <url>/paper%20review/2020/09/20/StyleGAN/</url>
      <content type="text"><![CDATA[StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks ë¥¼ ì½ê³  ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.StyleGAN : A Style-Based Generator Architecture for Generative Adversarial Networks (CVPR 2019)Overview      GANì˜ generator ë¶€ë¶„ì€ black boxë¡œ ì—¬ê²¨ì ¸ ì´ë¯¸ì§€ ìƒì„± ê³¼ì •ì„ ì´í•´í•˜ê¸° ì–´ë ¤ì› ìŒ.    style transfer ì—ì„œ ê¸°ë°˜í•œ generator êµ¬ì¡°          ê° ë ˆì´ì–´ë§ˆë‹¤ styleì˜ ì •ë³´ë¥¼ ìž…íž˜. -&gt; AdaIN      ì „ì²´ì ì¸ ìŠ¤íƒ€ì¼(ë¨¸ë¦¬ ìƒ‰, ì¸ì¢…, ì„±ë³„ ë“±), ì„¸ì„¸í•œ ë¶€ë¶„(ê³±ìŠ¬ ë“±) ë“±ê¹Œì§€ ì¡°ì • ê°€ëŠ¥ -&gt; noise        baseline : progressive GAN          latent vectorë¡œ ë¶€í„° ì´ë¯¸ì§€ í•©ì„±í•˜ê³  ì ì  í•´ìƒë„ë¥¼ ì˜¬ë ¤ì„œ high-resolution image ìƒì„± =&gt; scale-specific control            loss function, discriminator ë“± ìˆ˜ì •í•˜ì§€ ì•Šê³  ì˜¤ì§ ì œë„ˆë ˆì´í„°ì— ëŒ€í•´ì„œë§Œ ë‹¤ë£¸.    latent spaceì˜ interpolation quality ì¸¡ì •í•˜ëŠ” measure ì œì•ˆ          perceptual path length      linear separability        FFHQ ë°ì´í„°ì…‹ ì˜¤í”ˆRelated work (Basic concepts)  Progressive GAN          GANì„ ì €í•´ìƒë„ì—ì„œ ê³ í•´ìƒë„ë¡œ ì ì§„ì ìœ¼ë¡œ í•™ìŠµ        style transfer          content image &amp; style imageê°€ ìžˆì„ ë•Œ content ì´ë¯¸ì§€ì™€ ìœ ì‚¬í•˜ê²Œ style imageì— ìž…ížˆëŠ” ê²ƒ      Methods(Explain one of the methods that the thesis used.)Generator Architecture      left : traditional generaotr : latent code zë¥¼ input layerì— ë°”ë¡œ ë„£ìŒ.        right : style-based generator          first, map the input to an intermediate latent space W.      then controls the generator through adaptive instance normalization (AdaIN) at each conv. layer.      Gaussian noise is added after each conv.      ì œì•ˆí•œ ëª¨ë¸ì„ ì°¨ê·¼ì°¨ê·¼ ëœ¯ì–´ë³´ìžë©´Mapping Networkhttps://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431input vector zë¥¼ ë°”ë¡œ input layerì— ë„£ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, mapping networkë¥¼ ê±°ì³ intermediate vector w ë¡œ ë³€í™˜í•œ í›„ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œë‹¤.  ë°”ë¡œ ì¸í’‹ ë ˆì´ì–´ì— ë„£ì§€ ì•ŠëŠ” ì´ìœ  : ê³ ì •ëœ input distributionì— ë§žì¶°ì•¼ í•´ì„œ non-linearí•˜ê²Œ mappingì´ ë˜ê³ , ì´ê²ƒì€ ë¨¸ë¦¬ ìƒ‰ë“±ê³¼ ê°™ì€ attributeë¥¼ ë³€ê²½í•˜ê¸° íž˜ë“¤ì–´ì§€ê¸° ë•Œë¬¸.  ìœ„ì²˜ëŸ¼ intermediate vectorë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ìœ ë™ì ì¸ ê³µê°„ì— mapping ì‹œí‚¬ ìˆ˜ ìžˆê¸° ë•Œë¬¸ì— visual attribute ì¡°ì ˆì´ ì‰¬ì›Œì§„ë‹¤. =&gt; disentanglement í•˜ë‹¤.ì¦‰, ì´ ë„¤íŠ¸ì›Œí¬ì—ì„œëŠ” zë¡œë¶€í„° ë§Œë“¤ì–´ì§„ style wë¥¼ êµ¬í•˜ê³ , ì´ë¥¼ affine transformationì„ ê±°ì¹œ Aë¥¼ synthesis networkì— ë„˜ê²¨ì£¼ì–´ AdaIN operationì„ í†µí•´ ë ˆì´ì–´ì— ìŠ¤íƒ€ì¼ì„ ìž…ížŒë‹¤.Style Modules (AdaIN)https://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431  ìœ„ì—ì„œ ìƒì„±ëœ wëŠ” styleì— ëŒ€í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìžˆë‹¤.  Synthesis networkëŠ” í•™ìŠµê°€ëŠ¥í•œ constant tensor(4x4x512)ë¥¼ upsampling, convolutionì„ í†µí•´ 1024x1024x3 ì´ë¯¸ì§€ë¡œ ë³€í™˜ì‹œí‚¨ë‹¤.  wì˜ affine transfomationì„ í†µí•´ ì–»ì–´ì§„ Aë¥¼ ê°€ì§€ê³  AdaIN operationì„ í†µí•´ ìŠ¤íƒ€ì¼ì„ ìž…ížŒë‹¤.          normalizeí•˜ê³ , ì´ë¥¼ scaleí•˜ê³  biasë¥¼ ë”í•¨. ì´ê²Œ ìŠ¤íƒ€ì¼ì„ ìž…ížˆëŠ” íš¨ê³¼ë¥¼ ë‚¸ë‹¤.      ë§¤ conv ë ˆì´ì–´ë§ˆë‹¤ í•˜ë¯€ë¡œ, ê°ê°ì˜ ë ˆì´ì–´ë§ˆë‹¤ ë‹¤ë¥¸ ìŠ¤íƒ€ì¼ì„ ì¡°ì •í•  ìˆ˜ ìžˆë‹¤. ì´ ë§ì€ ê³§, ê° ë ˆì´ì–´ê°€ íŠ¹ì •í•œ attributeë§Œì„ ë‹´ë‹¹í•œë‹¤ëŠ” ëœ».                  ì„¸ë°€í•œ ìŠ¤íƒ€ì¼ ì¡°ì • ê°€ëŠ¥í•´ì§„ë‹¤.                    Stochastic variationë¨¸ë¦¬ì¹´ë½, ìˆ˜ì—¼ ë“± stochasticí•œ ìš”ì†Œë“¤ì€ ì‚¬ì§„ì˜ ë””í…Œì¼ì— ë§¤ìš° ì¤‘ìš”í•¨.  ìœ„ì˜ architectureì—ì„œ noiseê°€ ì´ì— ëŒ€í•œ ì—­í• ì„ í•œë‹¤.  synthesis networkì—ì„œ by adding per-pixel noise after each convolution.Style Mixingtwo random latent codes(w1,w2)ë¥¼ ì‚¬ìš©í•˜ëŠ” regularization ê¸°ë²•  í•˜ë‚˜ì˜ wë¡œ í•™ìŠµí•  ê²½ìš° ì—¬ëŸ¬ ë ˆì´ì–´ì— ëŒ€í•œ styleì´ correlateë˜ëŠ” ë¬¸ì œì ì´ ìƒê¸¸ ìˆ˜ ìžˆìŒ.  ex. w1 ìŠ¤íƒ€ì¼ë¡œ ìž…í˜€ë†“ì§€ë§Œ, ëžœë¤ìœ¼ë¡œ ëª‡ ê°œëŠ” w2 ìŠ¤íƒ€ì¼ì„ ì‚¬ìš©í•œë‹¤ â€¦  ìœ„ì™€ ê°™ì€ ë°©ë²•ì„ í†µí•´ ê° ë ˆì´ì–´ê°€ ë‹´ë‹¹í•˜ëŠ” ìŠ¤íƒ€ì¼ì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„ì§€ì„ ìˆ˜ ìžˆë‹¤.  (dropoutê³¼ ë¹„ìŠ·í•œ ì›ë¦¬ë¼ê³  í•¨)Disentanglement studies  ì´ ë‚´ìš©ì´ ì–´ë ¤ì›Œì„œ ì œëŒ€ë¡œ ì´í•´í•˜ì§€ ëª»í•¨. ì§§ê²Œ ìš”ì•½í•˜ê² ìŒ.  Disentanglment : latent spaceê°€ ì„ í˜•ì ì¸ êµ¬ì¡°ë¥¼ ê°€ì§€ê²Œ ë˜ì–´, í•˜ë‚˜ì˜ factorê°€ ì›€ì§ì˜€ì„ ë•Œ ì •í•´ì§„ íŠ¹ì„±ì´ ë°”ë€Œê²Œ ë§Œë“œëŠ” ê²ƒ.          ì˜ˆ. zì˜ íŠ¹ì •í•œ ê°’ì„ ë°”ê¿¨ì„ ë•Œ ìƒì„±ë˜ëŠ” ì´ë¯¸ì§€ì˜ í•˜ë‚˜ì˜ íŠ¹ì„±(ì„±ë³„, ë¨¸ë¦¬ì¹´ë½ ê¸¸ì´ ë“±)ë§Œ ì˜í–¥ì„ ì£¼ê²Œ ë˜ëŠ” ê²ƒ        fixed distributionì„ ë”°ë¥´ê²Œ ë˜ë©´ ì–µì§€ë¡œ ë¼ì›Œë§žì¶”ê²Œ ë˜ì–´ ì–´ìƒ‰í•œ ì´ë¯¸ì§€ê°€ ë§Œë“¤ì–´ì§ˆ ìˆ˜ ìžˆìŒ.  í•˜ì§€ë§Œ ì´ ëª¨ë¸ì²˜ëŸ¼ ë¹„ì„ í˜• mapping functionì„ ê°€ì§€ê²Œ ë  ê²½ìš°, ê³ ì •ëœ ë¶„í¬ë¥¼ ë”°ë¥¼ í•„ìš”ê°€ ì—†ìŒ.          ìœ„ ê·¸ë¦¼ì—ì„œ (c)ì™€ ê°™ì€ í˜•íƒœê°€ ë¨. ì–´ëŠì •ë„ aì™€ ìƒê¹€ìƒˆê°€ ë¹„ìŠ·í•˜ë©´ì„œ ìžì—°ìŠ¤ëŸ½ê²Œ ë§žì¶œ ìˆ˜ ìžˆê²Œ ëœ ê²ƒ        A major beneï¬t of our generator architecture is that the intermediate latent space W does not have to support sam-pling according to any ï¬xed distribution; its sampling density is induced by the learned piecewise continuous mapping f(z).  ë³¸ ë…¼ë¬¸ì—ì„œëŠ” disentanglementë¥¼ í•™ìŠµí•  ìˆ˜ ìžˆëŠ” ë‘ ê°€ì§€ í‰ê°€ ì§€í‘œë¥¼ ì œì•ˆí•¨.          Perceptual path length      Linear seperability      ìœ„ì˜ ë‚´ìš©ì„ ìžì„¸ížˆ ì•Œê³  ì‹¶ë‹¤ë©´ ì´ ê³³ì„ ì°¸ì¡°                  ë§í¬                    Conclusion  our investigations to the separation of high-level attributes and stochastic effects, as well as the linearity of the intermediate latent space will prove fruitful in improving the understanding and controllability of GAN synthesis.Appendix. Truncation trick in W      íŠ¸ë ˆì´ë‹ ì¤‘ì— í•˜ëŠ” ê²Œ ì•„ë‹ˆê³ , generatorê°€ ë§Œë“  ê²ƒ ì¤‘ì— ë” ë‚˜ì€ latent space ì„ ë½‘ëŠ” ë²•ì— ëŒ€í•œ trick        í•™ìŠµì´ ì™„ë£Œëœ ë„¤íŠ¸ì›Œí¬ì˜ inputì„ ì œì–´í•˜ëŠ” ë°©ë²•  ìœ„ ìˆ˜ì‹ì„ í†µí•œ w' vectorë¥¼ ë½‘ëŠ”ë‹¤.Additional studies(If you have some parts that cannot understand, you have to do additional studies for them. Itâ€™s optional.)disentanglementì— ëŒ€í•œ ëª…í™•í•œ ì´í•´ê°€ í•„ìš”í•¨.References(References for your additional studies)  https://www.youtube.com/watch?v=TWzEbMrH59o&amp;feature=youtu.be  https://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431  https://jayhey.github.io/deep%20learning/2019/01/16/style_based_GAN_2/  https://blog.lunit.io/2019/02/25/a-style-based-generator-architecture-for-generative-adversarial-networks/]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> GAN </tag>
        
          <tag> Generative Model </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[HarDNet:A Low Memory Traffic network]]></title>
      <url>/paper%20review/2020/09/11/HarDNet/</url>
      <content type="text"><![CDATA[HarDNet : A Low Memory Traffic network ë¥¼ ì½ê³  ê°œì¸ì ìœ¼ë¡œ ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.HarDNet : A Low Memory Traffic network (ICCV 2019)Key Idea  ê¸°ì¡´ì˜ metricsë“¤ì—ì„œì˜ inference time ì¸¡ì •ì€ ë¶€ì •í™•í•˜ë‹¤.          ìƒˆë¡œìš´ metric =&gt; memory traffic for accessing intermediate feature maps ì¸¡ì •                  inference latency ì¸¡ì •ì— ìœ ìš©í•  ê²ƒ, especially in such tasks as real-time object detection and semantic segmentation of high-resolution video.                    CIO : approximation of DRAM trafficì´ ë  ìˆ˜ ìžˆë‹¤.        computation, energy efficiencyë¥¼ ìœ„í•´ì„œëŠ” fewer MACs, less DRAMì´ ì¢‹ì€ ê²ƒìž„ -&gt; ì—°êµ¬ ë°©í–¥  ê°ê°ì˜ ë ˆì´ì–´ì˜ MoCì— soft constraintë¥¼ ì ìš©í–ˆìŒ.          low CIO network model with a reasonable increase of MACsë¥¼ ìœ„í•´      ë°©ë²• -&gt; avoid to employ a layer with a very low MoC such as a Conv1x1 layer that has a very large input/output channel ratio.                  input/output channel ratioê°€ í¬ë©´ low MoCë¥¼ ê°€ì§„ë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìžˆìŒ.                      Densely Connected Networksì— ì˜ê°ì„ ë°›ì•„ ëª¨ë¸ ë¹Œë”©í•¨.          DenseNetì˜ ë‹¤ìˆ˜ì˜ layer connectionsë“¤ì„ ì¤„ì˜€ìŒ. =&gt; concatenation costë¥¼ ì¤„ì´ê¸° ìœ„í•´      balance the input/output channel ratio by increasing the channel width of a layer according to its connections.        DRAM trafficBasic Concepts  MAC : number of multiply-accumulate operations or floating point operations  DRAM : Dynamic Random-Access Memory          read/write model param. and feature maps        CIO : Convolutional input/output          ëª¨ë“  conv layerì— ëŒ€í•´ IN(C,W,H) X OUT(C,W,H) sum        MoC : MACs over CIO of a layer = MACs/CIORelated Works  TREND : exploiting shortcuts  Highway networks, Residual Networks : add shortcuts to sum up a layer with multiple preceeding layers.  DenseNet : concatenates all preceeding layers as a shortcut achieving more efficent deep supervision.  ê·¸ëŸ¬ë‚˜ shortcutsëŠ” large memory usage, heavy DRAM trafficì„ ìœ ë°œí•  ìˆ˜ ìžˆë‹¤.                            Using shortcuts elongates the lifetime of a tensor, which may result in frequent data exchanges betwwen DRAM and cache.                      DenseNetì˜ sparsified version : LogDenseNet, SparseNet          Sparse                  The pros? If you have a lot of zeros, you donâ€™t have to compute some multiplications, and you donâ€™t have to store them. So you may gain on size and speed, for training and inference (more on this today).          The cons? Of course, having all these zeros will probably have an impact on network accuracy/performance.                    increase the growth rate(output channel width) to recover the accuracy dropping from the connection pruning, and the increase of growth rate can compromise the CIO reduction                  ì¦‰ increase of growth rateëŠ” ì¢‹ê²Œ ìž‘ìš©ëœë‹¤.                    Harmonic DenseNetSparsification and weighting  let layer k connect to layer k-2^n if 2^n divides k, where n is a non-negative integer and k-2^n &gt;= 0class HarDBlock(nn.Module):    def get_link(self, layer, base_ch, growth_rate, grmul):        if layer == 0:          return base_ch, 0, []        out_channels = growth_rate        link = []        for i in range(10):          dv = 2 ** i          if layer % dv == 0:            k = layer - dv            link.append(k)            if i &gt; 0:                out_channels *= grmul        out_channels = int(int(out_channels + 1) / 2) * 2        in_channels = 0        for i in link:          ch,_,_ = self.get_link(i, base_ch, growth_rate, grmul)          in_channels += ch        return out_channels, in_channels, link  2^n ê°œì˜ layerë“¤ì´ ì´ëŸ° ì‹ìœ¼ë¡œ processedë˜ë©´ layer [1 : 2^n -1]ëŠ” ë©”ëª¨ë¦¬ì—ì„œ flushëœë‹¤.          ì–´ë–»ê²Œ flush ëœë‹¤ëŠ” ê±´ì§€ ìž˜ ì´í•´ê°€ ë˜ì§€ ì•ŠìŒ.        Power-of-two-th harmonic wavesê°€ ë§Œë“¤ì–´ì§. ê·¸ëž˜ì„œ Harmonic ì´ë‹¤.      ì´ ë°©ì‹ì€ concatenation costë¥¼ ëˆˆì— ë„ê²Œ ê°ì†Œì‹œí‚¨ë‹¤.        layers with an index divided by a larger power of two are more influential than those that divided by a smaller power of two.          ë§Žì´ connectionë˜ë‹ˆê¹Œ ë‹¹ì—°ížˆ influential í•˜ë‹¤.      In this model, they amplify these key layers by increasing their channels, which can balance the channel ratio between the input and output of a layer to avoid a low MoC.                  ì´ëŸ° key layerë“¤ì„ amplify í–ˆìŒ(channel ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ì„œ)                      layer l has an initial growth rate k, we let its channel number to be k * m^n , where n is the max number satisfying that l is divided by 2^n                    m  ì€ low-dimensional compression factor ì—­í• ì„ í•œë‹¤.          m ì„ 2ë³´ë‹¤ ìž‘ê²Œí•˜ë©´ input channelì„ output channelë³´ë‹¤ ìž‘ê²Œ í•  ìˆ˜ ìžˆë‹¤.                          Empirically, settin m between 1.6 and 1.9                                          Transition and Bottleneck Layers  HDB(Harmonic Dense Block) : the proposed connection pattern forms a group of layers          is followed by a Conv1x1 layer as a transition        HDBì˜ depthëŠ” 2ì˜ ì œê³±ìˆ˜ë¡œ ì„¤ì •          HDBì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ê°€ ê°€ìž¥ í° ì±„ë„ìˆ˜ë¥¼ ê°€ì§€ë„ë¡ í•˜ê¸° ìœ„í•´ì„œ        DenseNet -&gt; gradientí•  ë•Œ ëª¨ë“  ë ˆì´ì–´ë¥¼ ë‹¤ passí•¨  ë…¼ë¬¸ì˜ HBD with depth L -&gt; pass through at most log L layers          degradationì„ ì™„í™”ì‹œí‚¤ê¸°ìœ„í•´, depth-L HDBë¥¼ layer Lê³¼ all its preceeding odd numbered layers  ë¥¼ concatenationì‹œí‚¨ë‹¤.      2~L-2ì˜ all even layerë“¤ì˜ ì•„ì›ƒí’‹ì€ HDBê°€ í•œë²ˆ ëë‚ ë•Œë§ˆë‹¤ ë²„ë ¤ì§„ë‹¤.        Bottleneck layer          DenseNetì—ì„œëŠ” param. efficiencyë¥¼ ìœ„í•´ ë§¤ Conv3x3 layerì „ì— bottleneckì„ ë‘ì—ˆë‹¤.      í•˜ì§€ë§Œ HarDnetì—ì„œëŠ” ìœ„ì—ì„œ ì´ë¯¸ channel ratio(ë§¤ ë ˆì´ì–´ë§ˆë‹¤ input&amp;output ì‚¬ì´ì˜)ì˜ ê· í˜•ì„ ìž¡ì•˜ìœ¼ë¯€ë¡œ bottleneck layerëŠ” ì“¸ëª¨ì—†ì–´ì§„ë‹¤.      ê·¸ëž˜ì„œ HBDì—ì„œëŠ” Bottleneck layerì—†ì´ Conv3x3 for all layers        Transition layer                inverted trainsition module                  maps input tensor to an additional max pooling function along with the original average pooling, followed by concatenation and Conv1x1.          50% of CIOë¥¼ ê°ì†Œì‹œí‚´                    Experiments      CamVid Dataset          replace all the blocks in a FC-DenseNet with HDBs      the architecture of FC-DenseNet with an encoder-decoder structure and block level shortcuts to create models for sematic segmentation.              We propose FC-HarDNet84 as specified in Table 3 for comparing with FC-DenseNet103. The new network achieves CIO reduction by 41% and GPU inference time reduction by 35%. A smaller version, FC-HarDNet68, also outperforms FC-DenseNet56 by a 65% less CIO and 52% less GPU inference time.        ImageNet Datasets  Object Detection          HarDNet-68 as a backbone model for a Single Shot Detector (SSD) and train it with PASCAL VOC 2007 and MS COCO datasets      Discussion  There is an assumption with the CIO, which is a CNN model that is processed layer by layer without a fusion. In contrast, fused-layer computation for multiple convolutional layers has been proposed.  CIO still failed to predict the actual inference time such as comparing two network models with significantly differnent architectures  In some of the layers CIO may dominate, but for the other layers, MACs can still be the key factor if its computational density is relatively higher. To precisely predict the inference latency of a network, we need to breakdown to each of the layers and investigate its MoC to predict the inference latency of the layer.  ì–´ì¨Œê±°ë‚˜ DRAM trafficì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•˜ê³  ì‹¶ì–´í•¨.  traffic reductionì„ ìœ„í•œ ê°€ìž¥ ì¢‹ì€ ë°©ë²•ì€ MoCë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” ê²ƒ          which might be counter-intuitive to the widely-accepted knowledge of that using more Conv1x1 achieves a higher efficiency.      ]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Traffic </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> Semantic Segmentation </tag>
        
          <tag> Network </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Neural Architecture Search With Reinforcement Learning]]></title>
      <url>/paper%20review/2020/08/29/NEURAL-ARCHITECTURE-SEARCH-WITH-REINFORCEMENT-LEARNING/</url>
      <content type="text"><![CDATA[Neural Architecture Search With Reinforcement Learning ë…¼ë¬¸ì„ ì½ê³  ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.Overview(You should include contents of summary and introduction.)  we use a re- current network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set.RNNì„ ì´ìš©í•´ì„œ neural networkì˜ model description(í•˜ì´í¼ íŒŒë¼ë¯¸í„°: # of filters, stride length â€¦)ì„ ë¬¸ìžì—´ë¡œ ìƒì„±í•œë‹¤.ê°•í™”í•™ìŠµì„ í†µí•´ expected accuracyë¥¼ ìµœëŒ€ë¡œ ë§Œë“ ë‹¤.  Controllerì—ì„œ pì˜ í™•ë¥ ë¡œ Aë¼ëŠ” Architectureë¥¼ ìƒì„±í•œë‹¤.  ìžì‹ ë„¤íŠ¸ì›Œí¬ì—ì„œëŠ” A ì•„í‚¤í…ì³ë¥¼ í›ˆë ¨ì‹œì¼œ ì •í™•ë„ Rì„ êµ¬í•œë‹¤.  ì •í™•ë„ë¥¼ ë¦¬ì›Œë“œì˜ ì‹ í˜¸ë¡œ ì‚¬ìš©í•œë‹¤. policy gradientë¥¼ ê³„ì‚°í•´ì„œ ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.  ë°˜ë³µí•˜ë‹¤ë³´ë©´ ë” ë†’ì€ í™•ë¥ ë¡œ ë” ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì´ëŠ” ì•„í‚¤í…ì³ë¥¼ ì°¾ì„ ìˆ˜ ìžˆë‹¤.Related work (Basic concepts)NAS ë¶€ë¶„ì˜ ê±°ì˜ ìµœì´ˆë¼ê³  ë³¼ ìˆ˜ ìžˆìŒ.ì´ì „ ì—°êµ¬ë“¤ : Hyperparameter optimization  it is difficult to ask them to generate a variable-length configuration that specifies the structure and connectivity of a networkìœ ì „ìž ì•Œê³ ë¦¬ì¦˜  search-based ë°©ì‹ì´ë¼ íƒìƒ‰ì†ë„ê°€ ëŠë¦¼.ì»¨íŠ¸ë¡¤ëŸ¬ì—ì„œì˜ Neural Architecture ë°©ì‹ì€ ì´ì „ ì˜ˆì¸¡ê°’ë“¤ì„ inputìœ¼ë¡œ ë°›ì•„ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ í•œ ë²ˆì— í•˜ë‚˜ì”© ì˜ˆì¸¡í•˜ëŠ” auto-regressiveí•œ ë°©ì‹ì´ë‹¤.Methods(Explain one of the methods that the thesis used.)ì´ ë…¼ë¬¸ì˜ Key point : skip connection ì˜ˆì¸¡í•˜ì—¬ ëª¨ë¸ì˜ ë³µìž¡ë„ë¥¼ ë†’ì¸ ê²ƒ, íŒŒë¼ë¯¸í„° ì ‘ê·¼ë°©ì‹ì„ ì‚¬ìš©í•´ì„œ í›ˆë ¨ ì†ë„ë¥¼ ë†’ì¸ ê²ƒ  Generate Model with a Controller Recurrent Neural Network  It predicts filter height, filter width, stride height, stride width, and number of filters for one layer and repeats. Every prediction is carried out by a softmax classifier and then fed into the next time step as input.ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ì´ìš©í•˜ì—¬ CNN ëª¨ë¸ì— ì‚¬ìš©í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ ìƒì„±í•¨.ë ˆì´ì–´ë§ˆë‹¤ ì‚¬ìš©í•  í•„í„°, Stride ê°’ì„ ì˜ˆì¸¡í•˜ê³  ë°˜ë³µí•¨.í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì˜ˆì¸¡ì‹œì— softmax classifierë¥¼ ê±°ì¹œê°’ì´ ë‹¤ìŒ ìŠ¤í…ì˜ inputìœ¼ë¡œ ë“¤ì–´ê°.ì»¨íŠ¸ë¡¤ëŸ¬ RNNì´ ì•„í‚¤í…ì³ë¥¼ ìƒì„±í•˜ë©´ ìƒì„±ëœ ì•„í‚¤í…ì³ì˜ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ë¥¼ í›ˆë ¨ì‹œí‚´.  The parameters of the controller RNN, Î¸c, are then optimized in order to maximize the expected validation accuracy of the proposed architectures.Validation setìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ì˜ ì •í™•ë„ë¥¼ ì¸¡ì •í•˜ê³ , ì»¨íŠ¸ë¡¤ëŸ¬ RNNì˜ íŒŒë¼ë¯¸í„° ì„¸íƒ€CëŠ” ì •í™•ë„ì˜ ê¸°ëŒ€ê°’ì„ ìµœëŒ€í™”í•˜ê¸° ìœ„í•´ ìµœì í™”ë¨.  Training with Reinforce      controller to maximize its expected reward        ì»¨íŠ¸ë¡¤ëŸ¬ token list a[1]:a[T] : Architecture predicted by the controller RNN viewed as a sequence of actions    ìžì‹ ë„¤íŠ¸ì›Œí¬ëŠ” ìƒì„±ëœ êµ¬ì¡°ì˜ ì •í™•ë„ Rì„ ì¶œë ¥í•˜ê³ , ì´ Rì„ ê°•í™”í•™ìŠµì˜ ë¦¬ì›Œë“œë¡œ ì‚¬ìš©í•´ì„œ ì»¨íŠ¸ë¡¤ëŸ¬ë¥¼ ê°•í™”í•™ìŠµ í›ˆë ¨ì‹œí‚´.  Layer í•˜ë‚˜ì§œë¦¬ CNNì—ì„œì˜ T=3ìž„.          a1 : filter height      a2 : filter width      a3 : # of filters        In this work, we use the REINFORCE rule from Williams (1992)  Standard REINFORCE Update Rule  Rì€ ë¯¸ë¶„ ë¶ˆê°€ëŠ¥í•¨. =&gt; policy gradientë¥¼ ì¨ì„œ ì„¸íƒ€ Cë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.Accelerate Training with Parallelism and Asynchronous Updates  ìžì‹ ë„¤íŠ¸ì›Œí¬ : í•˜ë‚˜ì˜ ëª¨ë¸ì„ ëœ»í•¨  ì—¬ëŸ¬ ì»¨íŠ¸ë¡¤ëŸ¬ * ì—¬ëŸ¬ ìžì‹ ë„¤íŠ¸ì›Œí¬ =&gt; ë§Žì€ ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“¤ì–´ëƒ„          í›ˆë ¨ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ íŒŒë¼ë¯¸í„°-ì„œë²„ êµ¬ì¡° ì‚¬ìš©      Sê°œì˜ íŒŒë¼ë¯¸í„° ì„œë²„ê°€ ìžˆê³  ì´ ì„œë²„ì™€ ì—°ê²°ëœ Kê°œì˜ ë³µì œëœ ì»¨íŠ¸ë¡¤ëŸ¬ì— ê³µìœ ëœ íŒŒë¼ë¯¸í„° ê°’ì´ ì €ìž¥ë¨.ê°ê°ì˜ ì»¨íŠ¸ë¡¤ëŸ¬ëŠ” mê°œì˜ ìžì‹ ë„¤íŠ¸ì›Œí¬ë¥¼ ë³µì œí•´ì„œ ë³‘ë ¬ë¡œ í›ˆë ¨ì‹œí‚´.ì´ ë•Œ ìžì‹ ë„¤íŠ¸ì›Œí¬ì˜ ì •í™•ë„ëŠ” íŒŒë¼ë¯¸í„° ì„œë²„ì— ë³´ë‚¼ ì„¸íƒ€ Cì— ëŒ€í•œ gradientë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ê¸°ë¡ë¨.  Increase Architecture Complexity with Skip Connection and Other Layer TypesSkip connectionì„ ì¶”ê°€í•´ì„œ íƒìƒ ê³µê°„ì„ ë„“ížŒë‹¤.ë ˆì´ì–´ë§ˆë‹¤ anchor pointë¥¼ ë”í•´ì„œ ì´ì „ ë ˆì´ì–´ë“¤ ì¤‘ ì–´ë–¤ ë ˆì´ì–´ë¥¼ í˜„ìž¬ ë ˆì´ì–´ì˜ inputìœ¼ë¡œ í• ì§€ ê²°ì •í•¨.  Generate Recurrent Cell Architecturesì§€ê¸ˆê¹Œì§€ CNNì„ ìœ„í•œ Neural Architecture, ì§€ê¸ˆì€ RNNRNN, LSTMì€ x(t), h(t-1)ì„ inputìœ¼ë¡œ í•˜ê³  h(t)ë¥¼ outputìœ¼ë¡œ í•˜ëŠ” íŠ¸ë¦¬êµ¬ì¡°ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìžˆìŒ(ë§¨ ì˜¤ë¥¸ìª½)RNN ì»¨íŠ¸ë¡¤ëŸ¬ì—ì„œëŠ” íŠ¸ë¦¬ ë…¸ë“œë“¤ì˜ ê²°í•©ë°©ì„(addition, elementwise multiplication)ê³¼ í™œì„±í™”í•¨ìˆ˜(sigmoid,tanh)ë¥¼ ì„ íƒí•  ìˆ˜ ìžˆìŒ.  ê·¸ë¦¼ (b)ì˜ Cell indices ì˜ ì™¼ìª½ 1ë¶€ë¶„ì´ ì˜ë¯¸í•˜ëŠ” ê²ƒì€ ë‹¤ìŒ ë©”ëª¨ë¦¬êµ¬ì¡° C_tì™€ ì—°ê²°ë˜ëŠ”ê²ƒì€ Tree index 1 ì´ë©° ì˜¤ë¥¸ìª½ 0ë¶€ë¶„ì€ h_t ë¥¼ êµ¬í• ë•Œ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ Tree index 0 ì´ë¼ëŠ” ê²ƒìž…ë‹ˆë‹¤. ê·¸ë¦¼ (b)ì˜ Tree index 2 ëŠ” Tree0ê³¼ Tree1ì˜ ê²°í•©ë°©ì‹ì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒìœ¼ë¡œ ê·¸ë¦¼ì—ì„  elementwise multiplicationì™€ sigmoidì˜ ê²°í•©ì´ ë©ë‹ˆë‹¤.Experimentsê¸°ì¡´ SOTA ëª¨ë¸ê³¼ ë¹„êµí–ˆì„ ë•Œ ì•½ê°„ì˜ ì„±ëŠ¥ ê°ì†ŒëŠ” ìžˆì—ˆì§€ë§Œ ë” ìž‘ì€ íŒŒë¼ë¯¸í„°ë¡œ êµ¬í˜„ì´ ë˜ì—ˆìŒ,  CNN (CIFAR-10 dataset)  RNN (Penn Treebank dataset)  Transfer Learning on Neural Machine Translation          LSTMì„ ë¹¼ê³  NASë¥¼ í†µí•´ ë§Œë“  cellì„ ë„£ì—ˆìŒ.      LSTMì— íŠ¹í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ íŠœë‹í•˜ì§€ ì•ŠìŒ      BELU score 0.5 ì˜¤ë¦„      Additional studies(If you have some parts that cannot understand, you have to do additional studies for them. Itâ€™s optional.)      Understanding Deep Learning Requires Rethinking Generalization        Designing Neural Network Architectures Using RL  References(References for your additional studies)https://www.youtube.com/watch?v=XP3vyVrrt3Qhttps://medium.com/@sunwoopark/slow-paper-neural-architecture-search-with-reinforcement-learning-6de601560522]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Reinforcement Learning </tag>
        
          <tag> Neural Architecture Search </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì´ìš©í•œ ìˆ˜í™” êµìœ¡ ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜-Handlang(2)]]></title>
      <url>/project/2020/08/26/Handlang2/</url>
      <content type="text"><![CDATA[DSC EWHAì—ì„œ 2019.9~2020.8 ê¹Œì§€ ì§„í–‰í•œ íŒ€í”„ë¡œì íŠ¸ë¡œ, ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì´ìš©í•œ ìˆ˜í™” í•™ìŠµ ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜ìž…ë‹ˆë‹¤.ì´ í¬ìŠ¤íŒ…ì—ì„œëŠ” ì›¹ì— ê´€ë ¨ëœ ê²ƒì„ ë‹¤ë£¹ë‹ˆë‹¤.Handlang - ASL(American Sign Language) Education by using deep learning modelë”¥ëŸ¬ë‹ìœ¼ë¡œ í•™ìŠµëœ ìˆ˜í™” ì¸ì‹ ëª¨ë¸ì„ ë°”íƒ•ìœ¼ë¡œ ì•ŒíŒŒë²³, ìˆ«ìžì— í•´ë‹¹ë˜ëŠ” ìˆ˜í™”ë¥¼ í•™ìŠµ ë° ì—°ìŠµ í•  ìˆ˜ ìžˆëŠ” ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜ìž…ë‹ˆë‹¤.Wireframe - FigmaFigmaë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ€ì›ë“¤ê³¼ í™ˆíŽ˜ì´ì§€ ì™€ì´ì–´í”„ë ˆìž„ì„ êµ¬ìƒí•˜ì˜€ìŠµë‹ˆë‹¤.Flaskì›¹ ê°œë°œ ì´ˆë³´ìžì—ê²Œ ë¹„êµì  ì‰¬ìš´ Flaskë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.Model Deploy  í•™ìŠµì‹œí‚¨ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë²•...from keras.models import load_model...model = load_model('model/handlang_model_4.h5') # ì§€ë¬¸ìž ëª¨ë¸model2 = load_model('model/su_adamax.h5') # ìˆ«ìž ëª¨ë¸Ajaxì›¹ìº ìœ¼ë¡œ ë°›ì€ ì´ë¯¸ì§€ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ Detectí•´ì•¼í•˜ê¸° ë•Œë¬¸ì— íŽ˜ì´ì§€ë¥¼ ìƒˆë¡œ ê³ ì¹˜ì§€ ì•Šì•„ë„ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ìžˆëŠ” Ajaxë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.Translationí•œê¸€/ì˜ì–´ ë²„ì „ì˜ ì›¹íŽ˜ì´ì§€ë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ flask_babelì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.Study &amp; QuizTeam HandlangProject Github Link]]></content>
      <categories>
        
          <category> Project </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Object Detection </tag>
        
          <tag> Flask </tag>
        
          <tag> ASL Education Application </tag>
        
          <tag> YOLO </tag>
        
          <tag> Fast R-CNN </tag>
        
          <tag> Inception v3 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì´ìš©í•œ ìˆ˜í™” êµìœ¡ ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜-Handlang(1)]]></title>
      <url>/project/2020/08/26/Handlang/</url>
      <content type="text"><![CDATA[DSC EWHAì—ì„œ 2019.9~2020.8 ê¹Œì§€ ì§„í–‰í•œ íŒ€í”„ë¡œì íŠ¸ë¡œ, ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì´ìš©í•œ ìˆ˜í™” í•™ìŠµ ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜ìž…ë‹ˆë‹¤.ì´ í¬ìŠ¤íŒ…ì—ì„œëŠ” ìˆ˜í™” ì¸ì‹ ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ëŒ€í•´ì„œë§Œ ë‹¤ë£¹ë‹ˆë‹¤.Handlang - ASL(American Sign Language) Education by using deep learning modelë”¥ëŸ¬ë‹ìœ¼ë¡œ í•™ìŠµëœ ìˆ˜í™” ì¸ì‹ ëª¨ë¸ì„ ë°”íƒ•ìœ¼ë¡œ ì•ŒíŒŒë²³, ìˆ«ìžì— í•´ë‹¹ë˜ëŠ” ìˆ˜í™”ë¥¼ í•™ìŠµ ë° ì—°ìŠµ í•  ìˆ˜ ìžˆëŠ” ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜ìž…ë‹ˆë‹¤.ëª¨ë¸ ì •í™•ë„ ê°œì„ ì„ ìœ„í•œ ì—¬ëŸ¬ ì‹œë„ë“¤[About models]YOLO darknetYOLO is the model with excellent performance in Object detection.We used darkflow, not yolo darknet, to take advantage of tensorflow.https://github.com/thtrieu/darkflowThe most attempts were made at darkflow.  YOLO_experiment_1          [a~z] 600 images each. training 500 epochs      acc : 0.42      Feedback -&gt; Predict performance is poor.  YOLO_experiment_2          pretrained weight - hand tracking model (https://github.com/Abdul-Mukit/dope_with_hand_tracking)      a~y 600 images each. 140 epochs      acc : 0.47              YOLO_experiment_3          pretrained weight - yolov2-tiny.weight(https://pjreddie.com/darknet/yolov2/)      a~y 600 images each. 220 epochs      acc : 0.56            Feedback -&gt; It is still not a satisfactory performance.Inception-v3We tried transfer learning by using inception-v3.  a~y 600 images each. 1000 steps  test acc. : about 88%          but not that much at real-timeâ€¦            Tensorflow-Object-Detection-APIWe tried transfer learning by using fast r-cnn.  a~y 600 images each. 6000 steps  test acc. : about 80%          not test by images, but test by webcam.                  poor performance                    Self-feedback: It is still not a satisfactory performance.Finally Custom CNN model(our current model)!handlang_model = Sequential()handlang_model.add(Conv2D(64, kernel_size=4, strides=1, activation='relu', input_shape=target_dims))handlang_model.add(Conv2D(64, kernel_size=4, strides=2, activation='relu'))handlang_model.add(Dropout(0.5))handlang_model.add(Conv2D(128, kernel_size=4, strides=1, activation='relu'))handlang_model.add(Conv2D(128, kernel_size=4, strides=2, activation='relu'))handlang_model.add(Dropout(0.5))handlang_model.add(Conv2D(256, kernel_size=4, strides=1, activation='relu'))handlang_model.add(Conv2D(256, kernel_size=4, strides=2, activation='relu'))handlang_model.add(Flatten())handlang_model.add(Dropout(0.5))handlang_model.add(Dense(512, activation='relu'))handlang_model.add(Dense(n_classes, activation='softmax'))handlang_model.summary()handlang_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=["accuracy"])Feedback : ìœ„ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³ , ì›¹ ì½”ë“œ ë‚´ì—ì„œì˜ trickì„ ì´ìš©í•˜ì—¬ ì¡°ê¸ˆ ë” ë¹ ë¥¸ ì¸ì‹ê³¼ ë†’ì€ ì •í™•ë„ë¥¼ ê°€ì§ˆ ìˆ˜ ìžˆì—ˆìŒ.Datasetsì•„ëž˜ ë°ì´í„° ì…‹ë“¤ì€ ëª¨ë¸ íŠ¸ë ˆì´ë‹ì— ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.ì°¸ê³ ë¡œ, ìš°ë¦¬ ëª¨ë¸ì—ì„œëŠ” ì•ŒíŒŒë²³ i,z ì œì™¸í–ˆìŠµë‹ˆë‹¤. (ì†ë™ìž‘ì´ í¬í•¨ë˜ì—ˆê¸° ë•Œë¬¸ì—)  https://www.kaggle.com/grassknoted/asl-alphabet          ê°€ìž¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì— ì‚¬ìš©ëœ ë°ì´í„° ì…‹ìž…ë‹ˆë‹¤.      ë‹¤ë¥¸ ëª¨ë¸ì—ì„œëŠ” ì•„ëž˜ì˜ ë°ì´í„°ì…‹ë“¤ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.  https://www.kaggle.com/rajarshighoshal/asltestimages  https://www.kaggle.com/muhammadkhalid/sign-language-for-alphabets  https://www.kaggle.com/ayuraj/asl-datasetTeam HandlangProject Github Link]]></content>
      <categories>
        
          <category> Project </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Object Detection </tag>
        
          <tag> Flask </tag>
        
          <tag> ASL Education Application </tag>
        
          <tag> YOLO </tag>
        
          <tag> Fast R-CNN </tag>
        
          <tag> Inception v3 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Meta Reinforcement Learning As Task Inference]]></title>
      <url>/paper%20review/2020/08/22/Meta-RL-as-task-inference/</url>
      <content type="text"><![CDATA[Tensorflow KR ë…¼ë¬¸ ì½ê¸° ëª¨ìž„ PR12(Season 3)ì˜ Meta Reinforcement Learning As Task Inference (PR-239)ë¥¼ ë°œí‘œí•˜ì‹  Changhoon Jeongë‹˜ ì˜ìƒì„ ë³´ê³  ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.Meta Reinforcement Learning As Task InferenceOverview(You should include contents of summary and introduction.)  ì ‘ê·¼ ë°©ë²• : Meta-RLì„ í•˜ë‚˜ì˜ paritally observedë¡œ ë³¸ë‹¤.          MDPì˜ ëª¨ë“  ì •ë³´ë¥¼ agentê°€ ì „ë¶€ ë°›ëŠ” ê²Œ ì•„ë‹ˆë¼ ë¶€ë¶„ì ìœ¼ë¡œë§Œ ê´€ì°°      RLì€ í•˜ë‚˜ì˜ controlë¬¸ì œë¡œ ë³¼ ìˆ˜ ìžˆëŠ”ë°, ì—¬ê¸°ì— inference problemì´ ì¶”ê°€ëœ ê²ƒ        ì¦‰ POMDP(Partially-Observable Markov Decision Processes)ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë¬¸ì œê°€ ë¨.          POMDPì˜ ì†”ë£¨ì…˜ : observation trajetoryë¥¼ ê°€ì§€ê³  optimal policyë¥¼ ì°¾ëŠ” ê²ƒ(ë¯¸ëž˜ ë³´ìƒì´ ìµœëŒ€ê°€ ë˜ëŠ” ìª½ìœ¼ë¡œ) == Reinforcement learning                  ê·¸ëŸ¬ë‚˜ ì£¼ì–´ì§„ ë°ì´í„°ê°€ partially observationì´ê¸° ë•Œë¬¸ì— ë˜ í•˜ë‚˜ì˜ ëª¨ë“ˆì´ í•„ìš”í•¨ -&gt; belief state                      Belief State : ì–´ë–¤ observation trajectoryê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì‹¤ì œ true stateì˜ probability          ì´ê±¸ êµ¬í•  ìˆ˜ ìžˆìœ¼ë©´ POMDPê°€ êµ¬í•´ì§                  POMDPê°€ êµ¬í•´ì§€ë©´ Meta-RL ë¬¸ì œ í•´ê²°!                    Observationì´ ì£¼ì–´ì¡Œì„ ë•Œ optimal policyë¥¼ êµ¬í•˜ëŠ” ê²ƒì€ belief stateë¥¼ êµ¬í•  ìˆ˜ ìžˆìœ¼ë©´ ë¬¸ì œê°€ í’€ë¦°ë‹¤!-&gt; ê·¸ëŸ¬ë©´ Meta-RL ë¬¸ì œë„ í’€ë¦°ë‹¤.  ì´ ë…¼ë¬¸ì˜ key point : ë‘ ê°€ì§€ Neural Networkë¥¼ ì‚¬ìš©          controlí•˜ëŠ” policy Network      belief stateë¥¼ ì˜ˆì¸¡í•˜ëŠ” inference Belief Module                  auxilary supervised learning ë¡œ í•´ê²° (Meta-learningë•Œì—ë§Œ)          ì¦‰, belief moduleì€ meta-learning ë•Œ supervised learningìœ¼ë¡œ í•™ìŠµë¨.                      off-policy ì‚¬ìš©(Meta-RL ì—ì„œëŠ” ëŒ€ê°œ on-policy)Related work (Basic concepts)  Meta Learning : Learning to Learn      A trajectory is just a sequence of states and actions.    Meta RL          env.ëŠ” MDPë¡œ í‘œí˜„ë¨. M = {S,A,P,r}      Agent &lt;-&gt; env. : ì„œë¡œ interactí•˜ë©´ì„œ future rewardë¥¼ Max. ì‹œí‚¤ëŠ” ì„¸íƒ€ ì°¾ê¸°      Meta learning : ì—¬ëŸ¬ ê°œì˜ taskë“¤ì„ samplingí•˜ì—¬ meta-learningí•œ í›„, meta-testì‹œ ë¹ ë¥´ê²Œ adaptionë  ìˆ˜ ìžˆì–´ì•¼ í•¨.                  RL -&gt; ê·¸ taskë“¤ ê°ê°ì´ í•˜ë‚˜ì˜ MDPë¡œ ì •ì˜ ê°€ëŠ¥!                                  MDPë¥¼ ì—¬ëŸ¬ê°œ samplingí•´ì„œ í•™ìŠµí•˜ê³ , ê·¸ê±¸ í†µí•´ optimal thetaì–»ëŠ” ê²ƒì´ ëª©í‘œ          testì‹œì—ëŠ” ì²˜ìŒ ë³´ëŠ” íƒœìŠ¤í¬(ì „ì²´ì ì¸ ì‰ìžŽì€ ë¹„ìŠ·í•´ì•¼)ì— ìž˜ ì ìš©ë˜ì–´ì•¼ í•¨.                    ì ‘ê·¼ ë°©ë²•                  Recurrent policies : RNN          Optimization problem : MAML          partially observed RL                          MDPì˜ ëª¨ë“  ì •ë³´ë¥¼ ë°›ëŠ” ê²Œ ì•„ë‹ˆë¼ ë¶€ë¶„ì ìœ¼ë¡œë§Œ ë°›ëŠ”ë‹¤ -&gt; inference problem ì¶”ê°€ë¨                                                  z : taskë“¤ì˜ ëª¨ë“  ì •ë³´ë¥¼ ë‹´ê³  ìžˆëŠ” true information              zë¥¼ inferenceí•˜ë©´ì„œ RL ì»¨íŠ¸ë¡¤ë„ í•  ìˆ˜ ìžˆëŠ” í•´ì„í•˜ëŠ” ê´€ì                                             MDP(Markov Decision Processes) : (X,A,P,p0, R, discount factor)          controlì´ ìžˆë‹¤ -&gt; ëŒ€í‘œì ìœ¼ë¡œ RL      stateê°€ ì™„ì „ížˆ ê´€ì¸¡ë˜ëƒ, ë¶€ë¶„ì ìœ¼ë¡œ ê´€ì¸¡ì´ ë˜ëƒ -&gt; MDP/POMDP        POMDP(Partially-Observable Markov Decision Processes)          MDPì˜ generalí•œ ë²„ì „      MDPì—ì„œ omega, Oê°€ ì¶”ê°€ë¨      X : state space. Agent ìž…ìž¥ì—ì„œëŠ” ë¶€ë¶„ì ìœ¼ë¡œ ê´€ì¸¡/ì•„ì˜ˆ ê´€ì¸¡í•  ìˆ˜ ì—†ê²Œ ë¨.      ê·¸ëž˜ì„œ AgentëŠ” ë¶€ë¶„ì ìœ¼ë¡œ ê´€ì¸¡ë˜ëŠ” observation stateë¥¼ í†µí•´ì„œë§Œ í•™ìŠµí•  ìˆ˜ ìžˆìŒ.              off-policy algorithm : í˜„ìž¬ í•™ìŠµí•˜ëŠ” policyê°€ ê³¼ê±°ì— í–ˆë˜ experienceë„ í•™ìŠµì— ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ê³ , ì‹¬ì§€ì–´ëŠ” í•´ë‹¹ policyê°€ ì•„ë‹ˆë¼ ì˜ˆë¥¼ ë“¤ì–´ ì‚¬ëžŒì´ í•œ ë°ì´í„°ë¡œë¶€í„°ë„ í•™ìŠµì„ ì‹œí‚¬ ìˆ˜ê°€ ìžˆë‹¤.    on-policy : 1ë²ˆì´ë¼ë„ í•™ìŠµì„ í•´ì„œ policy improvementë¥¼ ì‹œí‚¨ ìˆœê°„, ê·¸ policyê°€ í–ˆë˜ ê³¼ê±°ì˜ experienceë“¤ì€ ëª¨ë‘ ì‚¬ìš©ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤.Methods(Explain one of the methods that the thesis used.)      Solution to POMDP          observed trajectoryë¥¼ ê°€ì§€ê³  optimal policyë¥¼ ì°¾ëŠ” ê²ƒ(ë¯¸ëž˜ ë³´ìƒì˜ í•©ì´ ìµœëŒ€ê°€ ë˜ëŠ” action set, policy) == RL      ê·¸ëŸ¬ë‚˜ ì£¼ì–´ì§„ ë°ì´í„°ê°€ partially í•˜ê¸° ë•Œë¬¸ì— ë˜ í•˜ë‚˜ì˜ ëª¨ë“ˆì´ í•„ìš”í•¨      Belief State ë¥¼ êµ¬í•  ìˆ˜ ìžˆìœ¼ë©´ ìœ„ì˜ ë¬¸ì œê°€ í’€ë¦¼.      Belief State -&gt; POMDP -&gt; Meta-RL í•´ê²°        ìµœê·¼ Meta-RLì—ì„œì˜ POMDP í•´ì„ ë¬¸ì œ          ë°©ê¸ˆê¹Œì§€ëŠ” unobserved state      But ìš°ë¦¬ëŠ” unobserved task        ë‚´ê°€ ì–´ëŠ taskë¥¼ í’€ê³  ìžˆëŠ”ì§€ ëª¨ë¥´ëŠ” ìƒíƒœ      stateë¥¼ ì™„ì „ížˆ ê´€ì¸¡ì„ ëª»í•œë‹¤ëŠ” ê²ƒì´ ì•„ë‹ˆë¼!! ë‚´ê°€ ì–´ë–¤ taskë¥¼ í’€ê³  ìžˆëŠ”ì§€ë¥¼ ê´€ì¸¡í•˜ì§€ ëª»í•œë‹¤ëŠ” ê´€ì ìœ¼ë¡œ ë¬¸ì œë¥¼ í‘¸ëŠ” ê²ƒìž„ == ì–´ë–¤ taskë¥¼ í’€ì–´ì•¼ í•˜ëŠ”ì§€ task ì •ë³´ë¥¼ ì™„ì „í•˜ê²Œ ê´€ì¸¡í•  ìˆ˜ X -&gt; ê·¸ëž˜ì„œ Task Inference                  State, Actionì€ ëª¨ë“  MDPê°„ì— sharingë˜ì–´ ìžˆìŒ. Wê°€ ë¶™ì€ ê²ƒë“¤ì€ task-specificí•¨.                  ì´ ì„¸ê°€ì§€ì— ì ‘ê·¼í•´ì„œ Meta RLì„ í‘¼ë‹¤.                    ëª©í‘œ : meta-testì‹œ ì²˜ìŒ ë³´ëŠ” taskì—ë„ ì ì€ interactionìœ¼ë¡œ rewardë¥¼ Max. ì‹œí‚¤ëŠ” optimal policy ì°¾ê¸°            Meta-RL using POMDP    A, SëŠ” sharing      - SëŠ” true stateì™€ Agent ìž…ìž¥ì—ì„œ ëª¨ë¥´ëŠ” taskì— ëŒ€í•œ wë¥¼ concatí•´ì„œ ë§Œë“¦          ë‚˜ë¨¸ì§€ë“¤ì€ task-specificí•˜ê²Œ ì •ì˜      wë§Œì´ agentìž…ìž¥ì—ì„œëŠ” unobserved stateë¼ê³  ì •ì˜            optimal agent pi*ëŠ” ì•„ëž˜ì™€ ê°™ì€ ë¬¸ì œë¥¼ í‘¼ë‹¤.    agentìž…ìž¥ì—ì„œëŠ” ì‹¤ì œ task labelì¸ wì— access í•  ìˆ˜ ì—†ë‹¤ê³  ê°€ì •. (taskì— ëŒ€í•œ MDPë¥¼ ë‹¤ ëª¨ë¥´ëŠ” ê²ƒ)          ê³¼ê±° observation trajectoriesì€ LSTM, GRUë“±ìœ¼ë¡œ agent networkëŠ” í•™ìŠµí•  ìˆ˜ ìžˆìŒ.      POMDP ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ë©´ taskì— ëŒ€í•œ belief stateë¥¼ ê³„ì‚°í•  ìˆ˜ ìžˆì–´ì•¼ í•¨.                  observation trajectoryê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì‹¤ì œ true taskì¼ í™•ë¥  (posterior)                          ì´ê±¸ ê³„ì‚°í•  ìˆ˜ ìžˆìœ¼ë©´ POMDP ë¬¸ì œ í•´ê²°, ê·¸ëŸ¬ë‚˜ ê³„ì‚° ì–´ë ¤ì›€.                                appendix)                        belief stateì˜ posteriorëŠ” bayes Ruleê³¼ ìœ ì‚¬  policyê°€ ì‹ ì•ˆì— ì—†ìŒ -&gt; taskì— ëŒ€í•œ posteriorëŠ” policyì™€ independentí•˜ë‹¤.  off-policy algorithm ì‚¬ìš© ê°€ëŠ¥ (ë³´í†µ meta-RLì—ì„œëŠ” on-policy : ë°ì´í„°ë¥¼ ëª¨ìœ¼ìžë§ˆìž ë°”ë¡œ ì—…ë°ì´íŠ¸)Trainì–´ë–»ê²Œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ”ê°€      current state xì™€ current belief b_t(w)ë§Œ ìžˆìœ¼ë©´ ê°€ëŠ¥    ì•žì„œ ì–¸ê¸‰í•œ z ê°€ ì—¬ê¸°ì„œëŠ” belief state(taskì— ëŒ€í•œ ëª¨ë“  ì •ë³´ë¥¼ ë‹´ê³  ìžˆë‹¤)  belief state estimate ê°€ëŠ¥ -&gt; optimal policy êµ¬í•  ìˆ˜ ìžˆë‹¤.  ì»´í“¨íŒ… ì–´ë ¤ì›€ -&gt; 2ê°€ì§€ NN ëª¨ë¸ë¡œ approximate í•´ì•¼ í•œë‹¤.      Controlí•˜ëŠ” Policy Network        Belief Module    ì–´ë–»ê²Œ í•™ìŠµì‹œí‚¤ëŠ”ê°€?          Meta-Learning ì•ˆì—ì„œëŠ” Supervised Learningì´ ëœë‹¤.                  label ê°€ëŠ¥ì„±                          Task Description : taskë¥¼ ìž˜ í‘œí˜„í•˜ëŠ” representation.              Expert actions              Task embeddings                                            meta-learning ë•Œì—ë§Œ ì‚¬ìš©. ì‹¤ì œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” meta-testì—ì„œ ì ì€ ì—í”¼ì†Œë“œë§Œìœ¼ë¡œ ë¹ ë¥´ê²Œ adaptí•  ë•ŒëŠ” ì´ ì •ë³´ë“¤ì„ ë”ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ.      taskë¥¼ supervised learningìœ¼ë¡œ í’€ì—ˆë‹¤!      w, h_t : independent of policy =&gt; belief networkë„ off-policyë¡œ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥      ArchitectureLSTM, IB : optional (IB : regularization íš¨ê³¼)A. Baseline LSTM Agent : belief networkê°€ ì—†ëŠ” ì¼ë°˜ agentB. ë…¼ë¬¸ì—ì„œ ì œì•ˆëœ ëª¨ë¸ : trajectory ë„£ì–´ì„œ Belief network í•™ìŠµ    - Belief feature + trajectory í•´ì„œ policy network í•™ìŠµ    - ê°ìž ì—­í• ì— ì§‘ì¤‘C. Mixed : Network í•˜ë‚˜ì— í•©ì¹œ ê²ƒExperiment  Multi-armed bandit : 20 arms and 100 horizon.          20ë²ˆ ë‹¹ê²¼ì„ ë•Œ reward probability =&gt; task description. (armë“¤ì˜ vec.)        Semicircle : ë°˜ì› ì•ˆì—ì„œ naviagtion          task desription : ê°ë„        cheetah : task description -&gt; velocityMain contributions  Supervised Learningì„ í†µí•´ì„œ performance ë†’ìž„  Belief network í•™ìŠµì‹œí‚¬ ë•Œ off-policy ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš© ê°€ëŠ¥  continuous, sparse rewards í™˜ê²½ì—ì„œë„ ì¢‹ì•˜ë‹¤Additional studies(If you have some parts that cannot understand, you have to do additional studies for them. Itâ€™s optional.)POMDPBelief State?References(References for your additional studies)https://www.youtube.com/watch?v=phi7_QIhfJ4 - ë…¼ë¬¸ ì„¤ëª…https://newsight.tistory.com/250 - policy ë¶€ë¶„ ê°œë…]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Meta Learning </tag>
        
          <tag> Reinforcement Learning </tag>
        
          <tag> Meta RL </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Model-Agnostic Meta-Learning for fast adaptation of deep networks]]></title>
      <url>/paper%20review/2020/08/14/Model-Agnostic-Meta/</url>
      <content type="text"><![CDATA[Model-Agnostic Meta-Learning for fast adaptation of deep networks ë¥¼ ì½ê³  ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.Model-Agnostic Meta-Learning for fast adaptation of deep networksOverview(You should include contents of summary and introduction.)ì¢‹ì€ Weightë¡œ initializeí•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ê²ƒ.ì–´ë–»ê²Œ í•˜ë©´ ì¢‹ì€ initial weightë¥¼ ì°¾ì„ ìˆ˜ ìžˆëŠ”ê°€?ì–´ë–¤ inital weightë¥¼ ê°€ì§€ë©´ ëª¨ë¥´ëŠ” íƒœìŠ¤í¬ë“¤ì— ëŒ€í•´ì„œë„ ë¹¨ë¦¬ ì ì‘ì‹œí‚¬ ìˆ˜ ìžˆëŠ”ê°€?  Key Point- **ì£¼ì–´ì§„ íƒœìŠ¤í¬ë“¤ì— ëŒ€í•´ì„œ 1 step ê°”ì„ ë•Œ, ëª¨ë“  íƒœìŠ¤í¬ì— ëŒ€í•´ì„œ ë¡œìŠ¤ê°€ ë¯¸ë‹ˆë©ˆì´ ë˜ëŠ” í˜„ìž¬ì˜ ì„¸íƒ€ë¥¼ ì°¾ëŠ” ê²ƒ!**Related work (Basic concepts)  Model-Agnostic          í•™ìŠµì— ì‚¬ìš©ëœ modelì´ ë¬´ì—‡ì¸ì§€ì— êµ¬ì• ë°›ì§€ ì•Šê³  ë…ë¦½ì ìœ¼ë¡œ ëª¨ë¸ì„ í•´ì„í•  ìˆ˜ ìžˆë‹¤ëŠ” ëœ»      ì¦‰, í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ê³¼ ì„¤ëª…ì— ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ì„ ë¶„ë¦¬      ì´ ë°©ë²•ì€ ì–´ë–¤ ëª¨ë¸ì´ë“  ìƒê´€ ì—†ì´ ì ìš©í•  ìˆ˜ ìžˆëŠ” ë°©ë²•ì´ë‹¤.        Meta learning          learning to learn      ì¢‹ì€ ë©”íƒ€ ëŸ¬ë‹ ëª¨ë¸ = íŠ¸ë ˆì´ë‹ ë•Œ ì ‘í•˜ì§€ ì•Šì•˜ë˜ ìƒˆë¡œìš´ íƒœìŠ¤í¬ë‚˜ í™˜ê²½ì— ëŒ€í•´ì„œ ìž˜ ì ì‘ë˜ê±°ë‚˜ ì¼ë°˜í™”ê°€ ìž˜ ë¨.      Reinforcement learningê³¼ ê²°í•©í•œ meta-learning(meta reinforcement learning) ì–˜ê¸°ê°€ ë§Žì´ ë‚˜ì˜¤ê³  ìžˆìŒ      Few-shot classificationì€ supervised-learning ìƒí™©ì—ì„œ meta-learningì„ í™œìš©í•œ ì˜ˆì‹œìž„.                  í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ ìžì²´ê°€ í•˜ë‚˜ì˜ data sampleë¡œ í™œìš©ë˜ê³  ìžˆìŒ.                                  ì¦‰ Meta-learningì—ì„œëŠ” training, testì˜ ê°œë…ì´ ì¼ë°˜ê³¼ ì•½ê°„ ë‹¤ë¥´ê³ , ê·¸ ë•Œ ë“¤ì–´ê°€ëŠ” ë°ì´í„°ì…‹ë„ ë‹¤ë¥´ë‹¤.          ì•½ê°„ì˜ fine-tuning ê³¼ ìœ ì‚¬í•œ ì ‘ê·¼ë²•                      Few-shot learning          ì ì€ ìˆ˜ì˜ ë°ì´í„°ë¡œ í•™ìŠµ ì‹œí‚¤ëŠ” ê²ƒ      one-shot learning : í•œ ìž¥ì˜ ë°ì´í„°ë§Œìœ¼ë¡œ í•™ìŠµ ì‹œí‚¤ëŠ” ê²ƒ      K-shot learningì´ë¼ê³  ë§Žì´ ë¶€ë¥´ëŠ” ë“¯      Methods(Explain one of the methods that the thesis used.)  ì„¸íƒ€ 1,2,3 -&gt; 1,2,3ë²ˆ íƒœìŠ¤í¬ë¼ê³  ë³´ìž.  ë§Œì•½ 1ë²ˆ íƒœìŠ¤í¬ì— ëŒ€í•´ì„œ í•™ìŠµì„ ì‹œí‚¨ë‹¤ ê·¸ëŸ¬ë©´, 1ì˜ optimum pointë¡œ ê°€ê²Œë” í•™ìŠµì‹œì¼œì•¼ í•¨.  ê·¼ë° ìƒ˜í”Œì´ ë§Žì§€ ì•Šìœ¼ë‹ˆê¹Œ ì¤‘ê°„ì— Local min.ì— ë¹ ì§€ëŠ” ê²½ìš° ë“± ì›í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í˜ëŸ¬ê°€ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ë” í¼.  ë©”íƒ€ëŸ¬ë‹ì„ í†µí•´ ì„¸íƒ€ë¥¼ ì € ì (í™”ì‚´í‘œê°€ ê°€ë¦¬í‚¤ëŠ”ì )ì— ê°€ì§€ê³  ì˜¤ë©´ 1,2,3ì— ê°€ìž¥ ê°€ê¹Œìš´ í¬ì¸íŠ¸. ì¦‰, ì—¬ê¸°ë¥¼ ì–´ë–»ê²Œ ë³´ë‚¼ê±°ëƒëŠ” ë¬¸ì œ!  ì˜ˆë¥¼ ë“¤ì–´, 3ë²ˆ íƒœìŠ¤í¬ì— ëŒ€í•œ One-shotì´ ì£¼ì–´ì¡Œì„ ë•Œ gradient 1ë²ˆí•´ì„œ 3ë²ˆìª½ìœ¼ë¡œ ë”± ê°€ê³  ì‹¶ì€ ê²ƒ.ìˆ˜ì‹ìœ¼ë¡œ ë³´ëŠ” ì•„ì´ë””ì–´  ië²ˆì§¸ íƒœìŠ¤í¬ì— ëŒ€í•œ ì„¸íƒ€í”„ë¼ìž„ ì •ì˜  1 step gradientë¥¼ ê°”ì„ ë•Œì˜ í¬ì¸íŠ¸ìž„.ìš°ë¦¬ëŠ” ì´ ì„¸íƒ€ í”„ë¼ìž„ì˜ í¬ì¸íŠ¸ì—ì„œ lossê°€ ìµœì†Œê°€ ë˜ê²Œ í•˜ê³  ì‹¶ìŒ! â€“&gt; ì„¸íƒ€ í”„ë¼ìž„ì—ì„œ ë¡œìŠ¤ê°€ ë¯¸ë‹ˆë©ˆì´ ë˜ëŠ” ì„¸íƒ€ë¥¼ ì°¾ê³  ì‹¶ë‹¤!!  ì„¸íƒ€ í”„ë¼ìž„ì„ ìœ„ì—ì„œ ì •ì˜í–ˆìœ¼ë¯€ë¡œ, loss ì‹ì— ë„£ì„ ìˆ˜ ìžˆë‹¤.  ì„¸íƒ€ì—ì„œ 1 ìŠ¤í… ë” ê°„ í¬ì¸íŠ¸ì˜ ë¯¸ë‹ˆë©ˆì„ ì •ì˜í•˜ê²Œ ë˜ëŠ” ê²ƒ(== ì„¸íƒ€í”„ë¼ìž„)  ì„¸íƒ€ì— ëŒ€í•´ì„œ ë¯¸ë¶„í•¨!  íƒœìŠ¤í¬ê°€ ì—¬ëŸ¬ ê°€ì§€ ìžˆìœ¼ë‹ˆê¹Œ ì—¬ëŸ¬ ê°€ì§€ì— ëŒ€í•´ ì „ì²´ê°€ ë¯¸ë‹ˆë©ˆì´ ë˜ëŠ” í¬ì¸íŠ¸ë¥¼ ì°¾ì•„ì•¼ í•¨.  ì„¸íƒ€ í”„ë¼ìž„ ì•ˆì—ëŠ” ì´ë¯¸ ì„¸íƒ€ì— ëŒ€í•œ ë¯¸ë¶„ì´ ë“¤ì–´ê°€ìžˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œ ë¯¸ë¶„ì„ ë˜ í•˜ê²Œ ë˜ë©´ hessianì´ ë‚˜ì˜¬ ê²ƒì´ë‹¤(?)ë‹¤ì‹œ ìš”ì•½í•˜ìžë©´,      ìš°ë¦¬ê°€ ì°¾ê³  ì‹¶ì€ ì„¸íƒ€ëŠ” íƒœìŠ¤í¬ ê°ê°ì„ minimizeí•˜ëŠ” ì„¸íƒ€ê°€ ì•„ë‹ˆë¼,        ì£¼ì–´ì§„ íƒœìŠ¤í¬ë“¤ì— ëŒ€í•´ì„œ 1 step ê°”ì„ ë•Œ ëª¨ë“  íƒœìŠ¤í¬ì— ëŒ€í•´ì„œ minimumì´ ë˜ëŠ”        ì§€ê¸ˆ í˜„ìž¬ì˜ ì„¸íƒ€ë¥¼ ì°¾ëŠ” ê²ƒ.  Algorithm  Model-Agnostic Meta-Learning  ìš°ì„  íŒŒë¼ë¯¸í„°ë“¤ì„(ì„¸íƒ€) ëžœë¤í•˜ê²Œ initialize  íƒœìŠ¤í¬ë“¤ì„ samplingí•˜ê³ , forë¬¸ ì•ˆì—ì„œëŠ” ê°ê°ì˜ íƒœìŠ¤í¬ì— ëŒ€í•œ ê·¸ëž˜ë””ì–¸íŠ¸ë¥¼ ì°¾ìŒ          1 step ë” ê°€ëŠ” ê·¸ëž˜ë””ì–¸íŠ¸        ëª¨ë“  íƒœìŠ¤í¬ë“¤ì— ëŒ€í•´ì„œ ë‹¤ì‹œ ê·¸ëž˜ë””ì–¸íŠ¸ë¥¼ í•´ì„œ sumí•¨.  ì›ëž˜ íŒŒë¼ë¯¸í„° ì„¸íƒ€ë¥¼ ì—…ë°ì´íŠ¸  ìœ„ì˜ ê³¼ì • ë°˜ë³µ  MAML for Few-shot Supervised learning  regression loss      classification loss    Few-shot ì´ë¯¸ì§€ classificationì¼ ê²½ìš° lossë¥¼ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ë¡œ ê³„ì‚°  ë©”íƒ€ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ìƒ˜í”Œë§ì„ í•¨.          ìµœì¢… ë©”íƒ€ íŒŒë¼ë¯¸í„°(ì„¸íƒ€)ë¥¼ ì°¾ê¸° ìœ„í•´ì„œ ì“°ì´ëŠ” ìƒ˜í”Œë“¤      ì´ì „ì˜ ìƒ˜í”Œ DëŠ” ì„¸íƒ€ í”„ë¼ìž„ì„ ìœ„í•œ ìƒ˜í”Œë“¤ìž„.        MAML for Reinforcement Learning  RewardëŠ” ë¯¸ë¶„ì´ ê°€ëŠ¥í•´ì•¼í•˜ë‹ˆê¹Œ policy gradientë¥¼ ì‚¬ìš©í•¨.  f(theta)ëŠ” policyë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë‰´ëŸ´ ë„·  negative rewardë¥¼ lossë¡œ ì‚¬ìš©  ì—í”¼ì†Œë“œ ê¸¸ì´ë§Œí¼ ì­‰ ì§„í–‰í•´ì„œ sumí•œ ê²ƒì´ lossê°€ ë¨.  ë§ˆì°¬ê°€ì§€ë¡œ ê° íƒœìŠ¤í¬ì— ëŒ€í•´ì„œ ìƒ˜í”Œì„ í•˜ê³ , ì „ì²´ ì—í”¼ì†Œë“œ length ë§Œí¼ Kë²ˆ trajecctories ìƒ˜í”Œ  ê·¸ëž˜ë””ì–¸íŠ¸ ê³„ì‚°í•´ì„œ 1 step ë” ê°„ í¬ì¸íŠ¸ë¥¼ ì°¾ì•„ë‚´ê³   1 step ë” ê°„ íŒŒë¼ë¯¸í„° ì…‹ì—ì„œì˜ ìƒ˜í”Œ trajectoriesë“¤ì„ ìƒ˜í”Œë§ í•œ ë‹¤ìŒì—  ë‹¤ í•˜ê³  ë°”ê¹¥ìœ¼ë¡œ ë‚˜ì™€ì„œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ lossë¥¼ ê³„ì‚°í•˜ê³  ê·¸ëž˜ë””ì–¸íŠ¸ë¥¼ êµ¬í•¨.1,2,3ë²ˆ ëª¨ë‘ í¬ê²Œ ë‹¤ë¥´ì§€ ì•Šë‹¤.Experimental Result  The goal of our experimental evaluation          ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ learningì„ í•  ìˆ˜ ìžˆëŠ”ê°€      ì„œë¡œ ë‹¤ë¥¸ ë„ë©”ì¸ì—ì„œ ì‚¬ìš©ì´ ë  ìˆ˜ ìžˆëŠ”ê°€ -&gt; supervised regression, classification, reinforcement learning      ì—¬ëŸ¬ ë²ˆ gradeint updateë¥¼ í• ìˆ˜ë¡ ë” ì¢‹ì•„ì§€ëŠ”ê°€        Regression  sine wave fitting ì„ ì‹¤í—˜.          ìž„ì˜ì˜ sine wave ë¥¼ ë§Œë“¤ì–´ì„œ, ê·¸ê²ƒì„ fitting í•˜ëŠ” ì˜ˆì œ        ì‚¼ê°í˜• : íŠ¸ë ˆì´ë‹ ìƒ˜í”Œ  ë¹¨ê°„ìƒ‰ = ground truth  ì—°ë‘ìƒ‰ = ë©”íƒ€ëŸ¬ë‹ì„ í†µí•´ í•™ìŠµëœ pre-weight  ì´ˆë¡ìƒ‰ = ê·¸ëž˜ë””ì–¸íŠ¸ë¥¼ 1 step / 10 steps ë°Ÿì•˜ì„ ë•Œa) K = 5(5ê°œì˜ ìƒ˜í”Œì´ ì£¼ì–´ì¡Œì„ ë•Œ)b) K = 10 (10ê°œì˜ ìƒ˜í”Œì´ ì£¼ì–´ì¡Œì„ ë•Œ)  ê·¸ëž˜ë””ì–¸íŠ¸ 10ë²ˆí•˜ë©´ ê±°ì˜ ë˜‘ê°™ì´ ë¨c,d) Pre-traineed model ì‚¬ìš©  fittingí•˜ëŠ” ë‰´ëŸ´ ë„·ì„ sine wave taskë¡œ ìž”ëœ© ë§Œë“¤ì–´ì„œ í‰ê· ì ì¸ sine waveì— ëŒ€í•´ í•™ìŠµëœ ê²ƒ  pre-updateëŠ” meta-learningìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ëª¨ë¸ê³¼ ìœ ì‚¬í•˜ë‚˜ 1 step ê°„ë‹¤ê³  í•´ì„œ ë§‰ ë³€í•˜ì§€ ì•ŠìŒ.          fittingì´ ìž˜ ì•ˆëœë‹¤.        Classification  One-shot, few-shot learningì—ì„œ ì£¼ë¡œ ì“°ì´ëŠ” ë°ì´í„° ì…‹ : Omniglot dataset          few-shot learningì˜ mnistê°™ì€ ë°ì´í„°ì…‹        First order approx          ë‘ ë²ˆ ë¯¸ë¶„í•˜ê¸° ìœ„í•´ hessianì´ ë“¤ì–´ê°„ë‹¤ê³  í–ˆìŒ.      ê·¼ë° ReLUëŠ” ì¤‘ê°„ì— ë¯¸ë¶„ ë¶ˆê°€ëŠ¥í•œ í¬ì¸íŠ¸ê°€ ìžˆê³ , ì´ë¥¼ ì œì™¸í•˜ë©´ Linearí•¨.      ë”°ë¼ì„œ ì´ê²ƒì„ first orderê¹Œì§€ë§Œ ê³„ì‚°í•˜ê³  ì—…ë°ì´íŠ¸í•´ë„ ì„±ëŠ¥ì´ ê·¸ë ‡ê²Œ ë–¨ì–´ì§€ì§€ ì•ŠëŠ”ë‹¤ê³  í•¨.      ì •ì„ëŒ€ë¡œ ë‘ ë²ˆ ë¯¸ë¶„í•˜ë©´ 33%ì •ë„ ë” ì˜¤ëž˜ê±¸ë¦°ë‹¤ê³  í•¨.        Reinforcement learning  2D navigation ì‹¤í—˜ : ìœ„ì¹˜ë¥¼ ì •í•´ì£¼ê³  goalê¹Œì§€ ê°€ê¸°  3step updateí•˜ë©´ ìž˜ ê°„ë‹¤  pre-trainí•˜ê³  fine-tuningí•˜ëŠ” ë°©ë²•ê³¼ ë¹„êµâ€¦Codefrom dragen1860 / MAML-Pytorch    def forward(self, x_spt, y_spt, x_qry, y_qry):        """        :param x_spt:   [b, setsz, c_, h, w]        :param y_spt:   [b, setsz]        :param x_qry:   [b, querysz, c_, h, w]        :param y_qry:   [b, querysz]        :return:        """        task_num, setsz, c_, h, w = x_spt.size()        querysz = x_qry.size(1)        losses_q = [0 for _ in range(self.update_step + 1)]  # losses_q[i] is the loss on step i        corrects = [0 for _ in range(self.update_step + 1)]        for i in range(task_num):            # 1. run the i-th task and compute loss for k=0            logits = self.net(x_spt[i], vars=None, bn_training=True)            loss = F.cross_entropy(logits, y_spt[i])            grad = torch.autograd.grad(loss, self.net.parameters())            fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, self.net.parameters())))            # this is the loss and accuracy before first update            with torch.no_grad():                # [setsz, nway]                logits_q = self.net(x_qry[i], self.net.parameters(), bn_training=True)                loss_q = F.cross_entropy(logits_q, y_qry[i])                losses_q[0] += loss_q                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)                correct = torch.eq(pred_q, y_qry[i]).sum().item()                corrects[0] = corrects[0] + correct            # this is the loss and accuracy after the first update            with torch.no_grad():                # [setsz, nway]                logits_q = self.net(x_qry[i], fast_weights, bn_training=True)                loss_q = F.cross_entropy(logits_q, y_qry[i])                losses_q[1] += loss_q                # [setsz]                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)                correct = torch.eq(pred_q, y_qry[i]).sum().item()                corrects[1] = corrects[1] + correct            for k in range(1, self.update_step):                # 1. run the i-th task and compute loss for k=1~K-1                logits = self.net(x_spt[i], fast_weights, bn_training=True)                loss = F.cross_entropy(logits, y_spt[i])                # 2. compute grad on theta_pi                grad = torch.autograd.grad(loss, fast_weights)                # 3. theta_pi = theta_pi - train_lr * grad                fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, fast_weights)))                logits_q = self.net(x_qry[i], fast_weights, bn_training=True)                # loss_q will be overwritten and just keep the loss_q on last update step.                loss_q = F.cross_entropy(logits_q, y_qry[i])                losses_q[k + 1] += loss_q                with torch.no_grad():                    pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)                    correct = torch.eq(pred_q, y_qry[i]).sum().item()  # convert to numpy                    corrects[k + 1] = corrects[k + 1] + correct        # end of all tasks        # sum over all losses on query set across all tasks        loss_q = losses_q[-1] / task_num        # optimize theta parameters        self.meta_optim.zero_grad()        loss_q.backward()        # print('meta update')        # for p in self.net.parameters()[:5]:        # 	print(torch.norm(p).item())        self.meta_optim.step()        accs = np.array(corrects) / (querysz * task_num)        return accsAdditional studies(If you have some parts that cannot understand, you have to do additional studies for them. Itâ€™s optional.)Advanced researches  Meta-SGD : ì„±ëŠ¥ì´ ë” ê´œì°®ìŒ  Bayesian Model-Agnostic Meta-Learning          í•œ í¬ì¸íŠ¸ë¡œ ì§€ì •í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ probabilityë¥¼ ì´ìš©      optimum pointë“¤ì´ ì„œë¡œ ê°€ê¹ê³  ëª°ë ¤ìžˆìœ¼ë©´ ì¢‹ê² ì§€ë§Œ, ì—¬ëŸ¬ ê³³ì— ìžˆì„ ìˆ˜ë„ ìžˆê³  í™•ë¥ ì ìœ¼ë¡œ ë¶„í¬í•  ìˆ˜ë„ ìžˆìŒ.      ì´ëŸ° ê²ƒë“¤ì„ ì–´ë–»ê²Œ ìž˜ ì •ì˜í•  ìˆ˜ ìžˆëŠëƒì— ëŒ€í•œ approachì¸ ë“¯        Gradient-based meta-learning with learned layerwise metric and subspace          ê·¸ëž˜ë””ì–¸íŠ¸ í¬ì¸íŠ¸ë“¤ì´ ë§ˆêµ¬ìž¡ì´ë¡œ ê°ˆ ìˆ˜ ìžˆì§€ë§Œ, optimum í¬ì¸íŠ¸ë“¤ì´ ë¶„í¬í•´ìžˆëŠ” subspaceê°€ ìžˆì„ ìˆ˜ ìžˆìŒ.      subspace ì•ˆì—ì„œë§Œ ê·¸ëž˜ë””ì–¸íŠ¸ë¥¼ ê°€ë©´ í›¨ì”¬ ë” ë¹¨ë¦¬ ê°ˆ ìˆ˜ ìžˆìŒ. ê·¸ê²ƒ ê´€ë ¨í•œ ë…¼ë¬¸        ICML 2019 : Online Meta-Learning (ê°™ì€ ì €ìž!)References(References for your additional studies)https://www.youtube.com/watch?v=fxJXXKZb-ikhttps://talkingaboutme.tistory.com/entry/DL-Meta-Learning-Learning-to-Learn-Fasthttps://elapser.github.io/machine-learning/2019/03/08/Model-Agnostic-Interpretation.html]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Meta Learning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Attention Is All You Need]]></title>
      <url>/paper%20review/2020/08/10/Attention/</url>
      <content type="text"><![CDATA[Attention Is All You Need ë¥¼ ì½ê³  ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.Attention Is All You NeedOverview(You should include contents of summary and introduction.)We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.  recurrent, convolution ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  Attentionì„ ì´ìš©í•´ì„œ ë” ë¹ ë¥¸ ì„±ëŠ¥ì— ë„ë‹¬Related work (Basic concepts)  layer normalization  RNN  AttentionMethods(Explain one of the methods that the thesis used.)Architecture      the encoder maps an input sequence of symbol representations (x1,â€¦,xn) to a sequence of continuous representations z = (z1,â€¦,zn)    Encoder          6ê°œì˜ identicla layerë¡œ êµ¬ì„±ë˜ì–´ ìžˆìŒ.                  each layer has 2 sub-layers.                          The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network.                                          Add : residual connectionì„ ì˜ë¯¸í•¨.      Layer Norm. : LayerNorm(x + Sublayer(x))      produce outputs of dimension d_model = 512        Decoder          Encoderì™€ ë™ì¼í•˜ê²Œ 6ê°œì˜ ë™ì¼í•œ ë ˆì´ì–´ë¡œ êµ¬ì„±ë¨.      Masked Multi-Head Attention                  We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.                    Attention  Scaled Dot-Product Attention          Input : Q(Queries), K(Keys), V(Values)      compute the dot products query with all keys, divide each by root(dk), and apply a softmax function to obtain the weights on the values.                We implement this inside of scaled dot-product attention by masking out (setting to âˆ’âˆž) all values in the input of the softmax which correspond to illegal connections.        Multi-Head Attention          linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively.              perform the attention function in parallel yielding dv -dimensional output values.      ìœ„ì˜ ê³¼ì •ì´ ì™„ë£Œëœ í›„ì—ëŠ” concatenate ì‹œí‚´        Position-wise Feed-Forward NetworksPositional EncodingTransformerì—ì„œëŠ” ì‹œê°„ì˜ ì—°ì†ì„±ì„ ëª¨ë¸ì˜ í•µì‹¬ë¶€ì—ì„œ ë‹¤ë£¨ì§€ ì•ŠìŒ. -&gt; ê·¸ëŸ¬ë‚˜ ì‹œê°„ì˜ ìˆœì„œëŠ” ì‹¤ì œ ì–¸ì–´ì—ì„œ ì¤‘ìš”í•˜ë¯€ë¡œ ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ í¬í•¨ì‹œí‚¤ê¸° ìœ„í•´ Positional Encodingì„ ì‚¬ìš©Why Self-Attention      Total computational complexity per layer        The amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.        The path length between long-range dependencies in the network.  A self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.Trainingë…¼ë¬¸ ì°¸ì¡°.Additional studies(If you have some parts that cannot understand, you have to do additional studies for them. Itâ€™s optional.)[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.[2] BERTReferences(References for your additional studies)https://www.youtube.com/watch?v=EyXehqvkfF0https://www.youtube.com/watch?v=mxGCEWOxfe8]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> NLP </tag>
        
          <tag> Transformer </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Generative Adversarial Nets]]></title>
      <url>/paper%20review/2020/08/08/GAN/</url>
      <content type="text"><![CDATA[Generative Adversarial Nets ë¥¼ ì½ê³  ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.Generative Adversarial NetsOverview(You should include contents of summary and introduction.)GANì—ëŠ” ë‘ ê°€ì§€ ëª¨ë¸ì´ ì¡´ìž¬í•¨.  Discriminator  GeneratorImageë¥¼ ë§Œë“¤ì–´ ë‚´ëŠ” Generator(G)ê°€ ì´ ë§Œë“¤ì–´ì§„ ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” Discriminator(D)ë¥¼ ìµœëŒ€í•œ ì†ì¼ ìˆ˜ ìžˆë„ë¡, í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ ëª©ì   ì¦‰, GëŠ” Dë¥¼ ìµœëŒ€í•œ ì†ì´ë ¤ê³  ë…¸ë ¥í•˜ê³ , DëŠ” Gê°€ ë§Œë“  ì´ë¯¸ì§€ë¥¼ ìµœëŒ€í•œ ê°ë³„í•˜ë ¤ê³  ë…¸ë ¥í•¨.  ì´ ê²½ìŸ ì†ì—ì„œ ë‘ ëª¨ë¸ì€ ëª¨ë‘ ë°œì „í•˜ê²Œ ë˜ê³ , ê²°ê³¼ì ìœ¼ë¡œëŠ” Gê°€ ë§Œë“  ì´ë¯¸ì§€ë¥¼ êµ¬ë³„í•  ìˆ˜ ì—†ëŠ” ìƒíƒœì— ë„ë‹¬í•˜ê²Œ ë¨.ìœ„ì˜ ëª©í‘œë¥¼ ì´ë£¨ê¸° ìœ„í•´ì„œëŠ”, (ref. output -&gt; [0,1] : 0==false, 1==true)  D : ì§„ì§œ ì´ë¯¸ì§€ë¥¼ ì§„ì§œ ì´ë¯¸ì§€ë¼ê³  ì¸ì‹(ë¶„ë¥˜)í•˜ë„ë¡ í•™ìŠµ  G : randomí•œ ì½”ë“œë¥¼ ë°›ì•„ì„œ imgë¥¼ ìƒì„±í•œ í›„, ê·¸ ì´ë¯¸ì§€ê°€ Dë¥¼ ì†ì—¬ì•¼ í•¨.          ì¦‰, D(G(z)) = 1(ì§„ì§œë¼ ì¸ì‹)ì´ ë‚˜ì˜¤ë„ë¡ í•™ìŠµ.                  í•™ìŠµí• ìˆ˜ë¡ ì§„ì§œê°™ì€ ê°€ì§œ imgê°€ ë§Œë“¤ì–´ì§€ëŠ” ê²ƒ                    Related work (Basic concepts)  generative model  Adversarial  VAEMethods(Explain one of the methods that the thesis used.)GAN loss/objective function  D ìž…ìž¥ì—ì„œëŠ” ìœ„ ìˆ˜ì‹ì´ 0ì¸ê²Œ Maximize  G ìž…ìž¥ì—ì„œëŠ” ì†ì´ëŠ” ê²Œ ì¢‹ìœ¼ë‹ˆ Mininmize+) GëŠ” ì²˜ìŒì— í˜•íŽ¸ì—†ëŠ” ì´ë¯¸ì§€ë¥¼ ë§Œë“¦.  DëŠ” ê·¸ ì´ë¯¸ì§€ë¥¼ ê°€ì§œë¼ í™•ì‹ . =&gt; D(G(z))=0  í•˜ì§€ë§Œ ìœ„ì˜ log(1-x) ë¡œëŠ” ê·¸ë•Œ ê¸°ìš¸ê¸°ì˜ ì ˆëŒ“ê°’ì´ ìž‘ìŒ.  practical use : Dê°€ ê°€ì§œë¼ í™•ì‹ í•˜ëŠ” ìƒí™©ì„ ìµœëŒ€í•œ ë¹¨ë¦¬ ë²—ì–´ë‚˜ë ¤ë©´, D(G(z))=0ì¸ ì ì—ì„œ ê¸°ìš¸ê¸°ê°€ ê±°ì˜ ë¬´í•œì¸ log(x)ë¥¼ ì”€  ëª¨ë¸ì´ ìƒì„±í•œ ì´ë¯¸ì§€ ë¶„í¬ì™€ ì‹¤ì œ ì´ë¯¸ì§€ ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•´ì£¼ëŠ” í•¨ìˆ˜ë¡œ Jenson-Shannon divergence ì‚¬ìš©í•¨.Approach  The minimax problem of GAN has a global opt. at p(g) = p(data)  Proposition 1.  Main Theorem.ìœ„ë¥¼ ì´ìš©í•´ì„œ Dê°€ optimal ê°€ì •.The global minimum of the virtual training criterion C(G) is achieved if and only if p(g)=p(data). At that point, C(G) achieves the value âˆ’log(4).  The proposed algorithm can find that global opt.  ê·¸ëž˜ì„œ ì•Œê³ ë¦¬ì¦˜ì´ ìœ„ ë¬¸ì œë¥¼ í’€ ìˆ˜ ìžˆëŠ”ê°€ë¥¼ í™•ì¸1ë²ˆ==&gt;minimax problem -&gt; global opt. ê°€ì§„ë‹¤ëŠ” ì¦ëª…ì´ì—ˆìŒ.  global opt. -&gt; ëª¨ë¸ì˜ ë¶„í¬ == ì‹¤ì œ ë¶„í¬  ì¦‰ ìš°ë¦¬ê°€ í’€ë ¤ëŠ” ë¬¸ì œ C(G)ê°€ convexë¬¸ì œìž„ì„ í™•ì¸í–ˆìŒ.          minimization problemì´ ì‰¬ì›Œì§.      MLPë¡œ ì¶©ë¶„ížˆ ê°€ëŠ¥í•˜ë‹¤.Vector arithmetic í•˜ë‹¤.ì•ˆê²½ ë‚€ ë‚¨ìž - ì•ˆê²½ ì•ˆ ë‚€ ë‚¨ìž + ì•ˆê²½ ì•ˆ ë‚€ ì—¬ìž = ì•ˆê²½ ë‚€ ì—¬ìžCode# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/generative_adversarial_network/main.pyimport osimport torchimport torchvisionimport torch.nn as nnfrom torchvision import transformsfrom torchvision.utils import save_image# Device configurationdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')# Hyper-parameterslatent_size = 64hidden_size = 256image_size = 784num_epochs = 200batch_size = 100sample_dir = 'samples'# Create a directory if not existsif not os.path.exists(sample_dir):    os.makedirs(sample_dir)# Image processing# transform = transforms.Compose([#                 transforms.ToTensor(),#                 transforms.Normalize(mean=(0.5, 0.5, 0.5),   # 3 for RGB channels#                                      std=(0.5, 0.5, 0.5))])transform = transforms.Compose([                transforms.ToTensor(),                transforms.Normalize(mean=[0.5],   # 1 for greyscale channels                                     std=[0.5])])# MNIST datasetmnist = torchvision.datasets.MNIST(root='../../data/',                                   train=True,                                   transform=transform,                                   download=True)# Data loaderdata_loader = torch.utils.data.DataLoader(dataset=mnist,                                          batch_size=batch_size,                                           shuffle=True)# DiscriminatorD = nn.Sequential(    nn.Linear(image_size, hidden_size),    nn.LeakyReLU(0.2),    nn.Linear(hidden_size, hidden_size),    nn.LeakyReLU(0.2),    nn.Linear(hidden_size, 1),    nn.Sigmoid())# Generator G = nn.Sequential(    nn.Linear(latent_size, hidden_size),    nn.ReLU(),    nn.Linear(hidden_size, hidden_size),    nn.ReLU(),    nn.Linear(hidden_size, image_size),    nn.Tanh())# Device settingD = D.to(device)G = G.to(device)# Binary cross entropy loss and optimizercriterion = nn.BCELoss()d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)def denorm(x):    out = (x + 1) / 2    return out.clamp(0, 1)def reset_grad():    d_optimizer.zero_grad()    g_optimizer.zero_grad()# Start trainingtotal_step = len(data_loader)for epoch in range(num_epochs):    for i, (images, _) in enumerate(data_loader):        images = images.reshape(batch_size, -1).to(device)                # Create the labels which are later used as input for the BCE loss        real_labels = torch.ones(batch_size, 1).to(device)        fake_labels = torch.zeros(batch_size, 1).to(device)        # ================================================================== #        #                      Train the discriminator                       #        # ================================================================== #        # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))        # Second term of the loss is always zero since real_labels == 1        outputs = D(images)        d_loss_real = criterion(outputs, real_labels)        real_score = outputs                # Compute BCELoss using fake images        # First term of the loss is always zero since fake_labels == 0        z = torch.randn(batch_size, latent_size).to(device)        fake_images = G(z)        outputs = D(fake_images)        d_loss_fake = criterion(outputs, fake_labels)        fake_score = outputs                # Backprop and optimize        d_loss = d_loss_real + d_loss_fake        reset_grad()        d_loss.backward()        d_optimizer.step()                # ================================================================== #        #                        Train the generator                         #        # ================================================================== #        # Compute loss with fake images        z = torch.randn(batch_size, latent_size).to(device)        fake_images = G(z)        outputs = D(fake_images)                # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf        g_loss = criterion(outputs, real_labels)                # Backprop and optimize        reset_grad()        g_loss.backward()        g_optimizer.step()                if (i+1) % 200 == 0:            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}'                   .format(epoch, num_epochs, i+1, total_step, d_loss.item(), g_loss.item(),                           real_score.mean().item(), fake_score.mean().item()))        # Save real images    if (epoch+1) == 1:        images = images.reshape(images.size(0), 1, 28, 28)        save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'))        # Save sampled images    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)    save_image(denorm(fake_images), os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1)))# Save the model checkpoints torch.save(G.state_dict(), 'G.ckpt')torch.save(D.state_dict(), 'D.ckpt')Additional studies(If you have some parts that cannot understand, you have to do additional studies for them. Itâ€™s optional.)ì´í›„ GAN ë…¼ë¬¸ë“¤References(References for your additional studies)https://www.youtube.com/watch?v=L3hz57whyNwhttps://www.youtube.com/watch?v=odpjk7_tGY0http://jaejunyoo.blogspot.com/2017/01/generative-adversarial-nets-2.html]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> GAN </tag>
        
          <tag> Generative Model </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[PEGASUS:Pre-training with Extracted Gap-sentences for Abstractive Summarization]]></title>
      <url>/paper%20review/2020/08/05/PEGASUS/</url>
      <content type="text"><![CDATA[PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization ë¥¼ ì½ê³  ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive SummarizationOverview  What is the good pre-training objectives tailored for abstractive text summarization?â€“&gt; GSG  self- supervised objective  important sentences are removed/masked from an input doc- ument and are generated together as one output sequence from the remaining sentences, similar to an extractive summary.          ì¤‘ìš”í•œ ë¬¸ìž¥ì€ ì¸í’‹ ê³¼ì •ì—ì„œ maskedë˜ê³ , ë‚¨ì€ ë¬¸ìž¥ë“¤ ì¤‘ì—ì„œ extractive summaryì²˜ëŸ¼ ê°€ì ¸ì˜¤ëŠ” ê±´ê°€ ë´„.        scoring : ROUGE  GSG, MLMRelated Work (Basic Concepts)  ë…¼ë¬¸ ì°¸ì¡°MethodsArchitectureGap Sentences Generation(GSG)  our proposed pre-training objec- tive involves generating summary-like text from an input document. In order to leverage massive text corpora for pre- training, we design a sequence-to-sequence self-supervised objective in the absence of abstactive summaries.  we select and mask whole sentences from documents, and concatenate the gap-sentences into a pseudo-summary. The corresponding position of each selected gap sentence is replaced by a mask token [MASK1] to inform the model. Gap sentences ratio, or GSR, refers to the number of selected gap sentences to the total number of sentences in the document, which is similar to mask rate in other works.  ì›Œë“œë¥¼ ë§ˆìŠ¤í‚¹í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼ ì„¼í…ìŠ¤ ë‹¨ìœ„ë¡œ ë§ˆìŠ¤í‚¹          ì´ê²ƒì„ íŠ¸ëžœìŠ¤í¬ë¨¸ ëª¨ë¸ì— ë„£ê³  ë§ˆìŠ¤í‚¹í•œ ë¬¸ìž¥ì„ ì¶”ë¡ í•˜ëŠ” íƒœìŠ¤í¬ ìˆ˜í–‰        ì–´ë–»ê²Œ í•˜ë©´ ì¤‘ìš”í•œ ì„¼í…ìŠ¤ë¥¼ ì„ íƒí•´ì„œ ë§ˆìŠ¤í‚¹í•  ìˆ˜ ìžˆëŠ”ê°€?          ROUGE1-F1 score ì‚¬ìš©      ROUGE  ROUGE + F1 score  ì •ë‹µ ì„¼í…ìŠ¤ê°€ ìžˆë‹¤ê³  ê°€ì •í•˜ëŠ” ê²ƒ          pretrainingì— í™œìš©í•˜ê³  ì‹¶ì€ ê²ƒì´ê¸° ë•Œë¬¸ì— ì •ë‹µ ì„¼í…ìŠ¤ê°€ ì—†ìŒ      ì…€ë ‰íŠ¸ ì„¼í…ìŠ¤ì™€ ë‚˜ë¨¸ì§€ ì„¼í…ìŠ¤ë¥¼ ë¹„êµí•´ì„œ ROUGE1-F1 ìŠ¤ì½”ì–´ë¥¼ êµ¬í•¨.        ê¸°ì¡´ì˜ ROUGE ì •ì˜ì™€ëŠ” ì•½ê°„ ë‹¤ë¥´ë‹¤.AlgorithmMasked Language Model(MLM)  Following BERT, we select 15% tokens in the input text, and the selected tokens are (1) 80% of time replaced by a mask token [MASK2], or (2) 10% of time replaced by a random token, or (3) 10% of time unchanged.  ëžœë¤í•˜ê²Œ íŠ¹ì • ë‹¨ì–´ë¥¼ masking ì‹œí‚´.          ì¸í’‹ì„ ë„£ì–´ì¤„ ë•Œ ì´ ë‹¨ì–´ë“¤ì„ ë„£ì–´ì£¼ì§€ ì•ŠëŠ”ë‹¤        ë§ˆìŠ¤í‚¹ëœ ë‹¨ì–´ê°€ ë¬´ì—‡ì¸ì§€ ì¶”ì¸¡í•˜ëŠ” â€¦-&gt; Can get gnereal linguistic knowledge-&gt; labeling, dataë¥¼ ê°€ì§ˆ í•„ìš”ì—†ì´ ëª¨ë“  ë°ì´í„°ë¡œ ì‚¬ìš©í•  ìˆ˜ ìžˆë‹¤.Pre-training each corpus  C4  HugeNewsFine-tuning datasets  XSum  CNN/DailyMail  NEWSROOM  Multi-News  â€¦tfdsì—ì„œ ê°€ì ¸ì˜´.Experiments  Pre-training ablation experiments to choices of pre-training corpus, objective, and vocabulary size Using PEGASUS_Base instead of PEGASUS_Large  Ind-Orig  masking : 30%  Unigram 96K  Larger Model Results          baseëª¨ë¸ë¡œ ì°¾ì€ ë‹¤ìŒì— largeëª¨ë¸ì„ ìƒì„±        Fine-tuning with low-resource          ì ì€ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œë„ ì„±ëŠ¥ì´ ì¢‹ë‹¤        Qualitative Observations          ì‚¬ëžŒì´ ë´¤ì„ ë•Œ ì§„ì§œ ì¢‹ì€ ì„±ëŠ¥ì¸ì§€ í™•ì¸      ì‹¤í—˜ê²°ê³¼ -&gt; ì‹¤ì œë¡œëŠ” GSG ë°©ì‹ë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¢‹ë‹¤.]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> NLP </tag>
        
          <tag> Text Summarization </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[You Only Look Once(YOLO):Unified, Real-Time Object Detection]]></title>
      <url>/paper%20review/2020/07/31/YOLO/</url>
      <content type="text"><![CDATA[You Only Look Once : Unified, Real-Time Object Detection ë¥¼ ì½ê³  ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.You Only Look Once : Unified, Real-Time Object DetectionOverview(You should include contents of summary and introduction.)ê¸°ì¡´ì˜ object detection = repurpose classifiers to perform detection  DPM : sliding window  R-CNN : region proposal  Classifierë¥¼ í†µí•´ í´ëž˜ìŠ¤ ë¶„ë¥˜ -&gt; post-processing í†µí•´ refine -&gt; rescore í›„ í•©ì¹¨YOLO : a single network, end-to-end directly on detection performance  Unified Model (feat. GoogLeNet, NIN)  Real-time ê°€ëŠ¥í•  ì •ë„ë¡œ ë¹ ë¦„.  Single regression problem          Reasons globally about the image when making predictions      bbox coord. &amp; class probability ë™ì‹œì— êµ¬í•  ìˆ˜ ìžˆìŒ.      CNN -&gt; Non-max. supprestion -&gt; Finish!        Can learns generalizable representations of obj.  ë‹¤ì–‘í•œ datasets ê°€ëŠ¥í•˜ë‹¤.Related work (Basic concepts)GoogLeNet architectureNIN(Network in Network)  ë„¤íŠ¸ì›Œí¬ë¥¼ êµ¬ì„±í•  ë•Œ ë˜ ë‹¤ë¥¸ micro network í¬í•¨í•˜ì—¬ ì„¤ê³„í–ˆì„ ë•Œ ì„±ëŠ¥ í–¥ìƒì´ ë¨.Methods(Explain one of the methods that the thesis used.)Unified Detection  ëª¨ë“  bbox, classë¥¼ í•œë²ˆì— ê³ ë ¤í•œë‹¤ (=&gt; ì¦‰, ì „ì²´ ì´ë¯¸ì§€ë¡œ í•œë²ˆ ë³¸ë‹¤)1. S x S grid2. bbox &amp; confidence score(bboxì—ì„œ ë¬¼ì²´ê°€ ë§Žì´ í¬í•¨ë˜ëŠ”ì§€)    - confidence = Pr(object) * IOU    - no obj -&gt; 0(zero)    - each bbox contains : x,y,w,h,confidence    - x, y : bboxì•ˆì˜ cell ìœ„ì¹˜ (norm. 0~1)    - w, h : bbox width, height (norm. 0~1)    - c : bbox confidence3. Class probability  \[Pr(class(i)|Object) * Pr(Object)\]    Predictions are encoded as an S x S x (B*5 + C)  On Pascal Voc, S=7, B=2. C(class) = 20      - Thus, 7 x 7 x 30 tensor.Architecture  GoogLeNetì„ ì‘ìš©  24 conv. layer + 2 FC layer          Fast YOLO(tiny YOLO) : 9 conv layer ì‚¬ìš©í•¨.        GoogLeNetì—ì„œ ì“°ì´ëŠ” Inception module ëŒ€ì‹ , Reduction layer(1x1 conv)ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„° size ì¤„ìž„.  Feature Extractorì— í•´ë‹¹í•˜ëŠ” 20 conv layerë¡œ pretrainí•¨.          Pretrained layer ë°”íƒ•ìœ¼ë¡œ VOC dataì— fine-tuning        ì¢€ ë” ì¢‹ì€ detectionì„ ìœ„í•´ 224 -&gt; 448ë¡œ size 2ë°° ëŠ˜ë¦¼.  NIN(network in network) - 4 conv layer, 2 FC layer =&gt; classifier ì—­í• Loss function  MSEë³´ë‹¤ SSEê°€ ë‹¨ìˆœí•˜ë‹ˆ SSEë¡œ ì„ íƒ  Objectì˜ ìœ ë¬´ëŠ” grid cell ìžì²´ì—ì„œ cell ê¸°ì¤€ìœ¼ë¡œ classifyí•˜ë¯€ë¡œ ìœ ë¬´ë¥¼ íŒë‹¨í•˜ëŠ” lossë³´ë‹¤ëŠ” bbox ì¢Œí‘œë¥¼ ì°ëŠ” lossë¥¼ ë” í¬ê²Œ ë´„.          lambda(coord) = 5 and lambda(noobj) = 0.5        bbox ì¤‘ì—ì„œë„ larger boxëŠ” ì˜¤ì°¨ë¡œ ì¸í•œ ë¡œìŠ¤ ë³€ë™ì´ í´ ê²ƒì´ë¯€ë¡œ ë£¨íŠ¸ë¥¼ ì”Œì›Œì„œ ìž‘ì€ bboxì—ëŠ” ì „ë³´ë‹¤ ë” í¬ê²Œ ë°˜ì‘, í° bboxì—ëŠ” ì „ë³´ë‹¤ ì¡°ê¸ˆ ë” ìž‘ê²Œ ë°˜ì‘í•˜ë„ë¡ í•¨.  1(obj,i) : if object appears in cell i  1(obj,ij) : the j-th bbox predictor in cell i is â€œresponsibleâ€ for that predictionLimitations  Bboxê°€ grid sizeì— ì˜ì¡´.          Imageê°€ domainì— ìžì£¼ ë‚˜íƒ€ë‚˜ëŠ” object sizeë¥¼ ì œëŒ€ë¡œ ë°˜ì˜í•˜ì§€ ëª»í•œë‹¤ë©´ bad        Grid sizeë³´ë‹¤ ìž‘ì€ ë¬¼ì²´ê°€ ìžˆì„ ê²½ìš° badExperiment rusults      bg ê´€ë ¨í•´ì„œ ëœ ì‹¤ìˆ˜í•¨.        Fast R-CNNê³¼ combineí–ˆë”ë‹ˆ ì†ë„ ë–¨ì–´ì§    ìƒˆë¡œìš´ datasetì—ë„ ìž˜ ìž‘ë™í•¨.Code      pytorch        network implementation      # feature extractor ë¶€ë¶„       feature_extract_net = nn.Sequential(          nn.Conv2d(3, 64, 7, stride=2, padding=3),          nn.LeakyReLU(0.1, inplace=True),          nn.MaxPool2d(2),          nn.Conv2d(64, 192, 3, padding=1),          nn.LeakyReLU(0.1, inplace=True),          nn.MaxPool2d(2),          nn.Conv2d(192, 128, 1),          nn.LeakyReLU(0.1, inplace=True),          nn.Conv2d(128, 256, 3, padding=1),          nn.LeakyReLU(0.1, inplace=True),          nn.Conv2d(256, 256, 1),          nn.LeakyReLU(0.1, inplace=True),          nn.Conv2d(256, 512, 3, padding=1),          nn.LeakyReLU(0.1, inplace=True),          nn.MaxPool2d(2),          nn.Conv2d(512, 256, 1),          nn.LeakyReLU(0.1, inplace=True),          nn.Conv2d(256, 512, 3, padding=1),          nn.LeakyReLU(0.1, inplace=True),          nn.Conv2d(512, 256, 1),          nn.LeakyReLU(0.1, inplace=True),          nn.Conv2d(256, 512, 3, padding=1),          nn.LeakyReLU(0.1, inplace=True),          nn.Conv2d(512, 256, 1),          nn.LeakyReLU(0.1, inplace=True),          nn.Conv2d(256, 512, 3, padding=1),          nn.LeakyReLU(0.1, inplace=True),          nn.Conv2d(512, 256, 1),          nn.LeakyReLU(0.1, inplace=True),          nn.Conv2d(256, 512, 3, padding=1),          nn.LeakyReLU(0.1, inplace=True),          nn.Conv2d(512, 512, 1),          nn.LeakyReLU(0.1, inplace=True),          nn.Conv2d(512, 1024, 3, padding=1),          nn.LeakyReLU(0.1, inplace=True),          nn.MaxPool2d(2),          nn.Conv2d(1024, 512, 1),          nn.LeakyReLU(0.1, inplace=True),          nn.Conv2d(512, 1024, 3, padding=1),          nn.LeakyReLU(0.1, inplace=True),          nn.Conv2d(1024, 512, 1),          nn.LeakyReLU(0.1, inplace=True),          nn.Conv2d(512, 1024, 3, padding=1),          nn.LeakyReLU(0.1, inplace=True)      )          # classifier ë¶€ë¶„  conv = nn.Sequential(              # 4 conv layer              nn.Conv2d(1024, 1024, 3, padding=1),              nn.LeakyReLU(0.1, inplace=True),              nn.Conv2d(1024, 1024, 3, stride=2, padding=1),              nn.LeakyReLU(0.1),              nn.Conv2d(1024, 1024, 3, padding=1),              nn.LeakyReLU(0.1, inplace=True),              nn.Conv2d(1024, 1024, 3, padding=1),              nn.LeakyReLU(0.1, inplace=True)              Flatten(),              # 2 FC layer              nn.Linear(7 * 7 * 1024, 4096),              nn.LeakyReLU(0.1, inplace=True),              nn.Dropout(0.5, inplace=False),               nn.Linear(4096, S * S * (5 * B + C)),              nn.Sigmoid()      )      Additional studies(If you have some parts that cannot understand, you have to do additional studies for them. Itâ€™s optional.)NIN(network in network)References(References for your additional studies)https://www.youtube.com/watch?v=eTDcoeqj1_whttps://arclab.tistory.com/162https://arclab.tistory.com/165]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> Object Detection </tag>
        
          <tag> Real-Time </tag>
        
          <tag> YOLO </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Text Summarization with Pretrained Encoders]]></title>
      <url>/paper%20review/2020/07/26/Text-Summarization-with-Pretrained-Encoders/</url>
      <content type="text"><![CDATA[Text Summarization with Pretrained Encoders ë¥¼ ì½ê³  ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.Text Summarization with Pretrained Encodersì´ ì €ìžê°€ ì°¸ì—¬í•œ ì „ ë…¼ë¬¸ì´ Fine-tune BERT for Extractive Summarizationì˜€ìŒ.  ë”°ë¼ì„œ ë°œì „ëœ ë¶€ë¶„ì— ëŒ€í•´ ì‚´íŽ´ë³¼ ê²ƒìž„.ì´ì „ ì—°êµ¬ì™€ì˜ ì°¨ì´ì  (Differences from previous studies)  ì „ì—ëŠ” extractive summarizationì— ëŒ€í•´ì„œë§Œ ì‚´íŽ´ë´¤ì—ˆìŒ.  í˜„ ì—°êµ¬ì—ì„œëŠ” a general framework for both extractive and abstractive models (ë‘ ì„œë¨¸ë¦¬ ëª¨ë¸ì„ ì‚¬ìš©í•¨)Data  CNN/Dailymail : highlightedëœâ€¦ -&gt; extractive summaries  NYT : abstractive summaries  XSum : one-sentence summaryMethod  extractive model          built on top of the encoder by stacking several inter-sentence Transformer layers        abstractive model          a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismathch between two (the former is pretrained while the latter is not)        a two-staged fine-tuning approach          it can further boost the quality of the generated summaries      the combination of extractive and abstractive objectives can help generate better summaries (Gehrmann et al., 2018)      ì¦‰ ë‘ ê°€ì§€ ëª¨ë¸ì„ ì»´ë°”ì¸ì‹œí‚¤ë©´ ì„±ëŠ¥ í–¥ìƒì„ ë…¸ë¦´ ìˆ˜ ìžˆë‹¤ëŠ” ì•„ì´ë””ì–´ì—ì„œ ì°©ì•ˆí•œ ì—°êµ¬.        architecture of BERTSUM  Summarization Encoder          Interval Segment Embeddings                  sent(i)ë¥¼ í™€ìˆ˜,ì§ìˆ˜ ìˆœì„œì— ë”°ë¼ E(a) or E(b)ë¡œ segment embedding í•œë‹¤.              sentence -&gt; [sent1,sent2,sent3,sent4,sent5]  embedding -&gt; [E(a),E(b),E(a),E(b),E(a)]                                            Extractive Summarization          ì´ì „ ì—°êµ¬ì™€ ìˆ˜ì‹ ë™ì¼        Abstractive Summarization          the encoder is the pretrained BERTSUM and the decoder is a 6-layered Transformer initialized randomly.                  encoder &amp; decoder ì‚¬ì´ì˜ mismatch ê°€ëŠ¥ì„± ìžˆìŒ                          new fine-tuning schedule!                                          difference optimizer ì ìš©!        two-stage fine-tuning    1. fine-tune the encoder on the extractive summarization2. fine-tune it on the abstractive summarization              using extractive objectives can boost the performance of abstractive summarization.      Results      ROUGE score ì‚¬ìš©í•¨        ì „ë³´ë‹¤ ì„±ëŠ¥ë„ ë§Žì´ ì¢‹ì•„ì§.  ê´€ë ¨ ë…¼ë¬¸ì ‘ê¸°/íŽ¼ì¹˜ê¸° ë²„íŠ¼          Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. 2018. Bottom-up abstractive summarization. In Proceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing, pages 4098â€“4109, Brussels, Belgium.      Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74â€“81, Barcelona, Spain.      Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018a. Donâ€™t give me the details, just the summary! topic-aware convolutional neural networks for ex- treme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1797â€“1807, Brussels, Bel- gium.      Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018b. Ranking sentences for extractive summa- rization with reinforcement learning. In Proceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long Papers), pages 1747â€“1759, New Orleans, Louisiana.      Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sen- tence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan- guage Processing, pages 379â€“389, Lisbon, Portugal.      Xingxing Zhang, Furu Wei, and Ming Zhou. 2019. HI- BERT: Document level pre-training of hierarchical bidirectional transformers for document summariza- tion. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5059â€“5069, Florence, Italy. Association for Computational Linguistics.      Xingxing Zhang, Mirella Lapata, Furu Wei, and Ming Zhou. 2018. Neural latent extractive document sum- marization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Pro- cessing, pages 779â€“784, Brussels, Belgium.      ]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> NLP </tag>
        
          <tag> Text Summarization </tag>
        
          <tag> Encoder </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Fine-tune BERT for Extractive Summarization]]></title>
      <url>/paper%20review/2020/07/25/Fine-tune-BERT-for-Extractive-Summarization/</url>
      <content type="text"><![CDATA[Fine-tune BERT for Extractive Summarization ë¥¼ ì½ê³  ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.Fine-tune BERT for Extractive SummarizationSummarizationì˜ ì¢…ë¥˜  abstractive summarization : contains words or phrases that were not in the original textâ€¦          paraphrasingì™€ ìœ ì‚¬        extractive summarization : by copying and concatenating the most important spans in a document          ì¤‘ìš”í•œ ë¬¸ìž¥ì„ copy &amp; paste í•˜ëŠ” ê²ƒìœ¼ë¡œ ì„œë¨¸ë¦¬ë¥¼ ë§Œë“¦.        ì´ ë…¼ë¬¸ì—ì„œëŠ” í›„ìžì¸ extractive summarizationì„ ì‚¬ìš©Data  CNN/Dailymail  NYTMethodEncoding Multiple Sentences  insert a [CLS] token before each sentence and a [SEP] token after each sentence          In vanilla BERT, [CLS] is used as a symbol to aggregate features from one sentence or a pair of sentences.      Interval Segment Embeddings  sent(i)ë¥¼ í™€ìˆ˜,ì§ìˆ˜ ìˆœì„œì— ë”°ë¼ E(a) or E(b)ë¡œ segment embedding í•œë‹¤.sentence -&gt; [sent1,sent2,sent3,sent4,sent5]embedding -&gt; [E(a),E(b),E(a),E(b),E(a)]Fine-tuning with Summarization Layers  simple classifier : sigmoid function  inter-sentence transformer          extracting document-level features focusing on summarization tasks from the BERT outputs      ê³µì‹ì— layer normalizationê³¼ multi-head attention operationë¥¼ ì´ìš©í•œë‹¤ í•˜ëŠ”ë° MHAttëŠ” ë” ì•Œì•„ë´ì•¼í•  ë“¯í•¨.        Recurrent Neural Network          RNNì´ ë” ì¢‹ê²Œ ë§Œë“¤ ìˆ˜ ìžˆìŒ(ì™œì§€)      BERT outputì— LSTM ë ˆì´ëŸ¬ë¥¼ ì ìš©ì‹œì¼œì„œ summarization-specific featuresë¥¼ í•™ìŠµí•  ìˆ˜ ìžˆë„ë¡ í•¨.      Results  ROUGE score ì‚¬ìš©í•¨]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> NLP </tag>
        
          <tag> Text Summarization </tag>
        
          <tag> BERT </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Jacobian Matrix]]></title>
      <url>/math/2020/07/24/Jacobian-Matrix/</url>
      <content type="text"><![CDATA[Jacobian Matrix ì— ê´€í•œ ì„¤ëª…ì„ ì ì€ ê¸€ìž…ë‹ˆë‹¤.Jacobian Matrixìš”ì•½  íŽ¸ë¯¸ë¶„ì„ í•  ë•Œ ìœ ìš©í•œ í–‰ë ¬  íŽ¸ë¯¸ë¶„ í•  ë³€ìˆ˜ë“¤ì´ ë§Žê³ , ê·¸ ë³€ìˆ˜ë“¤ë¡œ ì´ë£¨ì–´ì ¸ ìžˆëŠ” í•¨ìˆ˜ê°€ ë§Žì„ ë•Œ ë‹¨ìˆœížˆ ê³±í•´ì„œ ë”í•˜ëŠ” í¼ìœ¼ë¡œ ë§Œë“¤ì–´ ë†“ì„ ìˆ˜ ìžˆëŠ” ê²ƒì„¤ëª…nê°œì˜ ë³€ìˆ˜ë¥¼ ê°€ì§„ í•¨ìˆ˜ê°€ mê°œ ìžˆì„ ë•Œì´ê²ƒë“¤ì„ ëª¨ë‘ íŽ¸ë¯¸ë¶„í•˜ê¸° ìœ„í•œ ë²¡í„°-&gt; Jacobian matrix  mê°œì˜ í•¨ìˆ˜ íŽ¸ë¯¸ë¶„ì„ ëª¨ì¡°ë¦¬ êµ¬í•  ìˆ˜ ìžˆìŒ.ì˜ˆì‹œì¼ë•Œ, Jacobian MatrixëŠ”Reference02. íŽ¸ë¯¸ë¶„ì„ ê°„ë‹¨í•˜ê²Œ! Jacobian Matrixwikipedia]]></content>
      <categories>
        
          <category> Math </category>
        
      </categories>
      <tags>
        
          <tag> Math </tag>
        
          <tag> Linear Algebra </tag>
        
          <tag> Calculas </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks]]></title>
      <url>/paper%20review/2020/07/23/Faster-R-CNN/</url>
      <content type="text"><![CDATA[Faster R-CNN : Towards Real-Time Object Detection with Region Proposal Networks ë¥¼ ì½ê³  ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.Faster R-CNN : Towards Real-Time Object Detection with Region Proposal NetworksOverview(You should include contents of summary and introduction.)  íë¦„ : R-CNN -&gt; (SPP-net) -&gt; Fast R-CNN -&gt; Faster R-CNN  R-CNN : Region with CNN features. RoI ë“¤ì„ ë½‘ì•„ë‚´ê³  CNNì— ê°ê° ì§‘ì–´ë„£ìŒ.          Region Proposal â€“ Selective Search : ì–´ë–»ê²Œ ë°”ìš´ë”© ë°•ìŠ¤(bbox)ë¥¼ ë½‘ì•„ë‚´ëŠ”ê°€      Training      Pre-train AlexNet      SVM, bounding-box regressor (CNNì— í•™ìŠµì´ ì•ˆëœë‹¤ëŠ” ë‹¨ì ì´ ìžˆìŒ)        Fast R-CNN          RoI projection -&gt; ë§¤ bounding boxë§ˆë‹¤ RoI pooling      ì–´ë–¤ RoIê°€ ë‚˜ì™€ë„ ë˜‘ê°™ì€ sizeê°€ ë‚˜ì˜¤ë„ë¡ max pooling =&gt; RoI pooling      Fixed-length feature vector from RoIê°€ ë¨.      FC ë ˆì´ì–´ì— ë„˜ê¸°ë©´ì„œ classification(K+1 class) + bounding box location ë™ì‹œì— ê³„ì‚°      Problems of Fast R-CNN : Out-of-network region proposals are the test-time computational bottleneck        Faster R-CNN          Notion : Region Proposalì„ Selective Search(CPUì—ì„œ í–ˆìŒ)ì„ í•˜ì§€ ë§ê³  ì‹¤ì œ ë„¤íŠ¸ì›Œí¬ ì•ˆì—ì„œ ê°™ì´ í•´ë³´ìž(GPUë¡œ ê³„ì‚° ê°€ëŠ¥)              Key Point : Region Proposal Network(RPN) + Fast R-CNN                    CNNì„ shareí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ, ì¦‰ ë„¤íŠ¸ì›Œí¬ê°€ í•˜ë‚˜ì¸ ê²ƒì²˜ëŸ¼ í•´ë³´ìž!                            RPN                      Loss function (ì‚¬ì§„)                    4-step Alternation Training      Summary of experiments(Explain figures briefly.)Proposal time ë§¤ìš° ì¤„ì–´ë“¦!Methods(Explain one of the methods that the thesis used.)Region Proposalì„ ì‹¤ì œ ë„¤íŠ¸ì›Œí¬ ì•ˆì—ì„œ ê°™ì´ í•´ë³´ìžRegion Proposal Network + Fast R-CNN CNN shareí•˜ìžë“± Overview ì—ì„œ ì„¤ëª…í•¨Ideas for further research(Explain your ideas for further research briefly.)Faster R-CNNì˜ í•œê³„ : RoI pooling í•  ë•Œ 7ì˜ ë°°ìˆ˜ê°€ ì•„ë‹Œ RoIê°€ ë‚˜ì˜¤ë©´ ë²„ë¦¼ì´ ìžˆì„ ê²ƒ  ì˜¤ì°¨ ë°œìƒâ€¦ (object detectionì—ëŠ” í° ë¬¸ì œê°€ ì•„ë‹ ìˆ˜ ìžˆì§€ë§Œ location ë“±ì´ ì¤‘ìš”í•œ ê²ƒì—ëŠ” ë¬¸ì œë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìžˆìŒ)  Mask R-CNN ìœ¼ë¡œ ê·¹ë³µì•„ì§ë„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸°ì—ëŠ” ì¡°ê¸ˆ ë¶€ì¡±í•´ ë³´ì´ê¸°ë„â€¦  Proposal ì‹œê°„ì€ ìœ ì˜ë¯¸í•˜ê²Œ ì¤„ì´ê¸° íž˜ë“¤ ê²ƒ ê°™ìŒ. Region-wise timeì„ ì¢€ ë” ì¤„ì´ëŠ” ë°©ë²•? Convë„?â€¦Additional studies(If you have some parts that cannot understand, you have to do additional studies for them. Itâ€™s optional.)R-CNNFast R-CNNMask R-CNNReferences(References for your additional studies)https://www.youtube.com/watch?v=kcPAGIgBGRs&amp;list=PLXiK3f5MOQ760xYLb2eWbtOKOwUC-bByj&amp;index=13&amp;t=0shttps://curt-park.github.io/2017-03-17/faster-rcnn/]]></content>
      <categories>
        
          <category> Paper Review </category>
        
      </categories>
      <tags>
        
          <tag> Deep Learning </tag>
        
          <tag> Computer Vision </tag>
        
          <tag> Object Detection </tag>
        
          <tag> Real-Time </tag>
        
          <tag> R-CNN </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[ìƒí™œì½”ë”© WEBn - WEB2 Javascript ì½”ìŠ¤ ì •ë¦¬ë¡]]></title>
      <url>/web/2020/03/06/WEB2/</url>
      <content type="text"><![CDATA[ìƒí™œì½”ë”© WEBnì˜ WEB2 Javascriptì½”ìŠ¤ë¥¼ ë“£ê³  ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.WEB2style  html ì•ˆì˜ body-style ì½”ë“œëŠ” CSSë¡œ ìž‘ì„±.  &lt;h2 style = "color: balck;"&gt;&lt;/h2&gt;          â€œ{CSS code}â€        class = "js" ì¼ ë•Œ.js{    ...}  idë¡œ ì“¸ ë•ŒëŠ” # ê¸°í˜¸ ì‚¬ìš©(id=â€firstâ€)#first{    ...}  ì—¬ê¸° ìžˆëŠ” ëª¨ë“  span tagì— ëŒ€í•´ì„œ ì ìš©ë¨.span{    ...}Javascript  ì‚¬ìš©ìžì™€ ìƒí˜¸ìž‘ìš©í•˜ëŠ” ì–¸ì–´  ì›¹ ë¸Œë¼ìš°ì €ëŠ” í•œ ë²ˆ í™”ë©´ì— ì¶œë ¥ë˜ë©´ ìžê¸° ìžì‹ ì„ ë°”ê¿€ ìˆ˜ ì—†ìŒ.  script íƒœê·¸ ì•ˆì— ìž‘ì„±  ë™ì (dynamic)í•¨. &lt;â€“&gt; html: ì •ì event  onclick = â€œ{Javascript code}â€  onkeydown, onchange ë“±ë“± ë§¤ìš° ë§ŽìŒ.div íƒœê·¸  ì–´ë–¤ ê¸°ëŠ¥ë„ ì˜ë¯¸ë„ ì—†ëŠ” íƒœê·¸.  CSS, JS í†µí•´ ì •ë³´ ë„£ìŒ.  ì¤„ë°”ê¿ˆ ìžë™span íƒœê·¸  ê¸°ëŠ¥ì€ divì™€ ë™ì¼  ì¤„ë°”ê¿ˆì´ ì—†ìŒ.ìžë°”ìŠ¤í¬ë¦½íŠ¸ë¡œ style ì‚¬ìš©í•˜ê¸°  document.querySelector('body').style.backgroundColor="black";ê°ì²´ í•¨ìˆ˜var coworkers{    "programmar":"ys",    "designer":"ys"};    coworkers.showAll = function(){        ...    }    var abc = function(){        ...    }jQuery í•¨ìˆ˜  $.í•¨ìˆ˜ì˜ ì˜ë¯¸  $('a') : ëª¨ë“  a tagì— ëŒ€í•´ ì œì–´í•˜ê² ë‹¤ìžì£¼ ì“°ì´ëŠ” ê°ì²´ë“¤ê³¼ ê²€ìƒ‰ì–´  document  DOM  window  ajax  cookie  offline web application  WebRTC  WebGL]]></content>
      <categories>
        
          <category> Web </category>
        
      </categories>
      <tags>
        
          <tag> Web </tag>
        
          <tag> Javascript </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[ìƒí™œì½”ë”© WEBn - WEB1 Html ì½”ìŠ¤ ì •ë¦¬ë¡]]></title>
      <url>/web/2020/03/05/WEB1/</url>
      <content type="text"><![CDATA[ìƒí™œì½”ë”© WEBnì˜ WEB1 Htmlì½”ìŠ¤ë¥¼ ë“£ê³  ì •ë¦¬í•œ ê¸€ìž…ë‹ˆë‹¤.WEB1Tag  &lt;strong&gt;&lt;/strong&gt; : ê°•ì¡°  &lt;u&gt;&lt;/u&gt; : ë°‘ì¤„  &lt;br&gt; : ë‹¨ìˆœ ì¤„ ë°”ê¿ˆ&lt;p&gt;&lt;/p&gt; : ë‹¨ë½ì˜ ì˜ë¯¸, ì •ë³´ë¡œì¨ ê°€ì¹˜ìžˆëŠ” html. í•˜ì§€ë§Œ ì‹œê°ì ìœ¼ë¡œëŠ” ìžìœ ë„ê°€ ë–¨ì–´ì§.          CSSë¡œ ë³´ì™„      &lt;p style="margin-top:40px"&gt;        &lt;h1&gt;&lt;/h1&gt; ~ &lt;h6&gt;&lt;/h6&gt; : heading  &lt;img stc="sea.jpg" width="100%"&gt;  &lt;li&gt;&lt;/li&gt; : ëª©ë¡          ul : liì˜ ë¶€ëª¨ tagë¡œ, lië¥¼ ê·¸ë£¹í•‘ì‹œì¼œì¤Œ. unordered list      ol : ë§ˆì°¬ê°€ì§€ë¡œ liì˜ ë¶€ëª¨ tag. ordered list        &lt;tr&gt; : í…Œì´ë¸” íƒœê·¸. 3ëŒ€ê°€ ê°™ì´ ë‹¤ë‹˜.  ë¬¸ì„œì˜ êµ¬ì¡° (tag)          title      meta      html      head      body        &lt;a&gt;&lt;/a&gt; : ë§í¬          href, target, title ì˜ ì†ì„±ì„ ì§€ë‹˜.      í†µê³„ì— ê¸°ë°˜í•œ í•™ìŠµ  html ì—ì„œ ë§Žì´ ì“°ì´ëŠ” íƒœê·¸ ëž­í‚¹ì„ ì–´ë–»ê²Œ ë³¼ ìˆ˜ ìžˆì„ê¹Œ?          https://www.advancedwebranking.com/html/      Internet VS WEB  Internet &gt; WEB, FTP, email â€¦  Internet          ì¤‘ì•™ì´ ì—†ìŒ. ë¶„ì‚°ë˜ì–´ ìžˆìŒ. -&gt; í•˜ë‚˜ê°€ ì‚¬ë¼ì ¸ë„ ë‹¤ë¥¸ ê²ƒë“¤ì´ ëŒ€ì²´ ê°€ëŠ¥.        http://info.cern.ch/          home of the first website      ì„œë²„ì™€ í´ë¼ì´ì–¸íŠ¸  Web browser = Client : ì •ë³´ ìš”ì²­  Web server = Server : ì •ë³´ ì‘ë‹µ          Web server ì§ì ‘ êµ¬ì¶•      Web hosting : ë§¡ê¸°ëŠ” ë°©ë²•      ì›¹ í˜¸ìŠ¤íŒ… (github page)  Static(html íŒŒì¼ë§Œâ€¦) Web Hostingì›¹ ì„œë²„ ìš´ì˜í•˜ê¸°  using bitnami MAMP stack  MAMP          M: Mac      A: Apache      M: MySQL      P: PHP            http://127.0.0.1:8080/index.htmlì„ ì›¹ë¸Œë¼ìš°ì €ì— ìž…ë ¥í•˜ë©´ ì›¹ë¸Œë¼ìš°ì €ëŠ” ê°™ì€ ì»´í“¨í„°ì— ì„¤ì¹˜ëœ ì›¹ì„œë²„ì—ê²Œ index.htmlì„ ìš”ì²­í•¨.        ì›¹ì„œë²„ëŠ” ì›¹íŽ˜ì´ì§€ë¥¼ ì €ìž¥í•˜ê¸°ë¡œ ì•½ì†ëœ ë””ë ‰í„°ë¦¬ì¸ htdocsì—ì„œ index.html íŒŒì¼ì˜ ì½”ë“œë¥¼ ì½ì–´ì„œ ì›¹ë¸Œë¼ìš°ì €ì—ê²Œ ì „ì†¡í•¨.        ì›¹ì„œë²„ëŠ” ì½”ë“œë¥¼ í•´ì„í•´ì„œ í™”ë©´ì— ì›¹íŽ˜ì´ì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.        ê°™ì€ ì™€ì´íŒŒì´ë¥¼ ì“°ê³  ìžˆì„ ë•Œ, macì˜ ip addressë¡œ ì ‘ì†í•˜ë©´  ì§œìž”.ì‚¬ìš©í•œ ì½”ë“œë“¤index.html&lt;!doctype html&gt;&lt;html&gt;&lt;head&gt;  &lt;title&gt;WEB1 - Welcome&lt;/title&gt;  &lt;meta charset="utf-8"&gt;&lt;/head&gt;&lt;body&gt;  &lt;h1&gt;&lt;a href="index.html"&gt;WEB&lt;/a&gt;&lt;/h1&gt;&lt;ol&gt;  &lt;li&gt;&lt;a href="1.html"&gt;html&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a href="2.html"&gt;css &lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a href="3.html"&gt;javascript &lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;WEB&lt;/h2&gt;  &lt;p&gt;The World Wide Web (WWW), commonly known as the Web, is an information system where documents and other web resources are identified by Uniform Resource Locators (URLs, such as https://www.example.com/), which may be interlinked by hypertext, and are accessible over the Internet.[1][2] The resources of the WWW are transferred via the Hypertext Transfer Protocol (HTTP) and may be accessed by users by a software application called a web browser and are published by a software application called a web server.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;1.html&lt;!doctype html&gt;&lt;html&gt;&lt;head&gt;  &lt;title&gt;WEB1 - html&lt;/title&gt;  &lt;meta charset="utf-8"&gt;&lt;/head&gt;&lt;body&gt;  &lt;h1&gt;&lt;a href="index.html"&gt;WEB&lt;/a&gt;&lt;/h1&gt;&lt;ol&gt;  &lt;li&gt;&lt;a href="1.html"&gt;html&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a href="2.html"&gt;css &lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a href="3.html"&gt;javascript &lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;HTMLì´ëž€ ë¬´ì—‡ì¸ê°€?&lt;/h2&gt;&lt;p&gt;  &lt;iframe width="560" height="315" src="https://www.youtube.com/embed/7T7r_oSp0SE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;&lt;/p&gt;  &lt;p&gt;&lt;a href="https://www.w3.org/TR/html51/" target="_blank" title="html5 specification"&gt;Hypertext Markup Language (HTML)&lt;/a&gt; is the standard markup language for &lt;strong&gt;creating web pages&lt;/strong&gt; and &lt;u&gt;web&lt;/u&gt; applications.  Web browsers receive HTML documents from a web server or from local storage and render them into multimedia web pages. HTML describes the structure of a web page semantically and originally included cues for the appearance of the document.&lt;/p&gt;  &lt;img src="sea.jpg" width="100%"&gt;  &lt;p style="margin-top:40px"&gt;HTML elements are the building blocks of HTML pages. With HTML constructs, images and other objects, such as interactive forms, may be embedded into the rendered page. It provides a means to create structured documents by denoting structural semantics for text such as headings, paragraphs, lists, links, quotes and other items. HTML elements are delineated by tags, written using angle brackets.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;2.html&lt;!doctype html&gt;&lt;html&gt;&lt;head&gt;  &lt;title&gt;WEB1 - CSS&lt;/title&gt;  &lt;meta charset="utf-8"&gt;&lt;/head&gt;&lt;body&gt;  &lt;h1&gt;&lt;a href="index.html"&gt;WEB&lt;/a&gt;&lt;/h1&gt;&lt;ol&gt;  &lt;li&gt;&lt;a href="1.html"&gt;html&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a href="2.html"&gt;css &lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a href="3.html"&gt;javascript &lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;CSS&lt;/h2&gt;&lt;p&gt;Cascading Style Sheets (CSS) is a style sheet language used for describing the presentation of a document written in a markup language like HTML.[1] CSS is a cornerstone technology of the World Wide Web, alongside HTML and JavaScript.[2]&lt;/p&gt;&lt;p&gt;CSS is designed to enable the separation of presentation and content, including layout, colors, and fonts.[3] This separation can improve content accessibility, provide more flexibility and control in the specification of presentation characteristics, enable multiple web pages to share formatting by specifying the relevant CSS in a separate .css file, and reduce complexity and repetition in the structural content.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;3.html&lt;!doctype html&gt;&lt;html&gt;&lt;head&gt;  &lt;title&gt;WEB1 - javascript&lt;/title&gt;  &lt;meta charset="utf-8"&gt;&lt;/head&gt;&lt;body&gt;  &lt;h1&gt;&lt;a href="index.html"&gt;WEB&lt;/a&gt;&lt;/h1&gt;&lt;ol&gt;  &lt;li&gt;&lt;a href="1.html"&gt;html&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a href="2.html"&gt;css &lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a href="3.html"&gt;javascript &lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;JavaScript&lt;/h2&gt;  &lt;p&gt;JavaScript (/ËˆdÊ’É‘ËvÉ™ËŒskrÉªpt/),[6] often abbreviated as JS, is a programming language that conforms to the ECMAScript specification.[7] JavaScript is high-level, often just-in-time compiled, and multi-paradigm. It has curly-bracket syntax, dynamic typing, prototype-based object-orientation, and first-class functions.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <categories>
        
          <category> Web </category>
        
      </categories>
      <tags>
        
          <tag> Web </tag>
        
          <tag> Html </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[ìŠ¬ëž™ ì±—ë´‡ ë§Œë“¤ê¸° - ì¼ì¼ ì»¤ë°‹ ì²´í¬ë´‡]]></title>
      <url>/project/2020/02/12/slackbot/</url>
      <content type="text"><![CDATA[ì¼ì¼ ì»¤ë°‹ ì—¬ë¶€ë¥¼ ì²´í¬í•´ì£¼ëŠ” Slack chatbot í”„ë¡œì íŠ¸ìž…ë‹ˆë‹¤.FrameworkFlaskë¥¼ ì´ìš©í•˜ì—¬ ì›¹ ì„œë²„ë¥¼ ë§Œë“¤ì–´ ì‚¬ìš©í•¨.ê¸°ëŠ¥(Chatbot functions)  ë‹¹ì¼ì— ê¹ƒí—ˆë¸Œ ì»¤ë°‹ì„ í•˜ì˜€ëŠ”ì§€ ì²´í¬í•˜ê³ , ë§¤ì¼ ì •í•´ì§„ ì‹œê°„ì— ì±—ë´‡ì´ ì§„í–‰ë„ë¥¼ ë³´ëƒ„.  ì¼ì¼ì»¤ë°‹ ê·œì¹™ì´ 13ì¼ + 1ì¼ íœ´ì‹ì´ì—ˆê¸° ë•Œë¬¸ì— ë£¨í‹´ì„ 13ì¼ë¡œ ìž¡ì•˜ìŒ.  ì»¤ë°‹ì„ í–ˆë‹¤ë©´ í•´ë‹¹ ì°¨ìˆ˜ ë‚ ì§œì— í•˜íŠ¸ë¥¼, ì•ˆí–ˆë‹¤ë©´ ê¹¨ì§„ í•˜íŠ¸  13ì¼ ê¸°ì¤€ ë‹¬ì„±ë¥ ì„ í‘œì‹œí•¨.Detail  slack ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ webclientë¥¼ ë§Œë“¦.          token í•„ìš”        í•´ë‹¹ ì±—ë´‡ì´ ì“°ì¸ ìŠ¬ëž™ì—ì„œëŠ” ê¹ƒí—ˆë¸Œ ì±—ë´‡ì´ ë“¤ì–´ê°„ ì»¤ë°‹ ë¡œê·¸ ì±„ë„ì´ ìžˆì—ˆìŒ.          ìœ„ì˜ ì±„ë„ ì •ë³´ë¡œ ì»¤ë°‹ì„ í–ˆëŠ”ì§€ ì•ˆí–ˆëŠ”ì§€ ì²´í¬í•¨.      ]]></content>
      <categories>
        
          <category> Project </category>
        
      </categories>
      <tags>
        
          <tag> Chatbot </tag>
        
          <tag> Slack </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
